# Подробный анализ ключевых пунктов лекции

## Пункт 2: Процесс создания языковой модели

### Сбор данных
- Основной источник данных - платформы вроде Hugging Face, содержащие большие текстовые корпуса
- Необходимо собрать разнообразные тексты для обеспечения широкого охвата языка
- Качество и разнообразие данных напрямую влияют на качество модели

### Нормализация текста
- Цель: создание однородного текста, подготовленного для дальнейшей обработки
- Процесс включает приведение к нижнему регистру, удаление специальных символов
- Важно сохранить смысловую нагрузку, удаляя только "шум"

### Токенизация
- Разбиение текста сначала на предложения, затем на слова
- Преподаватель показал, что современные подходы используют более сложные методы токенизации, чем просто разбиение на слова
- Каждому токену присваивается числовой идентификатор из словаря

### Обучение базовой модели
- Модель обучается предсказывать следующий токен на основе предыдущих
- Для каждой последовательности токенов модель выдаёт вектор вероятностей для всего словаря
- Слова с наибольшей вероятностью выбираются как следующие в последовательности

### Дополнительное обучение
- Используются специально созданные диалоги, написанные людьми-аннотаторами
- Существуют специальные руководства для аннотаторов, определяющие, как отвечать на различные запросы
- Это формирует "личность" и стиль общения модели

### Обучение с подкреплением
- Позволяет модели "рассуждать" и избегать галлюцинаций
- Модель получает награды за хорошие ответы и наказания за плохие
- Преподаватель упомянул пример Андрея Карпати с обучением модели генерации шуток

## Пункт 3: Детали нормализации текста

### Стемминг и лемматизация
- **Стемминг**: отсечение окончаний и суффиксов (например, "duplicating" → "duplic")
- **Лемматизация**: приведение к начальной форме с учётом морфологии (например, "sang", "sung" → "sing")
- Преподаватель продемонстрировал Potter Stemmer и WordNet Lemmatizer из NLTK
- В русском языке процесс сложнее из-за богатой морфологии

### Удаление стоп-слов
- Стоп-слова - это артикли, предлоги, союзы, не несущие смысловой нагрузки
- В NLTK есть готовые списки стоп-слов для разных языков
- Преподаватель показал, что слова вроде "a", "the", "and" имеют очень высокую частоту, но малую смысловую ценность
- Удаление стоп-слов позволяет сосредоточиться на значимых словах

### Построение словаря
- После нормализации создаётся словарь всех уникальных слов
- Каждому слову соответствует частота его появления в корпусе
- Преподаватель показал пример словаря с приблизительно 2000 слов
- Частотность определяет важность слова в модели

## Пункт 4: Техническая реализация

### Использование NLTK
- Natural Language Toolkit - основной инструмент для обработки текста
- Преподаватель продемонстрировал практические примеры кода
- NLTK предоставляет готовые функции для токенизации, стемминга, лемматизации
- Поддерживает разные языки, не только английский

### Практическая демонстрация
- Преподаватель показал пошаговую обработку текста из Википедии
- Продемонстрировал различия между стеммингом и лемматизацией на конкретных примерах
- Показал, как создать словарь с частотами слов
- Привёл пример, когда лемматизатор не справился с приведением "is" к "be"

### Решение проблемы OOV
- OOV (Out of Vocabulary) - слова, которых нет в словаре модели
- Преподаватель упомянул, что современные алгоритмы токенизации лучше справляются с этой проблемой
- Для небольших моделей эта проблема остаётся актуальной
- Необходимо иметь стратегию обработки неизвестных слов при использовании модели


# Дополненный анализ ключевых пунктов лекции с примерами кода

## Пункт 2: Процесс создания языковой модели

### Сбор данных
```python
# Пример загрузки данных с Hugging Face
from datasets import load_dataset

# Загрузка датасета с текстами Википедии
dataset = load_dataset("wikipedia", "20220301.en")
texts = dataset["train"]["text"]

# Или можно загрузить текст из файла
with open("wikipedia_sample.txt", "r", encoding="utf-8") as f:
    text = f.read()
```

### Нормализация текста
```python
# Привести текст к нижнему регистру
text_lower = text.lower()

# Удаление специальных символов
import re
text_clean = re.sub(r'[^\w\s]', '', text_lower)

# Дополнительная очистка
text_clean = text_clean.replace('\n', ' ').replace('\t', ' ')
```

### Токенизация
```python
import nltk
nltk.download('punkt')

# Токенизация по предложениям
sentences = nltk.sent_tokenize(text)
print("Первое предложение:", sentences[0])

# Токенизация по словам
words = nltk.word_tokenize(sentences[0])
print("Первые 5 слов:", words[:5])
```

### Обучение модели (упрощенный пример)
```python
# Преобразование токенов в числа с помощью словаря
vocab = {word: idx for idx, word in enumerate(set(words))}
numeric_tokens = [vocab[word] for word in words if word in vocab]

# Упрощенная схема обучения - создание пар вход-выход
X, y = [], []
sequence_length = 3
for i in range(len(numeric_tokens) - sequence_length):
    X.append(numeric_tokens[i:i+sequence_length])
    y.append(numeric_tokens[i+sequence_length])

# В реальном обучении эти данные подаются в нейросеть
```

## Пункт 3: Детали нормализации текста

### Стемминг и лемматизация
```python
from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download('wordnet')

# Инициализация стеммера и лемматизатора
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# Примеры из лекции
words_examples = ["machine", "learning", "is", "duplicating", "duplicated", "duplicate", "sang", "singing"]

# Стемминг
stemmed_words = [stemmer.stem(word) for word in words_examples]
print("Стемминг:", stemmed_words)
# Вывод примерно: ['machin', 'learn', 'is', 'duplic', 'duplic', 'duplic', 'sang', 'sing']

# Лемматизация
lemmatized_words = [lemmatizer.lemmatize(word) for word in words_examples]
print("Лемматизация:", lemmatized_words)
# Вывод примерно: ['machine', 'learning', 'is', 'duplicating', 'duplicated', 'duplicate', 'sang', 'singing']

# Для глаголов нужно указать часть речи
lemmatized_verbs = [lemmatizer.lemmatize(word, pos='v') for word in words_examples]
print("Лемматизация глаголов:", lemmatized_verbs)
# Здесь 'is' должно превратиться в 'be', но как отметил преподаватель, это не всегда работает корректно
```

### Удаление стоп-слов
```python
from nltk.corpus import stopwords
nltk.download('stopwords')

# Получение списка стоп-слов для английского языка
stop_words = set(stopwords.words('english'))
print("Примеры стоп-слов:", list(stop_words)[:10])

# Фильтрация стоп-слов из текста
filtered_words = [word for word in words if word.lower() not in stop_words]
print(f"Было слов: {len(words)}, стало: {len(filtered_words)}")

# В примере преподавателя частоты слов до фильтрации:
# "," - очень частая
# "the", "a" - частые
# "machine" - менее частая (4 раза)
```

### Построение словаря
```python
from collections import Counter

# Подсчет частот слов
word_counts = Counter(filtered_words)

# Получение наиболее частых слов
most_common = word_counts.most_common(10)
print("10 самых частых слов:", most_common)

# Преобразование в словарь для токенизации
vocabulary = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())}
print(f"Размер словаря: {len(vocabulary)} слов")
```

## Пункт 4: Техническая реализация

### Полный пример обработки текста с NLTK
```python
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
from collections import Counter

# Загрузка необходимых ресурсов
nltk.download(['punkt', 'wordnet', 'stopwords'])

# Исходный текст (пример с лекции)
text = "Machine learning is a field of study that gives computers the ability to learn without being explicitly programmed."

# 1. Токенизация предложений
sentences = sent_tokenize(text)
print("Предложения:", sentences)

# 2. Токенизация слов
tokenized_text = [word_tokenize(sentence) for sentence in sentences]
print("Токенизированный текст:", tokenized_text[0])

# 3. Нормализация
# Инициализация
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Обработка (приведение к нижнему регистру, лемматизация, удаление стоп-слов)
normalized_text = []
for sentence in tokenized_text:
    words = [lemmatizer.lemmatize(word.lower()) for word in sentence]
    filtered_words = [word for word in words if word.lower() not in stop_words]
    normalized_text.append(filtered_words)

print("Нормализованный текст:", normalized_text[0])

# 4. Построение словаря
all_words = [word for sentence in normalized_text for word in sentence]
word_counts = Counter(all_words)
vocabulary = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())}

print("Словарь с индексами:", dict(list(vocabulary.items())[:5]))
```

### Решение проблемы OOV
```python
# Вариант 1: Использование специального токена для неизвестных слов
vocabulary_small = {"machine": 0, "learning": 1, "field": 2, "study": 3}
oov_token = "<UNK>"
vocabulary_small[oov_token] = len(vocabulary_small)

# Токенизация с обработкой OOV
def tokenize_with_oov(text, vocab, oov_token="<UNK>"):
    words = word_tokenize(text.lower())
    return [vocab.get(word, vocab[oov_token]) for word in words]

sample = "Machine learning is amazing"
print("Токенизация с OOV:", tokenize_with_oov(sample, vocabulary_small))

# Вариант 2: Современный подход с byte-pair encoding (BPE)
# Преподаватель упоминал, что это более продвинутый способ
# Пример использования tokenizers из библиотеки transformers
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
encoded = tokenizer("Machine learning is a field that I haven't studied yet")
print("Токены GPT-2:", encoded.tokens())
print("Индексы токенов:", encoded.input_ids)
```

### Конкретные примеры из лекции
Преподаватель показал, что проблема с лемматизатором WordNet возникает при обработке глагола "is":
```python
lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize("is"))  # Выводит "is"
print(lemmatizer.lemmatize("is", pos="v"))  # Должен выводить "be", но не всегда работает
```

Также было продемонстрировано, как стемминг сокращает слова с общим корнем:
```python
stemmer = PorterStemmer()
duplicates = ["duplicate", "duplicated", "duplicating"]
stemmed = [stemmer.stem(word) for word in duplicates]
print(stemmed)  # Все становятся "duplic"
```

Эти примеры демонстрируют практическую реализацию концепций, обсуждаемых в лекции, и дают более конкретное представление о процессе обработки текста для языковых моделей.
