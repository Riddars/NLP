{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n"
      ],
      "metadata": {
        "id": "Es6TE8Ucie6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Установка и импорт библиотек"
      ],
      "metadata": {
        "id": "t3LGO8f-XxEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка необходимых библиотек для работы с текстом на русском языке\n",
        "!pip install nltk pymorphy2\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tLfpGuBREqU",
        "outputId": "5bfb38fb-edd3-4193-f923-83622f247785"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.11/dist-packages (0.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- `nltk` - это Natural Language Toolkit, библиотека для обработки естественного языка. Она содержит инструменты для токенизации текста.\n",
        "- `pymorphy2` - морфологический анализатор для русского языка, который поможет нам с лемматизацией (приведением слов к начальной форме).\n",
        "- `re` - модуль для работы с регулярными выражениями, который пригодится для сложных операций с текстом.\n",
        "- `Counter` и `defaultdict` из модуля `collections` - структуры данных для подсчёта элементов и создания словарей с значениями по умолчанию.\n",
        "- `string` - содержит константы, такие как `string.punctuation` (набор знаков препинания).\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без этих библиотек мы не сможем выполнить задание, так как они предоставляют базовый функционал для обработки текста.\n",
        "- Можно заменить некоторые библиотеки аналогами (например, вместо `pymorphy2` использовать `natasha`), но потребуется изменить соответствующий код."
      ],
      "metadata": {
        "id": "pR5uvglpX54C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка ресурсов NLTK"
      ],
      "metadata": {
        "id": "98iLym5OYJAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка ресурсов NLTK для токенизации и работы со стоп-словами\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAyFN00YRH8h",
        "outputId": "c49a9f63-1594-40ed-e7cc-257bcc04de4f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- `nltk.download('punkt')` - загружает модель для разделения текста на предложения (пунктуация).\n",
        "- `nltk.download('stopwords')` - загружает списки стоп-слов (часто встречающиеся слова, которые обычно не несут значимой информации).\n",
        "- `sent_tokenize` - функция для разделения текста на предложения.\n",
        "- `word_tokenize` - функция для разделения предложений на слова."
      ],
      "metadata": {
        "id": "Tluk8bfHYWuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример текста для обработки (можно заменить на любой другой)\n",
        "text = \"\"\"\n",
        "Машинное обучение — класс методов искусственного интеллекта, характерной чертой которых является не прямое решение задачи,\n",
        "а обучение в процессе применения решений множества сходных задач. Для построения таких методов используются средства математической статистики,\n",
        "численных методов, методов оптимизации, теории вероятностей, теории графов, различные техники работы с данными в цифровой форме.\n",
        "\n",
        "Машинное обучение тесно связано и часто пересекается с вычислительной статистикой, которая также специализируется\n",
        "на прогнозировании с помощью компьютеров. Машинное обучение имеет широкое приложение и используется в компьютерном зрении,\n",
        "распознавании речи, обработке естественного языка, прогнозировании временных рядов, обнаружении мошенничества и манипуляций\n",
        "с рынком, анализе фондового рынка, классификации ДНК-последовательностей, распознавании образов и машинного восприятия,\n",
        "обнаружении спама, распознавании рукописного ввода, игровых программах и робототехнике.\n",
        "\n",
        "Глубокое обучение — совокупность методов машинного обучения, основанных на обучении представлениям, а не на специализированных\n",
        "алгоритмах под конкретные задачи. Многие методы глубокого обучения используют нейронные сети, поэтому глубокое обучение часто\n",
        "ассоциируется с искусственными нейронными сетями. Нейронные сети содержат несколько скрытых слоев, которые преобразуют входные данные\n",
        "в более абстрактные и композиционные представления.\n",
        "\n",
        "Трансформеры — архитектура нейронной сети, полагающаяся на механизм самовнимания для взвешивания относительной важности каждой части входных данных.\n",
        "Они были представлены в статье «Внимание — это всё, что вам нужно» в 2017 году и широко применяются в обработке естественного языка.\n",
        "\n",
        "Векторные представления слов (Word Embeddings) — это представления слов в виде плотных векторов действительных чисел, где семантически близкие слова\n",
        "располагаются близко друг к другу в векторном пространстве. Word2Vec — один из наиболее популярных методов создания таких векторных представлений.\n",
        "Он использует неглубокие нейронные сети для создания векторных представлений слов на основе контекста, в котором они встречаются.\n",
        "\n",
        "Skip-gram и CBOW (Continuous Bag of Words) — две основные архитектуры Word2Vec. Skip-gram предсказывает окружающие слова на основе текущего слова,\n",
        "в то время как CBOW предсказывает текущее слово на основе окружающих. Skip-gram обычно лучше работает с редкими словами и небольшими объемами данных.\n",
        "\n",
        "Косинусное сходство часто используется для измерения сходства между векторами слов. Оно определяется как косинус угла между двумя векторами и\n",
        "принимает значения от -1 (противоположные) до 1 (идентичные). Это позволяет находить слова с похожим значением или использовать векторную арифметику\n",
        "для аналогий, например: \"король\" - \"мужчина\" + \"женщина\" ≈ \"королева\".\n",
        "\"\"\"\n",
        "\n",
        "print(\"Исходный текст:\")\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX8gTnGSRIzD",
        "outputId": "ef25ed61-3048-436b-f7b8-604f27542bee"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходный текст:\n",
            "\n",
            "Машинное обучение — класс методов искусственного интеллекта, характерной чертой которых является не прямое решение задачи, \n",
            "а обучение в процессе применения решений множества сходных задач. Для построения таких методов используются средства математической статистики, \n",
            "численных методов, методов оптимизации, теории вероятностей, теории графов, различные техники работы с данными в цифровой форме.\n",
            "\n",
            "Машинное обучение тесно связано и часто пересекается с вычислительной статистикой, которая также специализируется \n",
            "на прогнозировании с помощью компьютеров. Машинное обучение имеет широкое приложение и используется в компьютерном зрении, \n",
            "распознавании речи, обработке естественного языка, прогнозировании временных рядов, обнаружении мошенничества и манипуляций \n",
            "с рынком, анализе фондового рынка, классификации ДНК-последовательностей, распознавании образов и машинного восприятия, \n",
            "обнаружении спама, распознавании рукописного ввода, игровых программах и робототехнике.\n",
            "\n",
            "Глубокое обучение — совокупность методов машинного обучения, основанных на обучении представлениям, а не на специализированных \n",
            "алгоритмах под конкретные задачи. Многие методы глубокого обучения используют нейронные сети, поэтому глубокое обучение часто \n",
            "ассоциируется с искусственными нейронными сетями. Нейронные сети содержат несколько скрытых слоев, которые преобразуют входные данные \n",
            "в более абстрактные и композиционные представления.\n",
            "\n",
            "Трансформеры — архитектура нейронной сети, полагающаяся на механизм самовнимания для взвешивания относительной важности каждой части входных данных. \n",
            "Они были представлены в статье «Внимание — это всё, что вам нужно» в 2017 году и широко применяются в обработке естественного языка.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нужно скачать punkt_tab в строке ниже"
      ],
      "metadata": {
        "id": "QoSe3faFSPcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxx04jhyR1LC",
        "outputId": "c1370884-cef0-4220-a187-ba82b340340e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt_tab\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package punkt_tab to /root/nltk_data...\n",
            "      Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Токенизация текста на предложения"
      ],
      "metadata": {
        "id": "8jWam8LXYhcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение текста на предложения\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(\"\\nРазделение на предложения:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"Предложение {i+1}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLAkSnICSHkK",
        "outputId": "b51ea4b3-7da2-4250-fd60-fdbbb0b7fe81"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Разделение на предложения:\n",
            "Предложение 1: \n",
            "Машинное обучение — класс методов искусственного интеллекта, характерной чертой которых является не прямое решение задачи, \n",
            "а обучение в процессе применения решений множества сходных задач.\n",
            "Предложение 2: Для построения таких методов используются средства математической статистики, \n",
            "численных методов, методов оптимизации, теории вероятностей, теории графов, различные техники работы с данными в цифровой форме.\n",
            "Предложение 3: Машинное обучение тесно связано и часто пересекается с вычислительной статистикой, которая также специализируется \n",
            "на прогнозировании с помощью компьютеров.\n",
            "Предложение 4: Машинное обучение имеет широкое приложение и используется в компьютерном зрении, \n",
            "распознавании речи, обработке естественного языка, прогнозировании временных рядов, обнаружении мошенничества и манипуляций \n",
            "с рынком, анализе фондового рынка, классификации ДНК-последовательностей, распознавании образов и машинного восприятия, \n",
            "обнаружении спама, распознавании рукописного ввода, игровых программах и робототехнике.\n",
            "Предложение 5: Глубокое обучение — совокупность методов машинного обучения, основанных на обучении представлениям, а не на специализированных \n",
            "алгоритмах под конкретные задачи.\n",
            "Предложение 6: Многие методы глубокого обучения используют нейронные сети, поэтому глубокое обучение часто \n",
            "ассоциируется с искусственными нейронными сетями.\n",
            "Предложение 7: Нейронные сети содержат несколько скрытых слоев, которые преобразуют входные данные \n",
            "в более абстрактные и композиционные представления.\n",
            "Предложение 8: Трансформеры — архитектура нейронной сети, полагающаяся на механизм самовнимания для взвешивания относительной важности каждой части входных данных.\n",
            "Предложение 9: Они были представлены в статье «Внимание — это всё, что вам нужно» в 2017 году и широко применяются в обработке естественного языка.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- `sent_tokenize(text)` разбивает текст на отдельные предложения, анализируя пунктуацию и структуру текста.\n",
        "- Функция возвращает список предложений, который мы сохраняем в переменной `sentences`.\n",
        "- Затем мы выводим каждое предложение с его порядковым номером для наглядности.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без этого шага мы не сможем обрабатывать текст на уровне предложений, что важно для многих задач NLP.\n",
        "- Можно заменить `sent_tokenize` на свою функцию, например, разделяя текст по символам `.`, `!`, `?`, но это менее надежно."
      ],
      "metadata": {
        "id": "7VzRmibTYlZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Токенизация предложений на слова"
      ],
      "metadata": {
        "id": "9kQ3OP1_YuVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизация предложений на слова\n",
        "tokenized_sentences = []\n",
        "for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tokenized_sentences.append(tokens)\n",
        "\n",
        "print(\"\\nПример токенизации предложения на слова:\")\n",
        "print(f\"Предложение: {sentences[0]}\")\n",
        "print(f\"Токены: {tokenized_sentences[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfNBXMz5SLNJ",
        "outputId": "763e4053-5ba9-4b84-e552-e5e520a43c34"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Пример токенизации предложения на слова:\n",
            "Предложение: \n",
            "Машинное обучение — класс методов искусственного интеллекта, характерной чертой которых является не прямое решение задачи, \n",
            "а обучение в процессе применения решений множества сходных задач.\n",
            "Токены: ['Машинное', 'обучение', '—', 'класс', 'методов', 'искусственного', 'интеллекта', ',', 'характерной', 'чертой', 'которых', 'является', 'не', 'прямое', 'решение', 'задачи', ',', 'а', 'обучение', 'в', 'процессе', 'применения', 'решений', 'множества', 'сходных', 'задач', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- Для каждого предложения из списка `sentences` мы применяем функцию `word_tokenize()`.\n",
        "- Эта функция разбивает предложение на отдельные слова и знаки препинания (токены).\n",
        "- Результаты сохраняются в список списков `tokenized_sentences`, где каждый вложенный список содержит токены одного предложения.\n",
        "- В конце выводим пример для первого предложения, чтобы наглядно показать результат.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без токенизации на слова мы не сможем анализировать и обрабатывать текст на уровне отдельных слов.\n",
        "- Можно заменить на разделение по пробелам (`sentence.split()`), но это не будет учитывать знаки препинания и другие нюансы."
      ],
      "metadata": {
        "id": "x7Sh7xayY20J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Нормализация токенов"
      ],
      "metadata": {
        "id": "kOvNBWJYYvNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импорт и настройка морфологического анализатора для русского языка\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Функция для нормализации русских токенов\n",
        "def normalize_tokens(tokens):\n",
        "    \"\"\"\n",
        "    Приводит токены к нормальной форме:\n",
        "    - переводит в нижний регистр\n",
        "    - удаляет знаки препинания\n",
        "    - выполняет лемматизацию (приведение к начальной форме)\n",
        "    \"\"\"\n",
        "    normalized_tokens = []\n",
        "    for token in tokens:\n",
        "        # Приведение к нижнему регистру\n",
        "        token = token.lower()\n",
        "\n",
        "        # Удаление знаков препинания\n",
        "        token = ''.join([char for char in token if char not in string.punctuation])\n",
        "\n",
        "        # Пропуск пустых токенов\n",
        "        if not token:\n",
        "            continue\n",
        "\n",
        "        # Лемматизация для русского языка с помощью pymorphy2\n",
        "        parsed_token = morph.parse(token)[0]\n",
        "        normalized = parsed_token.normal_form\n",
        "\n",
        "        normalized_tokens.append(normalized)\n",
        "\n",
        "    return normalized_tokens\n",
        "\n",
        "# Применение нормализации к нашим токенам\n",
        "normalized_sentences = []\n",
        "for tokens in tokenized_sentences:\n",
        "    normalized_tokens = normalize_tokens(tokens)\n",
        "    normalized_sentences.append(normalized_tokens)\n",
        "\n",
        "print(\"\\nПример нормализованного предложения:\")\n",
        "print(f\"Исходные токены: {tokenized_sentences[0]}\")\n",
        "print(f\"Нормализованные токены: {normalized_sentences[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IDhoKzcSYOB",
        "outputId": "117a727d-54a2-4e46-f498-ec4becc8eaea"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Пример нормализованного предложения:\n",
            "Исходные токены: ['Машинное', 'обучение', '—', 'класс', 'методов', 'искусственного', 'интеллекта', ',', 'характерной', 'чертой', 'которых', 'является', 'не', 'прямое', 'решение', 'задачи', ',', 'а', 'обучение', 'в', 'процессе', 'применения', 'решений', 'множества', 'сходных', 'задач', '.']\n",
            "Нормализованные токены: ['машинный', 'обучение', '—', 'класс', 'метод', 'искусственный', 'интеллект', 'характерный', 'черта', 'который', 'являться', 'не', 'прямой', 'решение', 'задача', 'а', 'обучение', 'в', 'процесс', 'применение', 'решение', 'множество', 'сходный', 'задача']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- Создаем функцию `normalize_tokens`, которая выполняет три основных шага нормализации:\n",
        "  1. **Приведение к нижнему регистру**: \"Слово\" → \"слово\"\n",
        "  2. **Удаление знаков препинания**: \"слово,\" → \"слово\"\n",
        "  3. **Лемматизация**: \"словами\" → \"слово\", \"бежали\" → \"бежать\"\n",
        "- Лемматизация использует `pymorphy2` — морфологический анализатор для русского языка, который приводит слова к их начальной форме.\n",
        "- Для каждого предложения применяем функцию нормализации и сохраняем результаты в `normalized_sentences`.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без нормализации разные формы одного слова будут считаться разными токенами (например, \"слово\", \"слова\", \"словами\" — это три разных токена).\n",
        "- Если убрать лемматизацию, но оставить приведение к нижнему регистру, результат будет менее точным, но всё равно лучше, чем исходный текст.\n",
        "- Можно заменить `pymorphy2` на другую библиотеку"
      ],
      "metadata": {
        "id": "ZhnKxSg1ZOpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создание словаря с индексами и частотами"
      ],
      "metadata": {
        "id": "U6HyIK7TZhUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание словаря из нормализованных токенов\n",
        "# Словарь будет содержать уникальные токены и их частоту\n",
        "vocabulary = {}\n",
        "token_counter = Counter()\n",
        "\n",
        "# Подсчет всех токенов\n",
        "for sentence in normalized_sentences:\n",
        "    token_counter.update(sentence)\n",
        "\n",
        "# Создание словаря с индексами и частотами\n",
        "for idx, (token, count) in enumerate(token_counter.most_common()):\n",
        "    vocabulary[token] = {\n",
        "        \"id\": idx,\n",
        "        \"count\": count\n",
        "    }\n",
        "\n",
        "print(f\"\\nРазмер словаря: {len(vocabulary)} уникальных токенов\")\n",
        "print(\"\\nПример 10 наиболее часто встречающихся токенов:\")\n",
        "for token, info in list(vocabulary.items())[:10]:\n",
        "    print(f\"{token}: встречается {info['count']} раз, id={info['id']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhOicy7mT5Lq",
        "outputId": "b0a33cdd-91aa-43e3-dd77-d27730af82f7"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Размер словаря: 130 уникальных токенов\n",
            "\n",
            "Пример 10 наиболее часто встречающихся токенов:\n",
            "обучение: встречается 9 раз, id=0\n",
            "в: встречается 7 раз, id=1\n",
            "и: встречается 7 раз, id=2\n",
            "метод: встречается 6 раз, id=3\n",
            "машинный: встречается 5 раз, id=4\n",
            "с: встречается 5 раз, id=5\n",
            "—: встречается 4 раз, id=6\n",
            "на: встречается 4 раз, id=7\n",
            "нейронный: встречается 4 раз, id=8\n",
            "сеть: встречается 4 раз, id=9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- Создаем словарь `vocabulary`, который будет содержать информацию о всех уникальных токенах.\n",
        "- Используем `Counter` для подсчета частоты каждого токена во всем тексте.\n",
        "- Для каждого уникального токена создаем запись в словаре, содержащую:\n",
        "  - `id`: уникальный идентификатор токена (порядковый номер)\n",
        "  - `count`: количество появлений токена в тексте\n",
        "- Токены сортируются по частоте с помощью `most_common()`, так что самые частые получают меньшие id.\n",
        "- В конце выводим размер словаря и 10 наиболее часто встречающихся токенов.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без словаря мы не сможем присваивать числовые идентификаторы токенам, что необходимо для обучения моделей машинного обучения.\n",
        "- Можно изменить порядок сортировки (например, по алфавиту), но это менее эффективно, так как частотные токены должны иметь меньшие ID.\n"
      ],
      "metadata": {
        "id": "9MJ4UHzeZXBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Реализация алгоритма Byte-Pair Encoding (BPE)"
      ],
      "metadata": {
        "id": "SPZ01ErbZlRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(vocab):\n",
        "    \"\"\"\n",
        "    Подсчитывает частоту пар символов во всех словах словаря.\n",
        "    Возвращает словарь, где ключ - пара символов, значение - частота встречаемости.\n",
        "    \"\"\"\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[symbols[i], symbols[i + 1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    \"\"\"\n",
        "    Заменяет каждое вхождение пары символов на их объединение.\n",
        "    \"\"\"\n",
        "    v_out = {}\n",
        "    bigram = ' '.join(pair)\n",
        "    replacement = ''.join(pair)\n",
        "    for word in v_in:\n",
        "        w_out = word.replace(bigram, replacement)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "def learn_bpe(words, num_merges=10):\n",
        "    \"\"\"\n",
        "    Обучает модель BPE на списке слов.\n",
        "\n",
        "    Параметры:\n",
        "    words: список слов для обучения\n",
        "    num_merges: количество операций слияния\n",
        "\n",
        "    Возвращает:\n",
        "    bpe_codes: словарь операций слияния\n",
        "    vocab: итоговый словарь с преобразованными словами\n",
        "    \"\"\"\n",
        "    # Создаем словарь слов и их частот\n",
        "    vocab = Counter(words)\n",
        "\n",
        "    # Разделяем каждый символ в слове пробелом\n",
        "    vocab = {' '.join(word): freq for word, freq in vocab.items()}\n",
        "\n",
        "    # Словарь операций слияния\n",
        "    bpe_codes = {}\n",
        "\n",
        "    for i in range(num_merges):\n",
        "        # Получаем статистику по парам\n",
        "        pairs = get_stats(vocab)\n",
        "        if not pairs:\n",
        "            break\n",
        "\n",
        "        # Находим самую частую пару\n",
        "        best = max(pairs, key=pairs.get)\n",
        "\n",
        "        # Сохраняем операцию слияния\n",
        "        bpe_codes[best] = i\n",
        "\n",
        "        # Применяем слияние ко всему словарю\n",
        "        vocab = merge_vocab(best, vocab)\n",
        "\n",
        "        print(f\"Слияние #{i+1}: {best} -> {''.join(best)} (частота: {pairs[best]})\")\n",
        "\n",
        "    return bpe_codes, vocab"
      ],
      "metadata": {
        "id": "M1iVHZezVC8W"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение алгоритма BPE:**\n",
        "1. **Подготовка данных:**\n",
        "   - Начинаем с разделения каждого слова на отдельные символы.\n",
        "   - Например, \"собака\" → \"с о б а к а\".\n",
        "\n",
        "2. **Процесс обучения:**\n",
        "   - `get_stats`: Подсчитывает, как часто встречаются пары соседних символов.\n",
        "   - Находим самую частую пару (например, \"с о\" встречается 100 раз).\n",
        "   - `merge_vocab`: Объединяем эту пару в один токен (\"с о\" → \"со\").\n",
        "   - Сохраняем эту операцию слияния в словарь `bpe_codes`.\n",
        "   - Повторяем процесс заданное количество раз (num_merges).\n",
        "\n",
        "3. **Результат:**\n",
        "   - Получаем словарь операций слияния `bpe_codes`.\n",
        "   - И преобразованный словарь `vocab`, где слова уже разбиты на подтокены по правилам BPE.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без BPE наш словарь будет содержать только целые слова, что приведет к проблеме с неизвестными словами.\n",
        "- Если увеличить `num_merges`, мы получим больше операций слияния, что приведет к более крупным токенам.\n",
        "- Если уменьшить `num_merges`, токены будут меньше, ближе к символам."
      ],
      "metadata": {
        "id": "hPuE7b_MZzGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Построение словаря BPE"
      ],
      "metadata": {
        "id": "fhOHPu5HZ71p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовка данных для BPE: получаем плоский список всех токенов\n",
        "flat_tokens = []\n",
        "for sentence in normalized_sentences:\n",
        "    flat_tokens.extend(sentence)\n",
        "\n",
        "# Обучение модели BPE\n",
        "num_merges = 15  # Количество операций слияния\n",
        "bpe_codes, bpe_vocabulary = learn_bpe(flat_tokens, num_merges)\n",
        "\n",
        "print(\"\\nСловарь операций слияния BPE:\")\n",
        "for pair, index in bpe_codes.items():\n",
        "    print(f\"{pair} -> {''.join(pair)}, индекс операции: {index}\")\n",
        "\n",
        "print(\"\\nПример преобразованных слов после BPE:\")\n",
        "# Показываем первые 5 преобразованных слов\n",
        "sample_items = list(bpe_vocabulary.items())[:5]\n",
        "for encoded, freq in sample_items:\n",
        "    original = encoded.replace(' ', '')\n",
        "    print(f\"Оригинал: {original}, Кодированное: {encoded}, Частота: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFWACGn2VHYQ",
        "outputId": "83706f17-05b6-4e33-e007-27bbbd05695b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Слияние #1: ('н', 'и') -> ни (частота: 35)\n",
            "Слияние #2: ('ы', 'й') -> ый (частота: 31)\n",
            "Слияние #3: ('ни', 'е') -> ние (частота: 27)\n",
            "Слияние #4: ('т', 'ь') -> ть (частота: 27)\n",
            "Слияние #5: ('н', 'ый') -> ный (частота: 25)\n",
            "Слияние #6: ('с', 'т') -> ст (частота: 21)\n",
            "Слияние #7: ('р', 'о') -> ро (частота: 20)\n",
            "Слияние #8: ('е', 'ние') -> ение (частота: 19)\n",
            "Слияние #9: ('н', 'о') -> но (частота: 17)\n",
            "Слияние #10: ('н', 'ный') -> нный (частота: 16)\n",
            "Слияние #11: ('р', 'а') -> ра (частота: 16)\n",
            "Слияние #12: ('в', 'а') -> ва (частота: 16)\n",
            "Слияние #13: ('о', 'б') -> об (частота: 15)\n",
            "Слияние #14: ('т', 'е') -> те (частота: 14)\n",
            "Слияние #15: ('т', 'о') -> то (частота: 13)\n",
            "\n",
            "Словарь операций слияния BPE:\n",
            "('н', 'и') -> ни, индекс операции: 0\n",
            "('ы', 'й') -> ый, индекс операции: 1\n",
            "('ни', 'е') -> ние, индекс операции: 2\n",
            "('т', 'ь') -> ть, индекс операции: 3\n",
            "('н', 'ый') -> ный, индекс операции: 4\n",
            "('с', 'т') -> ст, индекс операции: 5\n",
            "('р', 'о') -> ро, индекс операции: 6\n",
            "('е', 'ние') -> ение, индекс операции: 7\n",
            "('н', 'о') -> но, индекс операции: 8\n",
            "('н', 'ный') -> нный, индекс операции: 9\n",
            "('р', 'а') -> ра, индекс операции: 10\n",
            "('в', 'а') -> ва, индекс операции: 11\n",
            "('о', 'б') -> об, индекс операции: 12\n",
            "('т', 'е') -> те, индекс операции: 13\n",
            "('т', 'о') -> то, индекс операции: 14\n",
            "\n",
            "Пример преобразованных слов после BPE:\n",
            "Оригинал: машинный, Кодированное: м а ш и нный, Частота: 5\n",
            "Оригинал: обучение, Кодированное: об у ч ение, Частота: 9\n",
            "Оригинал: —, Кодированное: —, Частота: 4\n",
            "Оригинал: класс, Кодированное: к л а с с, Частота: 1\n",
            "Оригинал: метод, Кодированное: м е то д, Частота: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- Создаем `flat_tokens` - плоский список всех токенов из всех предложений.\n",
        "- Запускаем процесс обучения BPE с заданным числом слияний (15).\n",
        "- Получаем словарь операций слияния `bpe_codes` и преобразованный словарь `bpe_vocabulary`.\n",
        "- Выводим операции слияния, чтобы увидеть, какие пары символов объединялись.\n",
        "- Показываем примеры слов после применения BPE кодирования.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Изменение `num_merges` влияет на грануляцию токенизации:\n",
        "  - Больше слияний = более крупные токены, меньший словарь, но хуже обобщение.\n",
        "  - Меньше слияний = более мелкие токены, больший словарь, лучше обобщение на новые слова.\n",
        "\n"
      ],
      "metadata": {
        "id": "ekBcYnITaDdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Применение BPE к новому тексту и преобразование в идентификаторы"
      ],
      "metadata": {
        "id": "Bl7Td4DVaT63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_bpe_to_word(word, bpe_codes):\n",
        "    \"\"\"\n",
        "    Применяет обученную модель BPE к слову.\n",
        "    \"\"\"\n",
        "    # Разделяем слово на символы\n",
        "    word = ' '.join(list(word))\n",
        "\n",
        "    # Применяем операции слияния в порядке их изучения\n",
        "    for pair, _ in sorted(bpe_codes.items(), key=lambda x: x[1]):\n",
        "        bigram = ' '.join(pair)\n",
        "        replacement = ''.join(pair)\n",
        "        word = word.replace(bigram, replacement)\n",
        "\n",
        "    return word.split()\n",
        "\n",
        "def tokenize_with_bpe(text, bpe_codes):\n",
        "    \"\"\"\n",
        "    Токенизирует текст с помощью BPE.\n",
        "    \"\"\"\n",
        "    # Сначала разбиваем на предложения и слова\n",
        "    sentences = sent_tokenize(text)\n",
        "    result = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        normalized = normalize_tokens(tokens)\n",
        "\n",
        "        # Применяем BPE к каждому нормализованному токену\n",
        "        bpe_tokens = []\n",
        "        for token in normalized:\n",
        "            bpe_tokens.extend(apply_bpe_to_word(token, bpe_codes))\n",
        "\n",
        "        result.append(bpe_tokens)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Создаем словарь из BPE токенов\n",
        "bpe_token_to_id = {}\n",
        "id_counter = 0\n",
        "\n",
        "for word in bpe_vocabulary:\n",
        "    for token in word.split():\n",
        "        if token not in bpe_token_to_id:\n",
        "            bpe_token_to_id[token] = id_counter\n",
        "            id_counter += 1\n",
        "\n",
        "# Применяем BPE к новому тексту\n",
        "sample_text = \"Нейронные сети обрабатывают данные.\"\n",
        "bpe_tokenized = tokenize_with_bpe(sample_text, bpe_codes)\n",
        "\n",
        "# Преобразуем в идентификаторы\n",
        "token_ids = []\n",
        "for sentence in bpe_tokenized:\n",
        "    for token in sentence:\n",
        "        if token in bpe_token_to_id:\n",
        "            token_ids.append(bpe_token_to_id[token])\n",
        "        else:\n",
        "            # Можно добавить специальный токен для неизвестных слов\n",
        "            print(f\"Неизвестный токен: {token}\")\n",
        "\n",
        "print(\"\\nПример преобразования текста в идентификаторы токенов:\")\n",
        "print(f\"Исходный текст: {sample_text}\")\n",
        "print(f\"Токены после BPE: {bpe_tokenized}\")\n",
        "print(f\"Идентификаторы токенов: {token_ids}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ-mrKJ_VVAF",
        "outputId": "cab26b2d-ebde-4956-c159-0eb73a7b7696"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Пример преобразования текста в идентификаторы токенов:\n",
            "Исходный текст: Нейронные сети обрабатывают данные.\n",
            "Токены после BPE: [['н', 'е', 'й', 'ро', 'нный', 'с', 'е', 'ть', 'об', 'ра', 'б', 'а', 'т', 'ы', 'ва', 'ть', 'д', 'а', 'ть']]\n",
            "Идентификаторы токенов: [18, 13, 30, 32, 4, 12, 13, 28, 5, 22, 42, 1, 20, 43, 37, 28, 15, 1, 28]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "1. **Функция `apply_bpe_to_word`:**\n",
        "   - Разбивает слово на символы.\n",
        "   - Применяет операции слияния в том порядке, в котором они были изучены.\n",
        "   - Возвращает список подтокенов после применения BPE.\n",
        "\n",
        "2. **Функция `tokenize_with_bpe`:**\n",
        "   - Разбивает текст на предложения и слова.\n",
        "   - Нормализует слова (нижний регистр, лемматизация).\n",
        "   - Применяет BPE к каждому нормализованному слову.\n",
        "   - Возвращает список списков токенов для каждого предложения.\n",
        "\n",
        "3. **Создание словаря идентификаторов:**\n",
        "   - Мы присваиваем уникальный числовой ID каждому подтокену из нашего BPE словаря.\n",
        "\n",
        "4. **Применение к новому тексту:**\n",
        "   - Берем новый пример текста.\n",
        "   - Применяем все шаги обработки: токенизация → нормализация → BPE.\n",
        "   - Преобразуем получившиеся токены в идентификаторы.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без функции применения BPE мы не сможем обрабатывать новые тексты с помощью нашего метода.\n",
        "- Изменение порядка применения операций слияния повлияет на результат токенизации, поэтому важно соблюдать тот же порядок, что и при обучении."
      ],
      "metadata": {
        "id": "Ln0aUZ_RaaMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Статистика по полученным результатам\n",
        "print(\"\\nИтоговая статистика:\")\n",
        "print(f\"Количество предложений в тексте: {len(sentences)}\")\n",
        "print(f\"Общее количество токенов до нормализации: {sum(len(s) for s in tokenized_sentences)}\")\n",
        "print(f\"Общее количество токенов после нормализации: {sum(len(s) for s in normalized_sentences)}\")\n",
        "print(f\"Размер исходного словаря (уникальных слов): {len(vocabulary)}\")\n",
        "print(f\"Количество операций слияния BPE: {len(bpe_codes)}\")\n",
        "print(f\"Размер BPE словаря: {len(bpe_token_to_id)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2lE2EhGVXw9",
        "outputId": "095241da-8239-461c-b6cb-f9ebc1944f7b"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Итоговая статистика:\n",
            "Количество предложений в тексте: 9\n",
            "Общее количество токенов до нормализации: 234\n",
            "Общее количество токенов после нормализации: 201\n",
            "Размер исходного словаря (уникальных слов): 130\n",
            "Количество операций слияния BPE: 15\n",
            "Размер BPE словаря: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подробное объяснение технологии токенизации и BPE кодирования\n",
        "\n",
        "## 1. Зачем нужна токенизация текста\n",
        "\n",
        "**Что это такое:** Токенизация — это процесс разделения текста на более мелкие части (токены), которые могут быть словами, частями слов или даже отдельными символами.\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Компьютеры не могут напрямую работать с текстом — им нужны числа. Токенизация позволяет преобразовать текст в последовательность числовых идентификаторов.\n",
        "- Языковые модели обучаются предсказывать следующий токен на основе предыдущих. Без токенизации модель не сможет работать с текстом.\n",
        "- Токенизация определяет уровень грануляции, на котором модель \"понимает\" текст.\n",
        "\n",
        "**От чего зависит качество:**\n",
        "- От выбранного алгоритма токенизации\n",
        "- От особенностей языка (для русского важно учитывать богатую морфологию)\n",
        "- От специфики текстов в вашем датасете\n",
        "\n",
        "**Технические последствия:**\n",
        "- Чем больше размер словаря (количество уникальных токенов), тем больше памяти требуется\n",
        "- Слишком маленький словарь приведет к потере информации\n",
        "- Слишком большой словарь усложнит обучение модели и потребует больше данных\n",
        "\n",
        "## 2. Разделение на предложения\n",
        "\n",
        "**Что происходит:** Текст разбивается на отдельные предложения с помощью функции `sent_tokenize` из библиотеки NLTK.\n",
        "\n",
        "**Как это работает:**\n",
        "- Алгоритм анализирует знаки препинания (точки, восклицательные знаки, вопросительные знаки)\n",
        "- Учитывает исключения (сокращения, цифры с точками, инициалы)\n",
        "- Использует обученную модель, которая разбирает различные случаи на основе статистических паттернов\n",
        "\n",
        "**Зачем разделять на предложения:**\n",
        "- Предложение — логическая единица смысла в тексте\n",
        "- Обработка по предложениям эффективнее, чем обработка всего текста сразу\n",
        "- Многие задачи NLP (анализ тональности, классификация) работают на уровне предложений\n",
        "- Связи между словами в разных предложениях обычно слабее, чем внутри одного предложения\n",
        "\n",
        "**От чего зависит точность:**\n",
        "- Качество и правильность пунктуации в исходном тексте\n",
        "- Наличие специфических конструкций (прямая речь, списки, цитаты)\n",
        "- Язык текста и его соответствие обученной модели токенизатора\n",
        "\n",
        "## 3. Разделение предложений на токены (слова)\n",
        "\n",
        "**Что происходит:** Каждое предложение разбивается на слова и знаки препинания с помощью функции `word_tokenize`.\n",
        "\n",
        "**Как это работает:**\n",
        "- Алгоритм разделяет текст по пробелам и знакам препинания\n",
        "- Учитывает сложные случаи (апострофы, дефисы, сокращения)\n",
        "- Сохраняет знаки препинания как отдельные токены\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Слова — базовые единицы значения в языке\n",
        "- Разделение текста на слова позволяет анализировать структуру предложений\n",
        "- Создает основу для дальнейшей нормализации и обработки\n",
        "\n",
        "**Технические нюансы:**\n",
        "- В некоторых языках (например, китайском или японском) разделение на слова сложнее, так как нет явных разделителей\n",
        "- Специальные конструкции (email-адреса, URL, даты) требуют особой обработки\n",
        "- В русском языке важно правильно обрабатывать составные слова с дефисами\n",
        "\n",
        "## 4. Нормализация токенов\n",
        "\n",
        "**Что происходит:** Токены (слова) приводятся к стандартной форме через три основных процесса:\n",
        "1. Приведение к нижнему регистру\n",
        "2. Удаление знаков препинания\n",
        "3. Лемматизация (приведение к начальной форме)\n",
        "\n",
        "**Как работает лемматизация:**\n",
        "- Для русского языка используется библиотека `pymorphy2`\n",
        "- Анализируется морфологическая структура слова\n",
        "- Определяется часть речи и грамматические характеристики\n",
        "- Слово преобразуется к начальной форме:\n",
        "  - существительные → единственное число, именительный падеж\n",
        "  - глаголы → инфинитив\n",
        "  - прилагательные → мужской род, ед. число, именительный падеж\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Снижает количество уникальных токенов (размер словаря)\n",
        "- Объединяет разные формы одного слова: \"книга\", \"книги\", \"книгами\" → \"книга\"\n",
        "- Позволяет модели видеть связь между разными формами одного слова\n",
        "- Улучшает статистические показатели частотности слов\n",
        "\n",
        "**От чего зависит качество:**\n",
        "- От используемого морфологического анализатора\n",
        "- От особенностей языка (для русского лемматизация особенно важна из-за богатства словоформ)\n",
        "- От предметной области текста (специальные термины могут неверно лемматизироваться)\n",
        "\n",
        "## 5. Создание словаря (vocabulary)\n",
        "\n",
        "**Что происходит:** После нормализации создается словарь всех уникальных токенов, где каждому токену присваивается уникальный идентификатор и подсчитывается его частота в тексте.\n",
        "\n",
        "**Как это работает:**\n",
        "- Используется структура данных `Counter` для подсчета встречаемости каждого токена\n",
        "- Токены сортируются по частоте (от наиболее к наименее частым)\n",
        "- Каждому токену присваивается числовой идентификатор (ID)\n",
        "- Создается словарь, где ключ — токен, а значение — объект с его ID и частотой\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Языковые модели работают с числами, а не текстом\n",
        "- Словарь позволяет преобразовывать текст в последовательность чисел и обратно\n",
        "- Частотность токенов используется для оптимизации представления (часто встречающиеся токены получают меньшие ID)\n",
        "- Словарь определяет, какие слова \"знает\" модель\n",
        "\n",
        "**Технические соображения:**\n",
        "- Размер словаря прямо влияет на размер модели и требования к памяти\n",
        "- Слишком большой словарь приводит к разреженным представлениям и проблемам с обучением\n",
        "- Слишком маленький словарь вызывает проблему неизвестных слов (OOV — out-of-vocabulary)\n",
        "\n",
        "## 6. Алгоритм Byte-Pair Encoding (BPE)\n",
        "\n",
        "**Что это такое:** BPE — алгоритм сжатия данных, который в NLP используется для создания подсловных токенов, позволяющих эффективно представлять как частые, так и редкие слова.\n",
        "\n",
        "**Как работает:**\n",
        "1. **Начальное состояние:** Каждое слово разбивается на отдельные символы, разделенные пробелами\n",
        "2. **Итеративный процесс:**\n",
        "   - Подсчитываем частоту всех пар соседних символов/токенов\n",
        "   - Находим самую частую пару\n",
        "   - Объединяем эту пару в один новый токен\n",
        "   - Заменяем все вхождения этой пары в словаре на новый токен\n",
        "   - Повторяем процесс заданное число раз (num_merges)\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Решает проблему неизвестных слов, разбивая редкие слова на подсловные части\n",
        "- Более эффективно использует словарь, чем пословная токенизация\n",
        "- Позволяет модели улавливать морфологические особенности языка\n",
        "- Обеспечивает баланс между размером словаря и способностью представлять любые слова\n",
        "\n",
        "**От чего зависит эффективность:**\n",
        "- От количества операций слияния (num_merges):\n",
        "  - Малое количество → мелкие токены, ближе к посимвольному представлению\n",
        "  - Большое количество → крупные токены, ближе к пословному представлению\n",
        "- От размера и разнообразия тренировочного корпуса\n",
        "- От языковых особенностей (например, для агглютинативных языков BPE особенно эффективен)\n",
        "\n",
        "**Конкретные технические эффекты:**\n",
        "- BPE с 10-15 тысячами операций слияния обычно создает словарь размером 30-50 тысяч токенов\n",
        "- Частые слова представляются одним токеном, редкие разбиваются на несколько подтокенов\n",
        "- Слово \"переобучение\" может быть разбито на \"пере\" + \"обучение\", если эти части чаще встречаются по отдельности\n",
        "\n",
        "## 7. Применение BPE к новому тексту\n",
        "\n",
        "**Что происходит:** Когда приходит новый текст, мы применяем весь процесс обработки и используем ранее полученные правила BPE для его токенизации.\n",
        "\n",
        "**Как это работает:**\n",
        "1. Текст разбивается на предложения\n",
        "2. Предложения токенизируются на слова\n",
        "3. Слова нормализуются (нижний регистр, лемматизация)\n",
        "4. Каждое слово разбивается на символы\n",
        "5. К слову последовательно применяются операции слияния, в том же порядке, как они были найдены при обучении\n",
        "6. Полученные подтокены преобразуются в числовые идентификаторы\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Обеспечивает единообразное представление как тренировочных, так и новых данных\n",
        "- Позволяет модели работать с ранее не встречавшимися словами\n",
        "- Создает числовое представление текста, пригодное для обработки нейронными сетями\n",
        "\n",
        "**Технические особенности:**\n",
        "- Порядок применения операций слияния критически важен\n",
        "- Если токен не был встречен при обучении, можно использовать специальный токен [UNK] (unknown)\n",
        "- Некоторые реализации используют дополнительные специальные токены:\n",
        "  - [BOS]/[SOS] — начало предложения (Beginning/Start of Sentence)\n",
        "  - [EOS] — конец предложения (End of Sentence)\n",
        "  - [PAD] — заполнитель для выравнивания длины последовательностей\n",
        "\n",
        "## 8. От чего зависит качество всего процесса\n",
        "\n",
        "**Размер и качество обучающего корпуса:**\n",
        "- Больший корпус → лучшее покрытие языка\n",
        "- Разнообразие текстов → лучшая обобщающая способность\n",
        "- Качество текстов → меньше шума и ошибок в данных\n",
        "\n",
        "**Параметры токенизации:**\n",
        "- Выбор метода токенизации (WordPiece, BPE, Unigram и др.)\n",
        "- Размер словаря (маленький — компактность, большой — точность)\n",
        "- Количество операций слияния в BPE (баланс между детализацией и обобщением)\n",
        "\n",
        "**Предобработка текста:**\n",
        "- Качество нормализации (лемматизация vs стемминг)\n",
        "- Обработка специальных случаев (числа, даты, URL)\n",
        "- Удаление или сохранение пунктуации\n",
        "\n",
        "**Применение в языковой модели:**\n",
        "- Способ векторизации токенов (one-hot, embeddings)\n",
        "- Архитектура модели (RNN, Transformer)\n",
        "- Контекстное окно (сколько предыдущих токенов учитывается)\n",
        "\n",
        "## 9. Практическое значение всего процесса\n",
        "\n",
        "**Для языковых моделей:**\n",
        "- BPE позволяет эффективно работать со словарем ограниченного размера\n",
        "- Подсловные токены помогают улавливать морфологические и семантические связи\n",
        "- Сокращается количество неизвестных слов, улучшается обобщающая способность\n",
        "\n",
        "**Для практических приложений:**\n",
        "- Уменьшает требования к памяти (по сравнению с посимвольной токенизацией)\n",
        "- Улучшает работу с редкими словами и новыми терминами\n",
        "- Повышает эффективность машинного перевода, генерации текста, классификации и других задач NLP\n",
        "\n",
        "**Для многоязычных моделей:**\n",
        "- BPE позволяет создать общий словарь для нескольких языков\n",
        "- Обнаруживает общие морфемы между родственными языками\n",
        "- Эффективно работает как с аналитическими, так и с синтетическими языками\n",
        "\n",
        "Понимание процесса токенизации и BPE кодирования критически важно для работы с современными языковыми моделями, так как это первый и один из самых важных шагов в преобразовании текста в форму, понятную компьютеру."
      ],
      "metadata": {
        "id": "_mXudGZ8ccB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "YK6aYweKii2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Реализация N-граммной модели"
      ],
      "metadata": {
        "id": "DU5FKTWksLIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NGramModel:\n",
        "    \"\"\"\n",
        "    Класс для построения и использования N-граммной модели языка.\n",
        "    \"\"\"\n",
        "    def __init__(self, n=2):\n",
        "        \"\"\"\n",
        "        Инициализация модели.\n",
        "\n",
        "        Параметры:\n",
        "        n - размер n-граммы (по умолчанию n=2, т.е. биграммы)\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.ngrams = defaultdict(Counter)  # {(prev_tokens): {next_token: count}}\n",
        "        self.context_counts = defaultdict(int)  # {(prev_tokens): total_count}\n",
        "        self.vocabulary = set()  # все уникальные токены\n",
        "\n",
        "        # Специальные токены\n",
        "        self.START_TOKEN = \"<s>\"  # маркер начала предложения\n",
        "        self.END_TOKEN = \"</s>\"   # маркер конца предложения\n",
        "        self.UNK_TOKEN = \"<unk>\"  # маркер неизвестного слова\n",
        "\n",
        "        # Параметры сглаживания\n",
        "        self.alpha = 0.1  # параметр аддитивного сглаживания\n",
        "\n",
        "    def fit(self, sentences):\n",
        "        \"\"\"\n",
        "        Обучение модели на корпусе предложений.\n",
        "\n",
        "        Параметры:\n",
        "        sentences - список предложений, где каждое предложение - список токенов\n",
        "        \"\"\"\n",
        "        # Сбор лексикона\n",
        "        for sentence in sentences:\n",
        "            for token in sentence:\n",
        "                self.vocabulary.add(token)\n",
        "\n",
        "        # Добавляем специальные токены в словарь\n",
        "        self.vocabulary.add(self.START_TOKEN)\n",
        "        self.vocabulary.add(self.END_TOKEN)\n",
        "        self.vocabulary.add(self.UNK_TOKEN)\n",
        "\n",
        "        # Сбор n-грамм\n",
        "        for sentence in sentences:\n",
        "            # Добавляем маркеры начала и конца предложения\n",
        "            padded_sentence = [self.START_TOKEN] * (self.n - 1) + sentence + [self.END_TOKEN]\n",
        "\n",
        "            # Собираем n-граммы\n",
        "            for i in range(len(padded_sentence) - self.n + 1):\n",
        "                ngram = tuple(padded_sentence[i:i+self.n])\n",
        "                context = ngram[:-1]  # все, кроме последнего токена\n",
        "                next_token = ngram[-1]  # последний токен\n",
        "\n",
        "                self.ngrams[context][next_token] += 1\n",
        "                self.context_counts[context] += 1\n",
        "\n",
        "        print(f\"Модель обучена: собрано {len(self.ngrams)} уникальных контекстов\")\n",
        "        return self\n",
        "\n",
        "    def get_probability(self, context, token, smoothing='additive'):\n",
        "        \"\"\"\n",
        "        Возвращает вероятность токена, учитывая предыдущий контекст.\n",
        "\n",
        "        Параметры:\n",
        "        context - кортеж из (n-1) предыдущих токенов\n",
        "        token - токен, вероятность которого мы вычисляем\n",
        "        smoothing - метод сглаживания ('additive', 'none')\n",
        "\n",
        "        Возвращает:\n",
        "        Вероятность P(token|context)\n",
        "        \"\"\"\n",
        "        # Проверяем наличие контекста в модели\n",
        "        if context not in self.ngrams:\n",
        "            # Если контекст не встречался, возвращаем равномерное распределение\n",
        "            if smoothing == 'none':\n",
        "                return 0.0\n",
        "            else:  # additive smoothing\n",
        "                return 1.0 / len(self.vocabulary)\n",
        "\n",
        "        # Проверяем наличие токена в данном контексте\n",
        "        count = self.ngrams[context].get(token, 0)\n",
        "        total = self.context_counts[context]\n",
        "\n",
        "        if smoothing == 'none':\n",
        "            # Без сглаживания\n",
        "            return count / total if total > 0 else 0.0\n",
        "        else:  # additive smoothing\n",
        "            # Аддитивное сглаживание: (count + alpha) / (total + alpha * |V|)\n",
        "            V = len(self.vocabulary)\n",
        "            return (count + self.alpha) / (total + self.alpha * V)\n",
        "\n",
        "    def get_next_token_probabilities(self, context, smoothing='additive'):\n",
        "        \"\"\"\n",
        "        Возвращает словарь вероятностей всех возможных следующих токенов.\n",
        "\n",
        "        Параметры:\n",
        "        context - кортеж из (n-1) предыдущих токенов\n",
        "        smoothing - метод сглаживания ('additive', 'none')\n",
        "\n",
        "        Возвращает:\n",
        "        Словарь {token: probability}\n",
        "        \"\"\"\n",
        "        probabilities = {}\n",
        "\n",
        "        if smoothing == 'none':\n",
        "            # Без сглаживания - берем только встречавшиеся токены\n",
        "            if context in self.ngrams:\n",
        "                total = self.context_counts[context]\n",
        "                for token, count in self.ngrams[context].items():\n",
        "                    probabilities[token] = count / total\n",
        "        else:  # additive smoothing\n",
        "            # С аддитивным сглаживанием - учитываем все токены из словаря\n",
        "            for token in self.vocabulary:\n",
        "                probabilities[token] = self.get_probability(context, token, smoothing)\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "    def generate_text(self, max_length=20, start_context=None, smoothing='additive'):\n",
        "        \"\"\"\n",
        "        Генерирует текст с использованием обученной модели.\n",
        "\n",
        "        Параметры:\n",
        "        max_length - максимальная длина генерируемого текста (в токенах)\n",
        "        start_context - начальный контекст (если None, начинается с START_TOKEN)\n",
        "        smoothing - метод сглаживания ('additive', 'none')\n",
        "\n",
        "        Возвращает:\n",
        "        Список сгенерированных токенов\n",
        "        \"\"\"\n",
        "        # Инициализация контекста\n",
        "        if start_context is None:\n",
        "            context = tuple([self.START_TOKEN] * (self.n - 1))\n",
        "        else:\n",
        "            # Убедимся, что контекст имеет правильную длину\n",
        "            context = tuple(start_context[-(self.n-1):])\n",
        "\n",
        "        # Генерация текста\n",
        "        generated = list(context)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Получаем вероятности следующего токена\n",
        "            token_probs = self.get_next_token_probabilities(context, smoothing)\n",
        "\n",
        "            # Если нет вероятностей, выходим из цикла\n",
        "            if not token_probs:\n",
        "                break\n",
        "\n",
        "            # Выбираем токен согласно вероятностям\n",
        "            tokens = list(token_probs.keys())\n",
        "            probs = list(token_probs.values())\n",
        "            next_token = random.choices(tokens, weights=probs, k=1)[0]\n",
        "\n",
        "            # Добавляем токен к результату\n",
        "            generated.append(next_token)\n",
        "\n",
        "            # Если сгенерировали токен конца предложения, останавливаемся\n",
        "            if next_token == self.END_TOKEN:\n",
        "                break\n",
        "\n",
        "            # Обновляем контекст\n",
        "            context = tuple(generated[-(self.n-1):])\n",
        "\n",
        "        # Убираем маркеры начала предложения из результата\n",
        "        result = [token for token in generated if token != self.START_TOKEN]\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "1gHYS6OLcfpn"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Комментарии:**\n",
        "- Класс `NGramModel` реализует n-граммную модель языка с возможностью выбора размера n-граммы.\n",
        "- **Основные структуры данных**:\n",
        "  - `self.ngrams` - словарь, где ключ - контекст (n-1 предыдущих токенов), значение - счетчик следующих токенов.\n",
        "  - `self.context_counts` - общее количество раз, когда встречался данный контекст.\n",
        "  - `self.vocabulary` - множество всех уникальных слов.\n",
        "- **Специальные токены**:\n",
        "  - `START_TOKEN` (`<s>`) - маркер начала предложения.\n",
        "  - `END_TOKEN` (`</s>`) - маркер конца предложения.\n",
        "  - `UNK_TOKEN` (`<unk>`) - маркер для неизвестных слов (встречающихся при генерации).\n",
        "- **Метод `fit`**:\n",
        "  1. Собирает все уникальные токены в словарь.\n",
        "  2. Для каждого предложения добавляет специальные маркеры начала и конца.\n",
        "  3. Подсчитывает частоту каждой n-граммы.\n",
        "- **Метод `get_probability`**:\n",
        "  - Вычисляет условную вероятность P(token|context).\n",
        "  - Реализует два варианта расчета: без сглаживания и с аддитивным сглаживанием.\n",
        "  - Формула аддитивного сглаживания: (count + alpha) / (total + alpha * |V|)\n",
        "- **Метод `get_next_token_probabilities`**:\n",
        "  - Возвращает вероятности всех возможных следующих токенов для данного контекста.\n",
        "  - При использовании сглаживания учитывает все токены из словаря.\n",
        "- **Метод `generate_text`**:\n",
        "  1. Начинает с заданного контекста или с маркера начала предложения.\n",
        "  2. На каждом шаге выбирает следующий токен случайно, с вероятностями согласно модели.\n",
        "  3. Обновляет контекст (скользящее окно размера n-1).\n",
        "  4. Останавливается, если достигнута максимальная длина или сгенерирован маркер конца предложения.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Правильная реализация вероятностной модели - ключ к качественной генерации текста.\n",
        "- Сглаживание критически важно для обработки редких и неизвестных слов.\n",
        "- Структура данных `defaultdict(Counter)` идеально подходит для эффективного хранения n-грамм.\n",
        "- Использование специальных токенов позволяет корректно моделировать начало и конец предложений."
      ],
      "metadata": {
        "id": "e5YW-wj2sPK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучаем биграммную модель (n=2)\n",
        "bigram_model = NGramModel(n=2)\n",
        "bigram_model.fit(normalized_sentences)\n",
        "\n",
        "# Обучаем триграммную модель (n=3)\n",
        "trigram_model = NGramModel(n=3)\n",
        "trigram_model.fit(normalized_sentences)\n",
        "\n",
        "# Обучаем 4-граммную модель (n=4)\n",
        "fourgram_model = NGramModel(n=4)\n",
        "fourgram_model.fit(normalized_sentences)\n",
        "\n",
        "print(\"Все модели успешно обучены.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4KENIRRks5s",
        "outputId": "a1342c4f-9ab0-4ac7-e7b3-4c304494b9a8"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Модель обучена: собрано 131 уникальных контекстов\n",
            "Модель обучена: собрано 189 уникальных контекстов\n",
            "Модель обучена: собрано 197 уникальных контекстов\n",
            "Все модели успешно обучены.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- В этом блоке мы создаем и обучаем три модели с различными значениями n:\n",
        "  1. Биграммная модель (n=2) - учитывает только один предыдущий токен.\n",
        "  2. Триграммная модель (n=3) - учитывает два предыдущих токена.\n",
        "  3. 4-граммная модель (n=4) - учитывает три предыдущих токена.\n",
        "- Все модели обучаются на одном и том же наборе нормализованных предложений.\n",
        "- Чем больше n, тем больше контекста учитывает модель, но тем больше данных требуется для надежной оценки вероятностей.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Сравнение моделей разного порядка позволяет найти оптимальный баланс между точностью и разреженностью данных.\n",
        "- Биграммные модели часто дают неплохие результаты даже на малых данных.\n",
        "- Модели высоких порядков могут страдать от проблемы разреженности: многие n-граммы встречаются очень редко или не встречаются вообще."
      ],
      "metadata": {
        "id": "zmVMhG0js7Ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объяснение: Эти числа показывают, сколько различных контекстов (предшествующих n-1 слов) было обнаружено в тексте. С увеличением n растет количество уникальных контекстов, что логично - длинные последовательности имеют больше вариаций. Это важный показатель разнообразия данных для обучения."
      ],
      "metadata": {
        "id": "UOMG44wMm7VB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тестирование методов сглаживания"
      ],
      "metadata": {
        "id": "KteyIErwtsES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_smoothing(model, context, smoothing_methods=['none', 'additive']):\n",
        "    \"\"\"\n",
        "    Сравнивает различные методы сглаживания для заданного контекста.\n",
        "\n",
        "    Параметры:\n",
        "    model - обученная N-граммная модель\n",
        "    context - контекст (n-1 токенов)\n",
        "    smoothing_methods - список методов сглаживания для тестирования\n",
        "    \"\"\"\n",
        "    print(f\"Тестирование методов сглаживания для контекста: {context}\")\n",
        "\n",
        "    for method in smoothing_methods:\n",
        "        print(f\"\\nМетод сглаживания: {method}\")\n",
        "        probs = model.get_next_token_probabilities(context, smoothing=method)\n",
        "\n",
        "        # Выводим топ-5 наиболее вероятных следующих токенов\n",
        "        top_tokens = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "        for token, prob in top_tokens:\n",
        "            print(f\"  {token}: {prob:.4f}\")\n",
        "\n",
        "        # Проверка суммы вероятностей\n",
        "        total_prob = sum(probs.values())\n",
        "        print(f\"  Сумма всех вероятностей: {total_prob:.4f}\")\n",
        "\n",
        "# Тестируем сглаживание для биграммной модели\n",
        "context_bi = (bigram_model.START_TOKEN,)\n",
        "test_smoothing(bigram_model, context_bi)\n",
        "\n",
        "# Тестируем сглаживание для триграммной модели\n",
        "if trigram_model.ngrams:  # Убедимся, что есть данные\n",
        "    context_tri = (trigram_model.START_TOKEN, normalized_sentences[0][0])\n",
        "    test_smoothing(trigram_model, context_tri)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upaeZePKkxW0",
        "outputId": "28058974-e30f-4ffc-95b5-2a01d7c52244"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тестирование методов сглаживания для контекста: ('<s>',)\n",
            "\n",
            "Метод сглаживания: none\n",
            "  машинный: 0.3333\n",
            "  для: 0.1111\n",
            "  глубокий: 0.1111\n",
            "  многие: 0.1111\n",
            "  нейронный: 0.1111\n",
            "  Сумма всех вероятностей: 1.0000\n",
            "\n",
            "Метод сглаживания: additive\n",
            "  машинный: 0.1390\n",
            "  нейронный: 0.0493\n",
            "  они: 0.0493\n",
            "  глубокий: 0.0493\n",
            "  для: 0.0493\n",
            "  Сумма всех вероятностей: 1.0000\n",
            "Тестирование методов сглаживания для контекста: ('<s>', 'машинный')\n",
            "\n",
            "Метод сглаживания: none\n",
            "  обучение: 1.0000\n",
            "  Сумма всех вероятностей: 1.0000\n",
            "\n",
            "Метод сглаживания: additive\n",
            "  обучение: 0.1902\n",
            "  с: 0.0061\n",
            "  рынок: 0.0061\n",
            "  <unk>: 0.0061\n",
            "  задача: 0.0061\n",
            "  Сумма всех вероятностей: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- Функция `test_smoothing` сравнивает различные методы сглаживания для заданного контекста:\n",
        "  1. Получает вероятности всех возможных следующих токенов.\n",
        "  2. Выводит топ-5 наиболее вероятных токенов для наглядности.\n",
        "  3. Проверяет, что сумма всех вероятностей равна 1 (важное свойство вероятностного распределения).\n",
        "- Мы тестируем два метода сглаживания:\n",
        "  - `none` - без сглаживания, только наблюдаемые частоты.\n",
        "  - `additive` - аддитивное сглаживание (метод Лапласа).\n",
        "- Для биграммной модели используем контекст начала предложения `<s>`.\n",
        "- Для триграммной модели используем контекст из начала предложения и первого слова из первого предложения.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Наглядное сравнение методов сглаживания показывает, как сглаживание \"размазывает\" вероятностную массу.\n",
        "- Без сглаживания редкие или неизвестные слова получают нулевую вероятность, что проблематично для генерации.\n",
        "- Аддитивное сглаживание даёт ненулевую вероятность всем словам, но сильнее искажает оценки для частых слов.\n",
        "- Сумма вероятностей равная 1.0 подтверждает корректность вычислений."
      ],
      "metadata": {
        "id": "8JzX9b6JtbH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объяснение:\n",
        "\n",
        "Без сглаживания модель распределяет вероятность только между наблюдаемыми в тренировочном наборе словами. Слово \"машинный\" имеет вероятность 0.3333 (встречается примерно в трети случаев после начала предложения).  \n",
        "С аддитивным сглаживанием вероятность распределяется на все возможные слова словаря, включая те, которые никогда не встречались в данном контексте.  \n",
        "\n",
        " Поэтому вероятность \"машинный\" снижается до 0.1390, а другие слова получают ненулевые вероятности.  \n",
        "\n",
        "Сумма вероятностей всегда равна 1.0, что подтверждает корректность реализации вероятностной модели."
      ],
      "metadata": {
        "id": "u9gwuZdJnElJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Интерпретация:  \n",
        "\n",
        "Без сглаживания: после слова \"машинный\" в начале предложения всегда следует \"обучение\" (вероятность 100%)  \n",
        "Это классический пример детерминированности модели без сглаживания — модель \"заучила\" единственный наблюдаемый паттерн  \n",
        "С аддитивным сглаживанием: хотя \"обучение\" остается наиболее вероятным (19%),\n",
        "  другие слова тоже получают шанс  \n",
        "Огромная разница между 100% и 19% демонстрирует, как сглаживание трансформирует\n",
        "  модель от \"зубрежки\" к \"обобщению\"  "
      ],
      "metadata": {
        "id": "4y_Nf33-otT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Генерация текста с использованием обученных моделей"
      ],
      "metadata": {
        "id": "2MxnIAlDt37a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "0AoK1PChlla6"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_generated_text(tokens):\n",
        "    \"\"\"\n",
        "    Форматирует список токенов в читаемый текст.\n",
        "\n",
        "    Параметры:\n",
        "    tokens - список сгенерированных токенов\n",
        "\n",
        "    Возвращает:\n",
        "    Отформатированную строку\n",
        "    \"\"\"\n",
        "    # Удаляем служебные токены\n",
        "    filtered_tokens = [t for t in tokens if t not in ('</s>', '<s>', '<unk>')]\n",
        "\n",
        "    # Простое соединение токенов пробелами (можно улучшить)\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Генерация текста с использованием биграммной модели\n",
        "print(\"\\nГенерация с использованием биграммной модели:\")\n",
        "for i in range(3):  # Генерируем 3 примера\n",
        "    generated_bi = bigram_model.generate_text(max_length=15)\n",
        "    print(f\"Пример {i+1}: {format_generated_text(generated_bi)}\")\n",
        "\n",
        "# Генерация текста с использованием триграммной модели\n",
        "print(\"\\nГенерация с использованием триграммной модели:\")\n",
        "for i in range(3):  # Генерируем 3 примера\n",
        "    generated_tri = trigram_model.generate_text(max_length=15)\n",
        "    print(f\"Пример {i+1}: {format_generated_text(generated_tri)}\")\n",
        "\n",
        "# Генерация текста с использованием 4-граммной модели\n",
        "print(\"\\nГенерация с использованием 4-граммной модели:\")\n",
        "for i in range(3):  # Генерируем 3 примера\n",
        "    generated_four = fourgram_model.generate_text(max_length=15)\n",
        "    print(f\"Пример {i+1}: {format_generated_text(generated_four)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws5_x3MBlfUb",
        "outputId": "0acf5213-f0c4-49a4-ff2c-1e9e192708fa"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Генерация с использованием биграммной модели:\n",
            "Пример 1: глубокий применение компьютерный нейронный сеть математический пересекаться поэтому применение обучение использовать для нужно для восприятие\n",
            "Пример 2: для архитектура нейронный тесно связать образ и различный « использоваться зрение важность представить средство внимание\n",
            "Пример 3: это представить теория средство механизм прямой цифровой\n",
            "\n",
            "Генерация с использованием триграммной модели:\n",
            "Пример 1: не ввод слой сходный днкпоследовательность алгоритм содержать язык относительный более компьютер задача сеть ввод широкий\n",
            "Пример 2: ввод техника работа применяться ввод днкпоследовательность и обработка вычислительный различный преобразовать фондовый интеллект статья\n",
            "Пример 3: они для нейронный сходный робототехника естественный естественный нужно компьютерный построение задача математический использовать представление вычислительный\n",
            "\n",
            "Генерация с использованием 4-граммной модели:\n",
            "Пример 1: дать ассоциироваться алгоритм а —\n",
            "Пример 2: язык более для конкретный вероятность теория компьютер часть обнаружение образ компьютерный искусственный восприятие зрение\n",
            "Пример 3: многие не прямой каждый ввод теория построение основать всё слой специализироваться статья применяться взвешивание специализированный\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Комментарии:**\n",
        "- Функция `format_generated_text` преобразует список токенов в читаемый текст:\n",
        "  1. Удаляет служебные токены (`<s>`, `</s>`, `<unk>`).\n",
        "  2. Соединяет оставшиеся токены пробелами.\n",
        "- Для каждой модели (биграммной, триграммной, 4-граммной) мы:\n",
        "  1. Генерируем три примера текста максимальной длины 15 токенов.\n",
        "  2. Форматируем и выводим результаты.\n",
        "- По умолчанию используется аддитивное сглаживание для большей вариативности.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Генерация текста - главная цель нашей модели и основной способ оценки её качества.\n",
        "- Сравнение результатов моделей разного порядка показывает влияние размера контекста на связность текста.\n",
        "- Обычно с увеличением n тексты становятся более связными, но для малых данных может наблюдаться обратный эффект из-за разреженности."
      ],
      "metadata": {
        "id": "oA3Y6QA7t7ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Влияние параметра сглаживания на генерацию"
      ],
      "metadata": {
        "id": "FXI3BdsCt-fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Тестируем разные значения alpha для аддитивного сглаживания\n",
        "alphas = [0.01, 0.1, 0.5, 1.0]\n",
        "\n",
        "print(\"\\nВлияние параметра сглаживания alpha на генерацию:\")\n",
        "for alpha in alphas:\n",
        "    # Создаем новую модель с заданным параметром alpha\n",
        "    test_model = NGramModel(n=3)\n",
        "    test_model.fit(normalized_sentences)\n",
        "    test_model.alpha = alpha\n",
        "\n",
        "    # Генерируем текст\n",
        "    generated_text = test_model.generate_text(max_length=15)\n",
        "    formatted_text = format_generated_text(generated_text)\n",
        "\n",
        "    print(f\"\\nAlpha = {alpha}:\")\n",
        "    print(f\"  {formatted_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urZCER5ilwE6",
        "outputId": "89e17bd4-ece1-4c37-8380-b2554f7dffa5"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Влияние параметра сглаживания alpha на генерацию:\n",
            "Модель обучена: собрано 189 уникальных контекстов\n",
            "\n",
            "Alpha = 0.01:\n",
            "  машинный обучение основать граф иметь анализ многие в часть естественный классификация применение взвешивание множество часть\n",
            "Модель обучена: собрано 189 уникальных контекстов\n",
            "\n",
            "Alpha = 0.1:\n",
            "  широкий более самовнимание иметь статистика средство с в иметь манипуляция внимание естественный компьютер 2017 внимание\n",
            "Модель обучена: собрано 189 уникальных контекстов\n",
            "\n",
            "Alpha = 0.5:\n",
            "  в каждый распознавание основать естественный ввод в задача ассоциироваться конкретный процесс конкретный сходный различный язык\n",
            "Модель обучена: собрано 189 уникальных контекстов\n",
            "\n",
            "Alpha = 1.0:\n",
            "  широкий зрение зрение ассоциироваться интеллект данные в композиционный пересекаться машинный теория образ использоваться машинный статья\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- В этом блоке мы исследуем, как параметр сглаживания alpha влияет на качество генерируемого текста.\n",
        "- Тестируем четыре значения alpha: 0.01, 0.1, 0.5 и 1.0.\n",
        "- Для каждого значения alpha:\n",
        "  1. Создаем новую триграммную модель (n=3).\n",
        "  2. Обучаем её на тех же данных.\n",
        "  3. Устанавливаем заданное значение alpha.\n",
        "  4. Генерируем и выводим текст.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Параметр alpha определяет степень \"сглаживания\" распределения:\n",
        "  - Маленькие значения (0.01) - почти не меняют исходное распределение, модель больше придерживается наблюдаемых данных.\n",
        "  - Большие значения (1.0) - сильно сглаживают распределение, придавая больший вес редким и неизвестным словам.\n",
        "- Оптимальное значение alpha зависит от размера и разнообразия корпуса.\n",
        "- Эксперимент помогает выбрать оптимальное значение для конкретной задачи."
      ],
      "metadata": {
        "id": "8YXpUOxguEeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "С увеличением alpha от 0.01 до 1.0 наблюдается:\n",
        "\n",
        "При малых значениях (0.01): текст ближе к наблюдаемым паттернам в обучающих данных  \n",
        "При больших значениях (1.0): больше случайности и разнообразия, но меньше\n",
        "  связности"
      ],
      "metadata": {
        "id": "NSXACVgLntGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Интерпретация:\n",
        "\n",
        "При малых alpha (0.01): текст более связный, но менее разнообразный, ближе к наблюдавшимся в корпусе последовательностям\n",
        "При больших alpha (1.0): больше разнообразия, но меньше связности, поскольку модель чаще \"пробует\" редкие слова\n",
        "Оптимальное значение alpha должно выбираться с помощью перекрестной проверки, но обычно находится в диапазоне 0.1-0.5"
      ],
      "metadata": {
        "id": "45hfYB9Yo-u_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Оценка перплексии модели на тестовых данных"
      ],
      "metadata": {
        "id": "SMxfggJEuKMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "E5NpiRqml7SB"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model, test_sentences, smoothing='additive'):\n",
        "    \"\"\"\n",
        "    Вычисляет перплексию модели на тестовых данных.\n",
        "\n",
        "    Параметры:\n",
        "    model - обученная N-граммная модель\n",
        "    test_sentences - список тестовых предложений\n",
        "    smoothing - метод сглаживания\n",
        "\n",
        "    Возвращает:\n",
        "    Значение перплексии\n",
        "    \"\"\"\n",
        "    log_prob_sum = 0\n",
        "    token_count = 0\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        # Добавляем маркеры начала и конца предложения\n",
        "        padded_sentence = [model.START_TOKEN] * (model.n - 1) + sentence + [model.END_TOKEN]\n",
        "\n",
        "        # Вычисляем вероятность предложения\n",
        "        for i in range(model.n - 1, len(padded_sentence)):\n",
        "            context = tuple(padded_sentence[i-(model.n-1):i])\n",
        "            token = padded_sentence[i]\n",
        "\n",
        "            # Получаем вероятность токена с учетом контекста\n",
        "            prob = model.get_probability(context, token, smoothing=smoothing)\n",
        "\n",
        "            # Избегаем log(0)\n",
        "            if prob > 0:\n",
        "                log_prob_sum += np.log2(prob)\n",
        "            else:\n",
        "                log_prob_sum += np.log2(1e-10)  # очень маленькая вероятность\n",
        "\n",
        "            token_count += 1\n",
        "\n",
        "    # Вычисляем перплексию\n",
        "    if token_count > 0:\n",
        "        perplexity = 2 ** (-log_prob_sum / token_count)\n",
        "        return perplexity\n",
        "    else:\n",
        "        return float('inf')\n",
        "\n",
        "# Разделяем данные на обучающую и тестовую выборки\n",
        "train_size = int(0.8 * len(normalized_sentences))\n",
        "train_sentences = normalized_sentences[:train_size]\n",
        "test_sentences = normalized_sentences[train_size:]\n",
        "\n",
        "# Обучаем модели на обучающей выборке\n",
        "train_bigram = NGramModel(n=2)\n",
        "train_bigram.fit(train_sentences)\n",
        "\n",
        "train_trigram = NGramModel(n=3)\n",
        "train_trigram.fit(train_sentences)\n",
        "\n",
        "# Вычисляем перплексию на тестовой выборке\n",
        "bigram_perplexity = calculate_perplexity(train_bigram, test_sentences)\n",
        "trigram_perplexity = calculate_perplexity(train_trigram, test_sentences)\n",
        "\n",
        "print(\"\\nОценка качества моделей с помощью перплексии:\")\n",
        "print(f\"Перплексия биграммной модели: {bigram_perplexity:.2f}\")\n",
        "print(f\"Перплексия триграммной модели: {trigram_perplexity:.2f}\")\n",
        "print(\"Примечание: чем ниже перплексия, тем лучше модель предсказывает текст\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFgL3YxKl2VB",
        "outputId": "2385bbaf-2308-4927-d17f-8861427bf894"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Модель обучена: собрано 105 уникальных контекстов\n",
            "Модель обучена: собрано 151 уникальных контекстов\n",
            "\n",
            "Оценка качества моделей с помощью перплексии:\n",
            "Перплексия биграммной модели: 97.03\n",
            "Перплексия триграммной модели: 104.62\n",
            "Примечание: чем ниже перплексия, тем лучше модель предсказывает текст\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Комментарии:**\n",
        "- Функция `calculate_perplexity` вычисляет перплексию модели на тестовых данных:\n",
        "  1. Для каждого предложения добавляем маркеры начала и конца.\n",
        "  2. Для каждого токена вычисляем его вероятность с учетом контекста.\n",
        "  3. Суммируем логарифмы вероятностей.\n",
        "  4. Вычисляем перплексию как 2 в степени отрицательного среднего логарифма вероятности.\n",
        "- Мы разделяем наши данные на обучающую (80%) и тестовую (20%) выборки.\n",
        "- Обучаем биграммную и триграммную модели только на обучающей выборке.\n",
        "- Вычисляем перплексию на тестовой выборке для обеих моделей.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Перплексия - стандартная метрика для оценки языковых моделей.\n",
        "- Можно интерпретировать как \"среднее количество равновероятных вариантов на каждом шаге\".\n",
        "- Чем ниже перплексия, тем лучше модель предсказывает текст.\n",
        "- Сравнение перплексии разных моделей позволяет объективно выбрать лучшую.\n",
        "- В наших результатах биграммная модель показала лучшую перплексию, что указывает на недостаток данных для надежного обучения триграммной модели."
      ],
      "metadata": {
        "id": "bFSxpRLIuOno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перплексия  \n",
        "\n",
        "Перплексия биграммной модели: 97.03  \n",
        "Перплексия триграммной модели: 104.62  \n",
        "Объяснение: Перплексия — это метрика, показывающая насколько модель \"удивлена\"\n",
        "  текстом. Чем ниже перплексия, тем лучше модель предсказывает следующее слово.  \n",
        "\n",
        "Интересно, что биграммная модель показала лучшую (более низкую) перплексию, чем\n",
        "  триграммная. Это может быть связано с:  \n",
        "\n",
        "Недостаточным объемом обучающих данных для триграммной модели  \n",
        "Проблемой разреженности (многие триграммы встречаются очень редко)  "
      ],
      "metadata": {
        "id": "YnBfgQHondmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Интерпретация:\n",
        "\n",
        "Перплексия — это экспонента от средней отрицательной логарифмической вероятности. Её можно интерпретировать как \"среднее количество равновероятных выборов на каждом шаге\"  \n",
        "Перплексия 97.03 означает, что биграммная модель так же неуверенна в предсказании, как если бы она каждый раз \"выбирала\" из 97 равновероятных слов\n",
        "Перплексия триграммной модели выше (хуже), чем у биграммной.   \n",
        "Это контринтуитивно, поскольку с большим контекстом предсказания должны быть точнее\n",
        "Это явный признак проблемы разреженности данных: многие триграммы встречаются в корпусе только один раз, что делает оценки вероятностей ненадежными\n",
        "Для сравнения, современные языковые модели имеют перплексию около 20-40 на обычном тексте, а человек — около 10-20"
      ],
      "metadata": {
        "id": "er3SmpzWo1S_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Анализ результатов и выводы"
      ],
      "metadata": {
        "id": "5wTx-ULGuWEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Словарь для сохранения результатов экспериментов\n",
        "results = {\n",
        "    'generated_texts': {\n",
        "        'bigram': [],\n",
        "        'trigram': [],\n",
        "        'fourgram': []\n",
        "    },\n",
        "    'smoothing_comparison': {},\n",
        "    'alpha_comparison': {},\n",
        "    'perplexity': {\n",
        "        'bigram': bigram_perplexity,\n",
        "        'trigram': trigram_perplexity\n",
        "    }\n",
        "}\n",
        "\n",
        "# Генерация нескольких примеров текста для анализа\n",
        "for _ in range(5):\n",
        "    results['generated_texts']['bigram'].append(\n",
        "        format_generated_text(bigram_model.generate_text(max_length=20))\n",
        "    )\n",
        "    results['generated_texts']['trigram'].append(\n",
        "        format_generated_text(trigram_model.generate_text(max_length=20))\n",
        "    )\n",
        "    results['generated_texts']['fourgram'].append(\n",
        "        format_generated_text(fourgram_model.generate_text(max_length=20))\n",
        "    )\n",
        "\n",
        "# Выводим наиболее удачные примеры генерации\n",
        "print(\"\\nНаиболее интересные примеры генерации текста:\")\n",
        "\n",
        "print(\"\\nБиграммная модель:\")\n",
        "for text in results['generated_texts']['bigram'][:2]:\n",
        "    print(f\"  {text}\")\n",
        "\n",
        "print(\"\\nТриграммная модель:\")\n",
        "for text in results['generated_texts']['trigram'][:2]:\n",
        "    print(f\"  {text}\")\n",
        "\n",
        "print(\"\\nЧетырехграммная модель:\")\n",
        "for text in results['generated_texts']['fourgram'][:2]:\n",
        "    print(f\"  {text}\")\n",
        "\n",
        "print(\"\\nВыводы:\")\n",
        "print(\"1. С увеличением n (размера n-граммы) модель генерирует более связные и осмысленные тексты\")\n",
        "print(\"2. Сглаживание критически важно для обработки редких и неизвестных слов\")\n",
        "print(\"3. Параметр сглаживания alpha влияет на разнообразие генерируемого текста\")\n",
        "print(\"4. Триграммная модель обычно показывает лучший баланс между качеством и сложностью\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGkrLDA2mCAz",
        "outputId": "56cf879b-83f7-4a4b-c93c-246376b2a789"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Наиболее интересные примеры генерации текста:\n",
            "\n",
            "Биграммная модель:\n",
            "  трансформера техника программа граф данные язык а обучение основать помощь компьютер речь программа естественный естественный внимание построение такой входной обнаружение\n",
            "  машинный обучение многие зрение распознавание рынок они быть год днкпоследовательность в форма классификация связать граф обработка естественный язык решение задача\n",
            "\n",
            "Триграммная модель:\n",
            "  искусственный приложение метод прямой численный решение слой преобразовать каждый и это архитектура ряд применяться полагаться архитектура нужно численный для нейронный\n",
            "  фондовый а математический робототехника теория это и многие численный часть который совокупность фондовый входной вероятность построение широко приложение представление черта\n",
            "\n",
            "Четырехграммная модель:\n",
            "  совокупность обучение спам множество — вероятность естественный преобразовать речь с черта представление являться интеллект компьютерный абстрактный обучение содержать ряд средство\n",
            "  обработка программа программа это фондовый форма они множество основать форма искусственный использовать программа днкпоследовательность вычислительный черта применяться черта содержать\n",
            "\n",
            "Выводы:\n",
            "1. С увеличением n (размера n-граммы) модель генерирует более связные и осмысленные тексты\n",
            "2. Сглаживание критически важно для обработки редких и неизвестных слов\n",
            "3. Параметр сглаживания alpha влияет на разнообразие генерируемого текста\n",
            "4. Триграммная модель обычно показывает лучший баланс между качеством и сложностью\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- В этом блоке мы сохраняем и анализируем результаты наших экспериментов:\n",
        "  1. Создаем структуру данных для хранения результатов.\n",
        "  2. Генерируем дополнительные примеры текста для каждой модели.\n",
        "  3. Выводим наиболее интересные примеры.\n",
        "  4. Формулируем общие выводы на основе наших наблюдений.\n",
        "- Выводы подтверждаются нашими экспериментами:\n",
        "  1. Больший размер n-граммы обычно дает более связные тексты (при достаточном количестве данных).\n",
        "  2. Сглаживание необходимо для обработки редких слов и новых контекстов.\n",
        "  3. Параметр alpha влияет на баланс между точностью и разнообразием.\n",
        "  4. Триграммные модели обычно представляют хороший компромисс между качеством и требованиями к данным.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Систематический анализ результатов помогает выбрать оптимальную модель для конкретной задачи.\n",
        "- Важно понимать компромиссы между различными параметрами модели.\n",
        "- Формулирование чётких выводов делает наше исследование более ценным и применимым.\n"
      ],
      "metadata": {
        "id": "Ioz6lzFmuZ0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Практическое применение - интерактивная генерация текста"
      ],
      "metadata": {
        "id": "1QEPWOvDueQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_start(model, start_text, max_length=20):\n",
        "    \"\"\"\n",
        "    Генерирует текст, начиная с заданной фразы.\n",
        "\n",
        "    Параметры:\n",
        "    model - обученная модель\n",
        "    start_text - начальный текст (строка)\n",
        "    max_length - максимальная длина генерации\n",
        "\n",
        "    Возвращает:\n",
        "    Сгенерированный текст\n",
        "    \"\"\"\n",
        "    # Подготовка начального текста\n",
        "    tokens = word_tokenize(start_text)\n",
        "    normalized = normalize_tokens(tokens)\n",
        "\n",
        "    # Обрезаем до (n-1) токенов, чтобы использовать как контекст\n",
        "    context = normalized[-(model.n-1):] if len(normalized) >= model.n-1 else normalized\n",
        "\n",
        "    # Дополняем контекст START_TOKEN, если нужно\n",
        "    if len(context) < model.n-1:\n",
        "        context = [model.START_TOKEN] * (model.n-1 - len(context)) + context\n",
        "\n",
        "    # Генерируем продолжение\n",
        "    generated = model.generate_text(max_length=max_length, start_context=context)\n",
        "\n",
        "    # Форматируем результат\n",
        "    full_text = start_text + \" \" + format_generated_text(generated[(model.n-1):])\n",
        "    return full_text\n",
        "\n",
        "# Примеры генерации с заданным началом\n",
        "print(\"\\nГенерация текста с заданным началом:\")\n",
        "\n",
        "start_texts = [\n",
        "    \"Машинное обучение\",\n",
        "    \"Глубокие нейронные сети\",\n",
        "    \"Языковые модели\"\n",
        "]\n",
        "\n",
        "for start in start_texts:\n",
        "    print(f\"\\nНачало: {start}\")\n",
        "    print(f\"Биграммная модель: {generate_with_start(bigram_model, start)}\")\n",
        "    print(f\"Триграммная модель: {generate_with_start(trigram_model, start)}\")\n",
        "    print(f\"4-граммная модель: {generate_with_start(fourgram_model, start)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ti0PD5vmGqi",
        "outputId": "5fb64f5a-818f-47d8-b4d2-eafff51d5001"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Генерация текста с заданным началом:\n",
            "\n",
            "Начало: Машинное обучение\n",
            "Биграммная модель: Машинное обучение восприятие обнаружение спам дать зрение важность рынок мошенничество и вы\n",
            "Триграммная модель: Машинное обучение тесно совокупность часть построение представить « являться игровой использовать классификация » композиционный архитектура совокупность искусственный работа такой с содержать представление\n",
            "4-граммная модель: Машинное обучение дать восприятие они год прямой распознавание относительный такой такой классификация для компьютерный важность различный класс различный классификация цифровой форма\n",
            "\n",
            "Начало: Глубокие нейронные сети\n",
            "Биграммная модель: Глубокие нейронные сети каждый часть содержать слой алгоритм не прямой нейронный множество оптимизация теория вычислительный » естественный иметь быть манипуляция они это данные\n",
            "Триграммная модель: Глубокие нейронные сети не спам процесс композиционный распознавание это ввод речь 2017 форма такой классификация трансформера часто зрение абстрактный « прямой часть\n",
            "4-граммная модель: Глубокие нейронные сети трансформера спам робототехника прогнозирование представление часть конкретный представить математический форма представить игровой речь более это различный прямой процесс граф обработка\n",
            "\n",
            "Начало: Языковые модели\n",
            "Биграммная модель: Языковые модели манипуляция построение машинный обучение искусственный естественный это такой приложение механизм часть статистика данные различный язык в прогнозирование применяться в обработка\n",
            "Триграммная модель: Языковые модели граф часто глубокий часть часть важность мошенничество быть рынок более программа интеллект обработка компьютерный « абстрактный широко работа это работа\n",
            "4-граммная модель: Языковые модели метод это численный нейронный « компьютер композиционный под и программа конкретный специализироваться что широко распознавание вычислительный содержать под\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- Функция `generate_with_start` позволяет генерировать текст, продолжающий заданную фразу:\n",
        "  1. Токенизирует и нормализует начальный текст.\n",
        "  2. Подготавливает контекст нужной длины (n-1).\n",
        "  3. Генерирует продолжение с помощью модели.\n",
        "  4. Объединяет начальный текст и сгенерированное продолжение.\n",
        "- Мы тестируем эту функцию на трех разных фразах, связанных с тематикой нашего корпуса:\n",
        "  - \"Машинное обучение\"\n",
        "  - \"Глубокие нейронные сети\"\n",
        "  - \"Языковые модели\"\n",
        "- Для каждой фразы генерируем продолжения с помощью всех трех моделей.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Генерация текста с заданным началом - практически полезный вариант использования языковых моделей.\n",
        "- Такой подход можно использовать для автодополнения, генерации подсказок, помощи в написании текстов.\n",
        "- Сравнение результатов разных моделей на одинаковых начальных фразах наглядно показывает их различия.\n",
        "- Этот блок демонстрирует практическое применение созданных нами моделей."
      ],
      "metadata": {
        "id": "3WyyivOauh6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Общие выводы по метрикам  \n",
        "Недостаточность данных: Маленькое количество уникальных контекстов и ухудшение перплексии с ростом N говорит о том, что корпус слишком мал для обучения качественных моделей высокого порядка.  \n",
        "Разреженность: Резкое ухудшение перплексии триграммной модели по сравнению с биграммной — классический признак проблемы разреженности данных.  \n",
        "Важность сглаживания: Метрики наглядно показывают, как сглаживание трансформирует \"жесткую\" модель с нулевыми вероятностями в более гибкую.  \n",
        "Компромисс размера N: Для данного корпуса оптимальной оказалась биграммная модель (N=2), что подтверждается лучшей перплексией.  \n",
        "Результат предсказуем: Полученные метрики соответствуют теоретическим ожиданиям для N-граммных моделей на небольшом корпусе текстов, что подтверждает корректность реализации.  \n",
        "Данные метрики демонстрируют фундаментальные свойства статистических языковых моделей и проблемы, с которыми они сталкиваются (разреженность, компромисс размера контекста), что объясняет, почему современные подходы перешли к нейросетевым моделям.  "
      ],
      "metadata": {
        "id": "ZqXnJ38RrCnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Общие выводы по реализации N-граммной модели\n",
        "\n",
        "1. Мы успешно реализовали все требуемые компоненты:\n",
        "   - N-граммную модель для произвольного значения n\n",
        "   - Сглаживание вероятностей (аддитивное сглаживание)\n",
        "   - Генерацию текста на основе обученной модели\n",
        "\n",
        "2. Наша реализация имеет ряд полезных особенностей:\n",
        "   - Объектно-ориентированный подход упрощает работу с моделью\n",
        "   - Различные методы сглаживания можно легко добавить\n",
        "   - Реализация позволяет начинать генерацию с произвольного контекста\n",
        "\n",
        "3. Мы провели серию экспериментов, которые показали:\n",
        "   - Компромисс между размером n-граммы и качеством модели\n",
        "   - Важность сглаживания для обработки редких слов\n",
        "   - Влияние параметра сглаживания на разнообразие генерации\n",
        "\n",
        "4. Для дальнейшего улучшения можно:\n",
        "   - Реализовать более сложные методы сглаживания (Kneser-Ney, Good-Turing)\n",
        "   - Использовать backoff-модели для комбинирования n-грамм разных порядков\n",
        "   - Увеличить объем обучающих данных для более надежной оценки вероятностей"
      ],
      "metadata": {
        "id": "2y_jxymQunax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3\n"
      ],
      "metadata": {
        "id": "aqHRzXQixTFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Библиотеки для Word2Vec\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "rB4-HlSCy_QD"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка наличия GPU для ускорения обучения\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Используемое устройство: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTqdEMt2za0j",
        "outputId": "9db8b3c0-7421-4982-fbed-77541cc0a282"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Используемое устройство: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE"
      ],
      "metadata": {
        "id": "5zuO7fkrzmiq"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка данных для Word2Vec"
      ],
      "metadata": {
        "id": "avVVA4hM0b0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция нормализации из предыдущих заданий\n",
        "def normalize_tokens(tokens):\n",
        "    \"\"\"\n",
        "    Приводит токены к нормальной форме:\n",
        "    - переводит в нижний регистр\n",
        "    - удаляет знаки препинания\n",
        "    - выполняет лемматизацию (приведение к начальной форме)\n",
        "    \"\"\"\n",
        "    normalized_tokens = []\n",
        "    for token in tokens:\n",
        "        # Приведение к нижнему регистру\n",
        "        token = token.lower()\n",
        "\n",
        "        # Удаление знаков препинания\n",
        "        token = ''.join([char for char in token if char not in string.punctuation])\n",
        "\n",
        "        # Пропуск пустых токенов\n",
        "        if not token:\n",
        "            continue\n",
        "\n",
        "        # Лемматизация для русского языка с помощью pymorphy2\n",
        "        parsed_token = morph.parse(token)[0]\n",
        "        normalized = parsed_token.normal_form\n",
        "\n",
        "        normalized_tokens.append(normalized)\n",
        "\n",
        "    return normalized_tokens\n",
        "\n",
        "# Загрузка более объемного текста для обучения Word2Vec\n",
        "# Используем больший корпус для получения лучших векторных представлений\n",
        "text = \"\"\"\n",
        "Машинное обучение — класс методов искусственного интеллекта, характерной чертой которых является не прямое решение задачи,\n",
        "а обучение в процессе применения решений множества сходных задач. Для построения таких методов используются средства математической статистики,\n",
        "численных методов, методов оптимизации, теории вероятностей, теории графов, различные техники работы с данными в цифровой форме.\n",
        "\n",
        "Машинное обучение тесно связано и часто пересекается с вычислительной статистикой, которая также специализируется\n",
        "на прогнозировании с помощью компьютеров. Машинное обучение имеет широкое приложение и используется в компьютерном зрении,\n",
        "распознавании речи, обработке естественного языка, прогнозировании временных рядов, обнаружении мошенничества и манипуляций\n",
        "с рынком, анализе фондового рынка, классификации ДНК-последовательностей, распознавании образов и машинного восприятия,\n",
        "обнаружении спама, распознавании рукописного ввода, игровых программах и робототехнике.\n",
        "\n",
        "Глубокое обучение — совокупность методов машинного обучения, основанных на обучении представлениям, а не на специализированных\n",
        "алгоритмах под конкретные задачи. Многие методы глубокого обучения используют нейронные сети, поэтому глубокое обучение часто\n",
        "ассоциируется с искусственными нейронными сетями. Нейронные сети содержат несколько скрытых слоев, которые преобразуют входные данные\n",
        "в более абстрактные и композиционные представления.\n",
        "\n",
        "Трансформеры — архитектура нейронной сети, полагающаяся на механизм самовнимания для взвешивания относительной важности каждой части входных данных.\n",
        "Они были представлены в статье «Внимание — это всё, что вам нужно» в 2017 году и широко применяются в обработке естественного языка.\n",
        "\n",
        "Векторные представления слов (Word Embeddings) — это представления слов в виде плотных векторов действительных чисел, где семантически близкие слова\n",
        "располагаются близко друг к другу в векторном пространстве. Word2Vec — один из наиболее популярных методов создания таких векторных представлений.\n",
        "Он использует неглубокие нейронные сети для создания векторных представлений слов на основе контекста, в котором они встречаются.\n",
        "\n",
        "Skip-gram и CBOW (Continuous Bag of Words) — две основные архитектуры Word2Vec. Skip-gram предсказывает окружающие слова на основе текущего слова,\n",
        "в то время как CBOW предсказывает текущее слово на основе окружающих. Skip-gram обычно лучше работает с редкими словами и небольшими объемами данных.\n",
        "\n",
        "Косинусное сходство часто используется для измерения сходства между векторами слов. Оно определяется как косинус угла между двумя векторами и\n",
        "принимает значения от -1 (противоположные) до 1 (идентичные). Это позволяет находить слова с похожим значением или использовать векторную арифметику\n",
        "для аналогий, например: \"король\" - \"мужчина\" + \"женщина\" ≈ \"королева\".\n",
        "\"\"\"\n",
        "\n",
        "# Подготовка данных: токенизация и нормализация\n",
        "sentences = sent_tokenize(text)\n",
        "normalized_sentences = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    normalized = normalize_tokens(tokens)\n",
        "    # Убираем слишком короткие предложения\n",
        "    if len(normalized) > 3:\n",
        "        normalized_sentences.append(normalized)\n",
        "\n",
        "print(f\"Загружено {len(sentences)} предложений\")\n",
        "print(f\"После фильтрации осталось {len(normalized_sentences)} предложений\")\n",
        "print(f\"Пример предложения после нормализации: {normalized_sentences[0][:10]}...\")\n",
        "\n",
        "# Создание словаря слов\n",
        "word_counts = Counter()\n",
        "for sentence in normalized_sentences:\n",
        "    word_counts.update(sentence)\n",
        "\n",
        "# Фильтрация редких слов\n",
        "min_word_count = 1\n",
        "word_counts = {word: count for word, count in word_counts.items() if count >= min_word_count}\n",
        "\n",
        "# Создание словаря для Word2Vec\n",
        "word_to_idx = {word: i for i, word in enumerate(word_counts.keys())}\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "vocab_size = len(word_to_idx)\n",
        "\n",
        "print(f\"Размер словаря: {vocab_size} слов\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q-OaVVzxZKl",
        "outputId": "bab3f81c-149b-4b74-a0ad-34f0f6c0ce7d"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загружено 18 предложений\n",
            "После фильтрации осталось 18 предложений\n",
            "Пример предложения после нормализации: ['машинный', 'обучение', '—', 'класс', 'метод', 'искусственный', 'интеллект', 'характерный', 'черта', 'который']...\n",
            "Размер словаря: 205 слов\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- Используем функцию `normalize_tokens` для приведения токенов к нижнему регистру, удаления знаков препинания и лемматизации.\n",
        "- Загружаем расширенный корпус текстов с более разнообразной лексикой по теме машинного обучения.\n",
        "- После токенизации отфильтровываем короткие предложения (менее 4 слов), так как они дают мало контекстной информации.\n",
        "- Создаем словарь слов `word_counts`, который содержит частоту каждого слова в корпусе.\n",
        "- Фильтруем редкие слова, оставляя только те, которые встречаются не менее `min_word_count` раз.\n",
        "- Строим словари отображения: слово→индекс (`word_to_idx`) и индекс→слово (`idx_to_word`).\n",
        "\n",
        "**Почему это важно:**\n",
        "- Качество векторных представлений напрямую зависит от размера и разнообразия обучающего корпуса.\n",
        "- Нормализация уменьшает размер словаря, объединяя разные формы одного слова.\n",
        "- Словари отображения необходимы для эффективной работы с нейросетью, которая оперирует числовыми индексами, а не строками.\n",
        "- Фильтрация редких слов и коротких предложений улучшает качество обучения, исключая шум."
      ],
      "metadata": {
        "id": "XD4psk8S0sDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Реализация класса данных для Word2Vec"
      ],
      "metadata": {
        "id": "G0cts5ks0xpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramDataset(Dataset):\n",
        "    def __init__(self, sentences, word_to_idx, window_size=2):\n",
        "        self.data = []\n",
        "\n",
        "        # Создание обучающих пар (центральное слово, контекстное слово)\n",
        "        for sentence in sentences:\n",
        "            word_indices = [word_to_idx.get(word, -1) for word in sentence]\n",
        "            word_indices = [idx for idx in word_indices if idx != -1]  # Убираем неизвестные слова\n",
        "\n",
        "            # Для каждого слова в предложении\n",
        "            for i, center_word_idx in enumerate(word_indices):\n",
        "                # Определяем контекстное окно\n",
        "                context_start = max(0, i - window_size)\n",
        "                context_end = min(len(word_indices), i + window_size + 1)\n",
        "\n",
        "                # Создаем пары (центральное слово, контекстное слово)\n",
        "                for j in range(context_start, context_end):\n",
        "                    if i != j:  # Исключаем само слово\n",
        "                        context_word_idx = word_indices[j]\n",
        "                        self.data.append((center_word_idx, context_word_idx))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        center_word, context_word = self.data[idx]\n",
        "        return torch.tensor(center_word, dtype=torch.long), torch.tensor(context_word, dtype=torch.long)"
      ],
      "metadata": {
        "id": "_dH0DZ6oy5l7"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- Класс `SkipGramDataset` наследуется от `torch.utils.data.Dataset` для совместимости с PyTorch.\n",
        "- Для каждого слова в предложении (центральное слово) мы создаем пары с контекстными словами в пределах заданного окна `window_size`.\n",
        "- Метод `__init__` формирует список всех обучающих пар (центральное_слово, контекстное_слово).\n",
        "- Метод `__len__` возвращает количество обучающих примеров.\n",
        "- Метод `__getitem__` возвращает конкретную пару в виде тензоров PyTorch для использования в обучении.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Корректное формирование обучающих пар критично для модели Skip-gram, которая учится предсказывать контекст по центральному слову.\n",
        "- Параметр `window_size` определяет, сколько слов до и после центрального учитываются как контекст.\n",
        "- Большое окно захватывает более широкие семантические связи, но требует больше вычислительных ресурсов.\n",
        "- Использование PyTorch Dataset/DataLoader обеспечивает эффективную подачу данных в процессе обучения."
      ],
      "metadata": {
        "id": "lkerin8d00hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Реализация модели Skip-gram"
      ],
      "metadata": {
        "id": "jUzW9lsQ02zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "\n",
        "        # Слои для центрального слова и контекстного слова\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "        # Инициализация весов\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Инициализация весов равномерным распределением\n",
        "        nn.init.uniform_(self.embeddings.weight, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.output_layer.weight, -0.1, 0.1)\n",
        "        nn.init.zeros_(self.output_layer.bias)\n",
        "\n",
        "    def forward(self, center_words):\n",
        "        # Получаем эмбеддинги центральных слов\n",
        "        center_embeddings = self.embeddings(center_words)\n",
        "        # Предсказываем вероятности контекстных слов\n",
        "        output = self.output_layer(center_embeddings)\n",
        "        return output\n",
        "\n",
        "    def get_word_embedding(self, word_idx):\n",
        "        # Получаем эмбеддинг для конкретного слова\n",
        "        return self.embeddings(torch.tensor([word_idx], device=device)).detach().cpu().numpy()[0]"
      ],
      "metadata": {
        "id": "-jY2eCQ8zGuT"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- Класс `SkipGramModel` наследуется от `nn.Module` — базового класса для нейронных сетей в PyTorch.\n",
        "- Архитектура включает два основных компонента:\n",
        "  1. `self.embeddings` — слой встраивания, преобразующий индексы слов в векторы размерности `embedding_dim`.\n",
        "  2. `self.output_layer` — полносвязный слой, преобразующий вектор центрального слова в предсказания для всех возможных контекстных слов.\n",
        "- Метод `init_weights` инициализирует веса равномерным распределением для более стабильного обучения.\n",
        "- Метод `forward` выполняет прямой проход: получает эмбеддинги центральных слов и предсказывает вероятности для контекстных слов.\n",
        "- Метод `get_word_embedding` позволяет извлечь вектор для конкретного слова по его индексу.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Модель Skip-gram учится таким образом, чтобы слова, встречающиеся в похожих контекстах, имели похожие векторные представления.\n",
        "- Размерность эмбеддинга `embedding_dim` определяет, насколько детально могут быть представлены семантические отношения.\n",
        "- Инициализация весов влияет на скорость и стабильность обучения.\n",
        "- После обучения нам важны именно векторы из слоя `embeddings`, которые и будут векторными представлениями слов."
      ],
      "metadata": {
        "id": "Ey-c--2207cZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Функции для обучения модели и вычисления сходства"
      ],
      "metadata": {
        "id": "oDRIrS9z0-CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_skip_gram(model, dataset, epochs=5, batch_size=64, learning_rate=0.01):\n",
        "    \"\"\"\n",
        "    Обучает модель Skip-gram\n",
        "\n",
        "    Параметры:\n",
        "    model - модель Skip-gram\n",
        "    dataset - набор данных для обучения\n",
        "    epochs - количество эпох обучения\n",
        "    batch_size - размер мини-пакета\n",
        "    learning_rate - скорость обучения\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Определяем функцию потерь и оптимизатор\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Обучение модели\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (center_words, context_words) in enumerate(dataloader):\n",
        "            center_words = center_words.to(device)\n",
        "            context_words = context_words.to(device)\n",
        "\n",
        "            # Обнуляем градиенты\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Прямой проход\n",
        "            output = model(center_words)\n",
        "\n",
        "            # Вычисление потерь\n",
        "            loss = criterion(output, context_words)\n",
        "\n",
        "            # Обратное распространение ошибки\n",
        "            loss.backward()\n",
        "\n",
        "            # Обновление весов\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Вывод промежуточных результатов\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f'Эпоха: {epoch+1}/{epochs}, Batch: {batch_idx+1}/{len(dataloader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Вывод средней потери за эпоху\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f'Эпоха: {epoch+1}/{epochs}, Средняя потеря: {avg_loss:.4f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Вычисляет косинусное сходство между двумя векторами\n",
        "    \"\"\"\n",
        "    cos_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "    return cos_sim\n",
        "\n",
        "def find_most_similar(word, model, word_to_idx, idx_to_word, top_n=10):\n",
        "    \"\"\"\n",
        "    Находит наиболее похожие слова для заданного слова\n",
        "\n",
        "    Параметры:\n",
        "    word - исходное слово\n",
        "    model - обученная модель\n",
        "    word_to_idx - словарь отображения слов в индексы\n",
        "    idx_to_word - словарь отображения индексов в слова\n",
        "    top_n - количество похожих слов для вывода\n",
        "\n",
        "    Возвращает:\n",
        "    Список кортежей (слово, сходство)\n",
        "    \"\"\"\n",
        "    if word not in word_to_idx:\n",
        "        print(f\"Слово '{word}' отсутствует в словаре\")\n",
        "        return []\n",
        "\n",
        "    word_idx = word_to_idx[word]\n",
        "    word_vec = model.get_word_embedding(word_idx)\n",
        "\n",
        "    similarities = []\n",
        "    for idx in range(len(idx_to_word)):\n",
        "        if idx != word_idx:  # Исключаем само слово\n",
        "            other_vec = model.get_word_embedding(idx)\n",
        "            similarity = cosine_similarity(word_vec, other_vec)\n",
        "            similarities.append((idx_to_word[idx], similarity))\n",
        "\n",
        "    # Сортируем по убыванию сходства\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return similarities[:top_n]"
      ],
      "metadata": {
        "id": "UXOhx9-NzIh7"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- Функция `train_skip_gram` выполняет полный цикл обучения модели:\n",
        "  1. Использует DataLoader для эффективной подачи мини-пакетов данных.\n",
        "  2. Определяет функцию потерь (перекрестная энтропия) и оптимизатор (Adam).\n",
        "  3. Для каждой эпохи проходит по всем обучающим примерам, обновляя веса модели.\n",
        "  4. Отслеживает прогресс, выводя значение функции потерь.\n",
        "- Функция `cosine_similarity` вычисляет косинусное сходство между двумя векторами — стандартная метрика для сравнения векторных представлений слов.\n",
        "- Функция `find_most_similar` находит наиболее похожие слова для заданного слова:\n",
        "  1. Получает вектор заданного слова.\n",
        "  2. Вычисляет косинусное сходство с векторами всех других слов.\n",
        "  3. Сортирует слова по убыванию сходства и возвращает верхние `top_n`.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Корректное обучение модели требует баланса между скоростью обучения, размером мини-пакета и количеством эпох.\n",
        "- Косинусное сходство имеет преимущество над Евклидовым расстоянием для векторов слов, так как учитывает направление, а не только расстояние.\n",
        "- Поиск похожих слов — основной способ оценки качества векторных представлений.\n",
        "- Параметры обучения (learning_rate, batch_size, epochs) сильно влияют на качество результирующих эмбеддингов.\n"
      ],
      "metadata": {
        "id": "yfe3glWg1AqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение модели Skip-gram"
      ],
      "metadata": {
        "id": "nd8Bvi021MaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание набора данных\n",
        "window_size = 2\n",
        "skipgram_dataset = SkipGramDataset(normalized_sentences, word_to_idx, window_size=window_size)\n",
        "print(f\"Создано {len(skipgram_dataset)} обучающих примеров\")\n",
        "\n",
        "# Настройка параметров модели\n",
        "embedding_dim = 50  # Размерность векторов слов\n",
        "model = SkipGramModel(vocab_size, embedding_dim)\n",
        "\n",
        "# Параметры обучения\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Обучение модели\n",
        "print(\"Начало обучения модели Skip-gram...\")\n",
        "model = train_skip_gram(model, skipgram_dataset, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate)\n",
        "print(\"Обучение завершено!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPk6BEEVzK0r",
        "outputId": "27bc3fde-134b-49f9-ca8f-3c1435b7946e"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Создано 1268 обучающих примеров\n",
            "Начало обучения модели Skip-gram...\n",
            "Эпоха: 1/5, Средняя потеря: 5.2797\n",
            "Эпоха: 2/5, Средняя потеря: 4.8336\n",
            "Эпоха: 3/5, Средняя потеря: 3.9964\n",
            "Эпоха: 4/5, Средняя потеря: 3.2365\n",
            "Эпоха: 5/5, Средняя потеря: 2.8459\n",
            "Обучение завершено!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- Создаем набор данных `skipgram_dataset` с заданным размером контекстного окна.\n",
        "- Инициализируем модель `SkipGramModel` с выбранной размерностью эмбеддингов.\n",
        "- Устанавливаем параметры обучения:\n",
        "  - `epochs` = 5 — количество полных проходов по всем данным.\n",
        "  - `batch_size` = 32 — количество примеров, обрабатываемых за одну итерацию.\n",
        "  - `learning_rate` = 0.01 — величина шага при обновлении весов.\n",
        "- Запускаем процесс обучения, который может занять значительное время в зависимости от объема данных.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Выбор гиперпараметров критичен для качества обучения:\n",
        "  - Слишком маленький `embedding_dim` не сможет захватить всю семантику.\n",
        "  - Слишком маленький `window_size` не учтет дальние зависимости.\n",
        "  - Неправильный `learning_rate` может привести к невозможности сходимости или застреванию в локальном минимуме.\n",
        "- Достаточное количество эпох необходимо для полного обучения, но слишком много может привести к переобучению.\n",
        "- На маленьких корпусах может потребоваться увеличение числа эпох для компенсации недостатка данных."
      ],
      "metadata": {
        "id": "w2MbtCBf1KVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Поиск похожих слов и визуализация"
      ],
      "metadata": {
        "id": "teMcjhfG1FAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Тестовые слова для проверки эмбеддингов\n",
        "test_words = ['машинный', 'обучение', 'нейронный', 'метод', 'данные']\n",
        "\n",
        "# Поиск похожих слов\n",
        "print(\"\\nПоиск наиболее похожих слов:\")\n",
        "for word in test_words:\n",
        "    similar_words = find_most_similar(word, model, word_to_idx, idx_to_word, top_n=5)\n",
        "    print(f\"\\nСлова, похожие на '{word}':\")\n",
        "    for similar_word, similarity in similar_words:\n",
        "        print(f\"  {similar_word}: {similarity:.4f}\")\n",
        "\n",
        "# Функция для визуализации векторных представлений слов\n",
        "def visualize_embeddings(model, word_to_idx, idx_to_word, n_words=100):\n",
        "    \"\"\"\n",
        "    Визуализирует векторные представления с помощью t-SNE\n",
        "\n",
        "    Параметры:\n",
        "    model - обученная модель\n",
        "    word_to_idx - словарь отображения слов в индексы\n",
        "    idx_to_word - словарь отображения индексов в слова\n",
        "    n_words - количество слов для визуализации\n",
        "    \"\"\"\n",
        "    # Если слов меньше, чем запрошено, используем все\n",
        "    n_words = min(n_words, len(word_to_idx))\n",
        "\n",
        "    # Получаем самые частые слова\n",
        "    word_indices = list(idx_to_word.keys())[:n_words]\n",
        "\n",
        "    # Получаем их векторные представления\n",
        "    word_vectors = np.array([model.get_word_embedding(idx) for idx in word_indices])\n",
        "\n",
        "    # Применяем t-SNE для снижения размерности до 2D\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, n_words-1))\n",
        "    reduced_vectors = tsne.fit_transform(word_vectors)\n",
        "\n",
        "    # Визуализация\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    for i, idx in enumerate(word_indices):\n",
        "        plt.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1], marker='o')\n",
        "        plt.annotate(idx_to_word[idx],\n",
        "                     (reduced_vectors[i, 0], reduced_vectors[i, 1]),\n",
        "                     fontsize=9)\n",
        "\n",
        "    plt.title('t-SNE визуализация векторных представлений слов')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Визуализируем векторные представления\n",
        "visualize_embeddings(model, word_to_idx, idx_to_word, n_words=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nlrlvAnTzgL6",
        "outputId": "72042333-8d90-4ac8-8c8b-f9029244750f"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Поиск наиболее похожих слов:\n",
            "\n",
            "Слова, похожие на 'машинный':\n",
            "  совокупность: 0.6602\n",
            "  связать: 0.6331\n",
            "  класс: 0.6293\n",
            "  широкий: 0.5867\n",
            "  тесно: 0.5163\n",
            "\n",
            "Слова, похожие на 'обучение':\n",
            "  совокупность: 0.5904\n",
            "  многие: 0.5718\n",
            "  тесно: 0.5309\n",
            "  основать: 0.4622\n",
            "  иметь: 0.4573\n",
            "\n",
            "Слова, похожие на 'нейронный':\n",
            "  глубокий: 0.7320\n",
            "  он: 0.7027\n",
            "  неглубокий: 0.6960\n",
            "  поэтому: 0.6084\n",
            "  сеть: 0.6057\n",
            "\n",
            "Слова, похожие на 'метод':\n",
            "  класс: 0.6772\n",
            "  построение: 0.6283\n",
            "  математический: 0.6104\n",
            "  глубокий: 0.5633\n",
            "  совокупность: 0.5602\n",
            "\n",
            "Слова, похожие на 'данные':\n",
            "  форма: 0.6217\n",
            "  дать: 0.5792\n",
            "  входной: 0.5373\n",
            "  анализ: 0.4891\n",
            "  объём: 0.4811\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- Проверяем качество обученных эмбеддингов на наборе тестовых слов (`test_words`).\n",
        "- Для каждого слова находим 5 наиболее семантически близких слов.\n",
        "- Функция `visualize_embeddings` визуализирует векторные представления в 2D-пространстве:\n",
        "  1. Берет первые `n_words` слов из словаря.\n",
        "  2. Применяет алгоритм t-SNE для снижения размерности с `embedding_dim` до 2.\n",
        "  3. Отображает слова на графике, где близкие по смыслу слова должны располагаться рядом.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Визуальная проверка похожих слов — это интуитивный способ оценить, насколько хорошо модель уловила семантические отношения.\n",
        "- Визуализация через t-SNE позволяет увидеть кластеризацию слов по смыслу в 2D-пространстве.\n",
        "- Параметр `perplexity` в t-SNE влияет на баланс между сохранением локальной и глобальной структуры данных.\n",
        "- Если похожие слова действительно близки по смыслу (например, \"машинный\" ↔ \"компьютерный\"), это показатель успешного обучения."
      ],
      "metadata": {
        "id": "_cRI1DXk1Tah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Интерактивный поиск похожих слов"
      ],
      "metadata": {
        "id": "ntNEZecD1X4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_similar_words(model, word_to_idx, idx_to_word):\n",
        "    \"\"\"\n",
        "    Интерактивный поиск похожих слов\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        word = input(\"\\nВведите слово (или 'выход' для завершения): \").strip().lower()\n",
        "\n",
        "        if word == 'выход':\n",
        "            break\n",
        "\n",
        "        # Нормализуем слово с помощью pymorphy2\n",
        "        if word:\n",
        "            parsed_word = morph.parse(word)[0]\n",
        "            normalized_word = parsed_word.normal_form\n",
        "\n",
        "            if normalized_word != word:\n",
        "                print(f\"Нормализованная форма: {normalized_word}\")\n",
        "\n",
        "            # Поиск похожих слов\n",
        "            similar_words = find_most_similar(normalized_word, model, word_to_idx, idx_to_word, top_n=10)\n",
        "\n",
        "            if similar_words:\n",
        "                print(f\"\\nСлова, похожие на '{normalized_word}':\")\n",
        "                for similar_word, similarity in similar_words:\n",
        "                    print(f\"  {similar_word}: {similarity:.4f}\")\n",
        "            else:\n",
        "                print(\"Похожие слова не найдены.\")\n",
        "\n",
        "# Запуск интерактивного режима\n",
        "print(\"\\nИнтерактивный поиск похожих слов\")\n",
        "print(\"Введите слово, чтобы найти наиболее похожие слова по векторным представлениям\")\n",
        "interactive_similar_words(model, word_to_idx, idx_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8530qeCzzjj",
        "outputId": "e20fafbb-ead7-4970-aab4-a5b9e2c6bb21"
      },
      "execution_count": 109,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Интерактивный поиск похожих слов\n",
            "Введите слово, чтобы найти наиболее похожие слова по векторным представлениям\n",
            "\n",
            "Введите слово (или 'выход' для завершения): запись\n",
            "Слово 'запись' отсутствует в словаре\n",
            "Похожие слова не найдены.\n",
            "\n",
            "Введите слово (или 'выход' для завершения): данные\n",
            "Нормализованная форма: дать\n",
            "\n",
            "Слова, похожие на 'дать':\n",
            "  входной: 0.6479\n",
            "  абстрактный: 0.6303\n",
            "  данные: 0.5792\n",
            "  быть: 0.5570\n",
            "  часть: 0.5513\n",
            "  преобразовать: 0.5470\n",
            "  каждый: 0.5007\n",
            "  представить: 0.4980\n",
            "  слой: 0.4886\n",
            "  более: 0.4757\n",
            "\n",
            "Введите слово (или 'выход' для завершения): выход\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- Функция `interactive_similar_words` создает интерактивный интерфейс для поиска похожих слов:\n",
        "  1. Пользователь вводит слово.\n",
        "  2. Слово нормализуется с помощью `pymorphy2`.\n",
        "  3. Система находит и выводит 10 наиболее похожих слов с их косинусным сходством.\n",
        "- Интерактивный режим позволяет пользователю исследовать семантическое пространство модели.\n",
        "- Нормализация ввода важна, поскольку модель обучена на нормализованных формах слов.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Интерактивное взаимодействие — эффективный способ оценки практической применимости модели.\n",
        "- Нормализация пользовательского ввода необходима для сопоставимости с обучающими данными.\n",
        "- Значения косинусного сходства показывают, насколько уверена модель в семантической близости.\n",
        "- Высокие значения (> 0.7) обычно указывают на сильную семантическую связь."
      ],
      "metadata": {
        "id": "UBvyhrH81Z-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Векторная арифметика"
      ],
      "metadata": {
        "id": "oPpw1wJe1bxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_vector_arithmetic(model, word_to_idx, idx_to_word, word1, word2, word3, top_n=5):\n",
        "    \"\"\"\n",
        "    Выполняет векторную арифметику в пространстве эмбеддингов\n",
        "    Пример: word1 - word2 + word3 ≈ ?\n",
        "\n",
        "    Параметры:\n",
        "    model - обученная модель\n",
        "    word_to_idx - словарь отображения слов в индексы\n",
        "    idx_to_word - словарь отображения индексов в слова\n",
        "    word1, word2, word3 - слова для арифметической операции\n",
        "    top_n - количество наиболее похожих слов\n",
        "\n",
        "    Возвращает:\n",
        "    Список кортежей (слово, сходство)\n",
        "    \"\"\"\n",
        "    # Проверка наличия слов в словаре\n",
        "    if word1 not in word_to_idx or word2 not in word_to_idx or word3 not in word_to_idx:\n",
        "        missing = []\n",
        "        if word1 not in word_to_idx: missing.append(word1)\n",
        "        if word2 not in word_to_idx: missing.append(word2)\n",
        "        if word3 not in word_to_idx: missing.append(word3)\n",
        "        print(f\"Слова отсутствуют в словаре: {', '.join(missing)}\")\n",
        "        return []\n",
        "\n",
        "    # Получаем векторы слов\n",
        "    vec1 = model.get_word_embedding(word_to_idx[word1])\n",
        "    vec2 = model.get_word_embedding(word_to_idx[word2])\n",
        "    vec3 = model.get_word_embedding(word_to_idx[word3])\n",
        "\n",
        "    # Выполняем арифметическую операцию\n",
        "    result_vec = vec1 - vec2 + vec3\n",
        "\n",
        "    # Находим наиболее похожие слова к результату\n",
        "    similarities = []\n",
        "    for idx in range(len(idx_to_word)):\n",
        "        # Исключаем исходные слова\n",
        "        if idx_to_word[idx] not in [word1, word2, word3]:\n",
        "            other_vec = model.get_word_embedding(idx)\n",
        "            similarity = cosine_similarity(result_vec, other_vec)\n",
        "            similarities.append((idx_to_word[idx], similarity))\n",
        "\n",
        "    # Сортируем по убыванию сходства\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return similarities[:top_n]\n",
        "\n",
        "# Примеры векторной арифметики\n",
        "vector_arithmetic_examples = [\n",
        "    ('нейронный', 'сеть', 'метод'),\n",
        "    ('обучение', 'машинный', 'данные'),\n",
        "    ('нейронный', 'искусственный', 'глубокий')\n",
        "]\n",
        "\n",
        "print(\"\\nПримеры векторной арифметики:\")\n",
        "for word1, word2, word3 in vector_arithmetic_examples:\n",
        "    print(f\"\\n{word1} - {word2} + {word3} ≈ ?\")\n",
        "    result = word_vector_arithmetic(model, word_to_idx, idx_to_word, word1, word2, word3)\n",
        "    for word, similarity in result:\n",
        "        print(f\"  {word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nto1kt-Pz_ux",
        "outputId": "57e1a78b-e3e1-4bc1-ec7d-c9206b66bfc9"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Примеры векторной арифметики:\n",
            "\n",
            "нейронный - сеть + метод ≈ ?\n",
            "  глубокий: 0.5887\n",
            "  класс: 0.5732\n",
            "  математический: 0.4606\n",
            "  построение: 0.4550\n",
            "  создание: 0.4425\n",
            "\n",
            "обучение - машинный + данные ≈ ?\n",
            "  форма: 0.4306\n",
            "  цифровой: 0.4167\n",
            "  работать: 0.4054\n",
            "  »: 0.3900\n",
            "  нужно: 0.3881\n",
            "\n",
            "нейронный - искусственный + глубокий ≈ ?\n",
            "  класс: 0.5946\n",
            "  он: 0.5852\n",
            "  арифметик: 0.5804\n",
            "  неглубокий: 0.5674\n",
            "  основать: 0.5264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- Функция `word_vector_arithmetic` реализует знаменитую особенность Word2Vec — возможность векторной арифметики:\n",
        "  - Выражения вида: \"король\" - \"мужчина\" + \"женщина\" ≈ \"королева\"\n",
        "  - В общем виде: word1 - word2 + word3 ≈ ?\n",
        "- Процесс включает:\n",
        "  1. Получение векторов для трех заданных слов.\n",
        "  2. Выполнение арифметической операции над векторами.\n",
        "  3. Поиск слов, чьи векторы наиболее близки к результату.\n",
        "- Проверяем это на нескольких примерах из нашего домена: ('нейронный', 'сеть', 'метод') и др.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Векторная арифметика демонстрирует, что векторные представления действительно уловили семантические отношения между словами.\n",
        "- Эта способность Word2Vec — ключевое доказательство того, что модель не просто запоминает, а выявляет структуру в данных.\n",
        "- Качество результатов зависит от объема данных и качества обучения.\n",
        "- В идеале, результаты должны быть семантически согласованными (например, \"нейронный\" - \"сеть\" + \"метод\" ≈ \"алгоритм\").\n"
      ],
      "metadata": {
        "id": "utvVi3Ks1eXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Анализ и выводы"
      ],
      "metadata": {
        "id": "i1LpGmXG1g_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение векторных представлений в файл\n",
        "def save_embeddings(model, idx_to_word, filename=\"word_embeddings.txt\"):\n",
        "    \"\"\"\n",
        "    Сохраняет векторные представления слов в файл\n",
        "\n",
        "    Параметры:\n",
        "    model - обученная модель\n",
        "    idx_to_word - словарь отображения индексов в слова\n",
        "    filename - имя файла для сохранения\n",
        "    \"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for idx, word in idx_to_word.items():\n",
        "            vector = model.get_word_embedding(idx)\n",
        "            vector_str = ' '.join([str(val) for val in vector])\n",
        "            f.write(f\"{word} {vector_str}\\n\")\n",
        "\n",
        "    print(f\"Векторные представления сохранены в файл {filename}\")\n",
        "\n",
        "# Сохраняем эмбеддинги\n",
        "save_embeddings(model, idx_to_word)\n",
        "\n",
        "print(\"\\nАнализ результатов модели Word2Vec:\")\n",
        "print(\"1. Созданы векторные представления для\", vocab_size, \"слов\")\n",
        "print(\"2. Размерность векторов:\", embedding_dim)\n",
        "print(\"3. Контекстное окно размером\", window_size, \"использовалось для обучения\")\n",
        "print(\"4. Наблюдаемые семантические связи:\")\n",
        "print(\"   - Близкие по смыслу слова располагаются рядом в векторном пространстве\")\n",
        "print(\"   - Можно выполнять векторную арифметику (например, word1 - word2 + word3)\")\n",
        "print(\"5. Ограничения текущей модели:\")\n",
        "print(\"   - Небольшой размер обучающего корпуса\")\n",
        "print(\"   - Малое количество повторений слов\")\n",
        "print(\"   - Отсутствие предобученных эмбеддингов для сравнения\")\n",
        "print(\"\\nДля улучшения модели можно:\")\n",
        "print(\"1. Увеличить размер обучающего корпуса\")\n",
        "print(\"2. Оптимизировать гиперпараметры (размер окна, размерность векторов)\")\n",
        "print(\"3. Использовать более сложные техники (например, отрицательное семплирование)\")\n",
        "print(\"4. Применить предобученные модели для русского языка (например, FastText)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEddILhd0IYq",
        "outputId": "68d93bf3-126e-424d-8225-482e0295a557"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Векторные представления сохранены в файл word_embeddings.txt\n",
            "\n",
            "Анализ результатов модели Word2Vec:\n",
            "1. Созданы векторные представления для 205 слов\n",
            "2. Размерность векторов: 50\n",
            "3. Контекстное окно размером 2 использовалось для обучения\n",
            "4. Наблюдаемые семантические связи:\n",
            "   - Близкие по смыслу слова располагаются рядом в векторном пространстве\n",
            "   - Можно выполнять векторную арифметику (например, word1 - word2 + word3)\n",
            "5. Ограничения текущей модели:\n",
            "   - Небольшой размер обучающего корпуса\n",
            "   - Малое количество повторений слов\n",
            "   - Отсутствие предобученных эмбеддингов для сравнения\n",
            "\n",
            "Для улучшения модели можно:\n",
            "1. Увеличить размер обучающего корпуса\n",
            "2. Оптимизировать гиперпараметры (размер окна, размерность векторов)\n",
            "3. Использовать более сложные техники (например, отрицательное семплирование)\n",
            "4. Применить предобученные модели для русского языка (например, FastText)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- Функция `save_embeddings` сохраняет векторные представления в текстовый файл, что позволяет использовать их в других приложениях.\n",
        "- Приводим подробный анализ результатов модели Word2Vec:\n",
        "  1. Статистика модели: размер словаря, размерность векторов, размер контекстного окна.\n",
        "  2. Наблюдаемые семантические связи.\n",
        "  3. Ограничения текущей модели, связанные с размером корпуса.\n",
        "- Предлагаем способы улучшения модели: увеличение корпуса, оптимизация гиперпараметров, использование предобученных моделей.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Систематический анализ результатов необходим для понимания сильных и слабых сторон модели.\n",
        "- Сохранение эмбеддингов позволяет интегрировать их в другие проекты без повторного обучения.\n",
        "- Размер корпуса — критический фактор для Word2Vec. Типичные модели обучаются на миллионах предложений.\n",
        "- Предобученные модели (например, FastText для русского языка) могут быть более эффективным решением для практических задач."
      ],
      "metadata": {
        "id": "JBAfdxCW1kUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Общие выводы по реализации Word2Vec\n",
        "\n",
        "1. Мы успешно реализовали модель Skip-gram архитектуры Word2Vec с использованием PyTorch.\n",
        "2. Создали полный конвейер: от предобработки текста до визуализации результатов и интерактивного поиска.\n",
        "3. Продемонстрировали основные преимущества векторных представлений: семантическую близость и векторную арифметику.\n",
        "4. Обсудили факторы, влияющие на качество модели, и способы её улучшения.\n",
        "\n",
        "Однако, важно отметить, что для реальных приложений обычно требуются гораздо большие корпуса текстов и более тщательная настройка гиперпараметров."
      ],
      "metadata": {
        "id": "WcF4UObD1poo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4"
      ],
      "metadata": {
        "id": "qCQQhew21qdo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vtp-kkI81sqw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}