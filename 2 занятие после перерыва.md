# 4. ТОКЕНИЗАЦИЯ ТЕКСТА И BYTE PAIR ENCODING (BPE)

## 4.1 Основы токенизации текста

**Токенизация** — это процесс разбиения текста на минимальные значимые единицы (токены), которые будут использоваться языковой моделью. Исторически токенизация проходила несколько этапов эволюции:

1. **Посимвольная токенизация**: каждый символ — отдельный токен
   - Преимущество: нет проблемы с неизвестными словами
   - Недостаток: очень длинные последовательности, неэффективное обучение

2. **Токенизация на уровне слов**: каждое слово — отдельный токен
   - Преимущество: более компактное представление текста
   - Недостаток: проблема неизвестных слов (OOV — Out-of-Vocabulary)

3. **Подсловная токенизация**: разбиение на части слов, BPE
   - Преимущество: баланс между компактностью и гибкостью
   - Сейчас используется в большинстве современных языковых моделей

## 4.2 Проблема словарей и OOV

### Ключевые проблемы традиционной токенизации:

1. **Компромисс размера словаря**:
   - Большой словарь → более точное представление, но высокие вычислительные затраты
   - Маленький словарь → экономия ресурсов, но много неизвестных слов

2. **Неизвестные слова (OOV)**:
   При токенизации на уровне слов модель не может обработать слова, которых не было в обучающем наборе:
   - В примере из лекции: если в словаре есть "low", "lowest", "newer", "wider", "new", но нет слова "lower", то "lower" становится OOV
   - Традиционное решение — замена на специальный токен "<UNK>", что приводит к потере информации

3. **Разреженность словаря**:
   - Много редких слов и словоформ, которые редко встречаются в текстах
   - Разные формы одного слова занимают отдельные места в словаре

## 4.3 Byte Pair Encoding (BPE) — основы алгоритма

**Byte Pair Encoding** — это метод сжатия данных, адаптированный для подсловной токенизации. В контексте NLP, BPE позволяет создать словарь, который включает как целые частотные слова, так и части менее частотных слов.

### Концептуальные преимущества BPE:

1. **Адаптивность к данным**:
   - Словарь формируется на основе статистики конкретного корпуса текстов
   - Наиболее частые сочетания символов становятся отдельными токенами

2. **Баланс между размером словаря и полнотой представления**:
   - Можно задать точный размер словаря через количество итераций слияния
   - Редкие слова разбиваются на подтокены вместо замены на "<UNK>"

3. **Сохранение смысловой информации**:
   - Даже неизвестное слово сохраняет связь с известными частями
   - Морфологически связанные слова часто имеют общие подтокены

## 4.4 Пошаговый алгоритм BPE

### Этап 1: Подготовка и инициализация

1. **Добавление символа конца слова**:
   - Каждое слово маркируется символом конца слова (обычно "_")
   - Пример: "low" → "low_", "lowest" → "lowest_"
   - Это позволяет различать токены внутри слова и в конце слова

2. **Создание базового словаря символов**:
   - Первоначальный словарь состоит из всех уникальных символов в корпусе
   - Для английского текста обычно это 26 букв, цифры, знаки препинания и специальные символы

3. **Подсчет частот слов в корпусе**:
   - Для примера из лекции: "low_" (5), "lowest_" (2), "newer_" (6), "wider_" (3), "new_" (2)

### Этап 2: Итеративный процесс слияния

Алгоритм выполняет следующие шаги заданное количество раз (количество итераций — это параметр):

1. **Представление текста текущими токенами**:
   - Изначально каждое слово представлено как последовательность символов
   - Пример: "low_" = ['l', 'o', 'w', '_']

2. **Подсчет частот всех пар токенов**:
   - Для каждой пары последовательных токенов подсчитывается, сколько раз она встречается в корпусе
   - Пример из лекции: пара 'e' + 'r' встречается чаще всего (в словах "newer_" и "wider_")

3. **Выбор самой частой пары**:
   - Определяется пара токенов с наибольшей частотой
   - В примере лекции: ('e', 'r') — самая частая пара

4. **Слияние выбранной пары**:
   - Выбранная пара заменяется новым токеном, который добавляется в словарь
   - 'e' + 'r' → 'er'

5. **Обновление представления текста**:
   - Все вхождения данной пары заменяются новым токеном
   - "newer_" = ['n', 'ew', 'er', '_'] вместо ['n', 'e', 'w', 'e', 'r', '_']

6. **Повторение шагов 2-5** заданное количество раз или до достижения желаемого размера словаря

### Пример итераций BPE из лекции:

**Исходный текст**: "low_" (5), "lowest_" (2), "newer_" (6), "wider_" (3), "new_" (2)

**Начальный словарь**: {_, d, e, i, l, n, o, r, s, t, w}

**Итерация 1**:
- Находим самую частую пару: ('e', 'r') — встречается 9 раз
- Создаем новый токен: 'er'
- Обновляем словарь: {_, d, e, i, l, n, o, r, s, t, w, er}
- Обновляем представление слов:
  - "newer_" = ['n', 'ew', 'er', '_']
  - "wider_" = ['w', 'id', 'er', '_']

**Итерация 2**:
- Находим самую частую пару: ('er', '_') — встречается 9 раз
- Создаем новый токен: 'er_'
- Обновляем словарь: {_, d, e, i, l, n, o, r, s, t, w, er, er_}
- Обновляем представление слов:
  - "newer_" = ['n', 'ew', 'er_']
  - "wider_" = ['w', 'id', 'er_']

И так далее, пока не достигнем заданного количества итераций...

## 4.5 Конкретная реализация BPE

### Основные функции для реализации BPE:

1. **Функция для подсчета статистики пар**:
```python
def get_stats(vocab):
    pairs = {}
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pair = (symbols[i], symbols[i+1])
            pairs[pair] = pairs.get(pair, 0) + freq
    return pairs
```

2. **Функция для слияния пары в словаре**:
```python
def merge_vocab(pair, v_in):
    v_out = {}
    bigram = ' '.join(pair)
    replacement = ''.join(pair)
    for word in v_in:
        w_out = word.replace(bigram, replacement)
        v_out[w_out] = v_in[word]
    return v_out
```

3. **Основная функция BPE**:
```python
def byte_pair_encoding(text, num_merges=10):
    # Инициализация словаря символами
    vocab = {' '.join(word) + ' _': count for word, count in get_word_counts(text).items()}
    
    # Итеративное слияние пар
    for i in range(num_merges):
        pairs = get_stats(vocab)
        if not pairs:
            break
        best_pair = max(pairs, key=pairs.get)
        vocab = merge_vocab(best_pair, vocab)
        
    # Создание словаря токенов
    bpe_tokens = {}
    for token, count in vocab.items():
        for subtoken in token.split():
            bpe_tokens[subtoken] = bpe_tokens.get(subtoken, 0) + count
            
    return bpe_tokens
```

## 4.6 Применение BPE словаря для токенизации

После создания словаря BPE, процесс токенизации нового текста выполняется следующим образом:

1. **Разбиение входного текста на слова**
2. **Добавление маркера конца слова** к каждому слову
3. **Последовательное применение слияний** в порядке их добавления в словарь
4. **Преобразование полученных токенов в соответствующие ID** из словаря

### Пример токенизации с помощью созданного словаря BPE:

```python
def apply_bpe_encoding(text, bpe_codes, bpe_vocab):
    tokens = []
    for word in text.split():
        word = word + '_'  # Добавляем маркер конца слова
        
        # Представляем слово как последовательность символов
        chars = list(word)
        
        # Применяем слияния
        while len(chars) > 1:
            # Ищем пару, которую можно слить
            pair_found = False
            for i in range(len(chars) - 1):
                pair = (chars[i], chars[i+1])
                if pair in bpe_codes:
                    chars[i] = chars[i] + chars[i+1]
                    chars.pop(i+1)
                    pair_found = True
                    break
            
            if not pair_found:
                break
        
        # Преобразуем токены в ID
        for token in chars:
            tokens.append(bpe_vocab.get(token, bpe_vocab.get('<UNK>')))
    
    return tokens
```

## 4.7 Практические аспекты и инструменты

### Существующие реализации BPE:

1. **TikToken**: библиотека для токенизации, используемая в моделях OpenAI
   - Поддерживает токенизацию для различных моделей (GPT-3, GPT-4)
   - Имеет онлайн-интерфейс для тестирования токенизации

2. **SentencePiece**: инструмент от Google
   - Объединяет BPE и Unigram Language Model
   - Работает напрямую с необработанным текстом без предварительной сегментации

3. **Токенизаторы Hugging Face**:
   - Поддерживают различные алгоритмы, включая BPE
   - Интегрированы с экосистемой моделей Transformers

### Особенности реальных реализаций BPE:

1. **Обработка специальных токенов**:
   - Добавление системных токенов (начало текста, конец текста, неизвестный токен)
   - Обработка пробелов и знаков препинания

2. **Оптимизации алгоритма**:
   - Эффективная реализация для больших корпусов
   - Параллельная обработка данных

3. **Регуляризация**:
   - Ограничения на длину токенов
   - Минимальная частота для создания нового токена

## 4.8 Преимущества BPE в контексте языковых моделей

1. **Эффективное использование контекстного окна**:
   - Ограниченный размер входа модели (например, 2048 или 4096 токенов)
   - BPE позволяет уместить больше семантической информации в тот же объем токенов

2. **Обработка многоязычных текстов**:
   - Адаптивность к разным языкам без необходимости создания отдельных словарей
   - Эффективная работа с языками с богатой морфологией (русский, финский)

3. **Работа с неологизмами и специальными терминами**:
   - Новые слова, которых не было в обучающем корпусе, разбиваются на значимые части
   - Имена собственные, технические термины сохраняют информативность

4. **Устойчивость к опечаткам и вариациям написания**:
   - Опечатки обычно меняют лишь часть токенов слова
   - Модель может использовать информацию из правильно токенизированных частей

## 4.9 Специфика BPE для разных языков

1. **Латинские языки**:
   - Токены часто соответствуют морфемам (корням, приставкам, суффиксам)
   - Часто встречающиеся окончания и аффиксы становятся отдельными токенами

2. **Кириллические языки (русский)**:
   - Богатая морфология приводит к большому количеству токенов для окончаний
   - Падежные формы и спряжения эффективно обрабатываются через подсловную токенизацию

3. **Азиатские языки (китайский, японский)**:
   - Необходимость работы на уровне иероглифов или их комбинаций
   - BPE позволяет выделять часто встречающиеся комбинации иероглифов

## 4.10 Ограничения и проблемы BPE

1. **Отсутствие семантического понимания**:
   - Токенизация основана только на статистике, не на значении
   - Слова могут разбиваться на семантически бессмысленные части

2. **Зависимость от обучающего корпуса**:
   - Словарь оптимизирован для конкретного распределения слов
   - Может работать хуже на текстах из других доменов

3. **Непредсказуемость токенизации**:
   - Одно и то же слово может быть токенизировано по-разному в зависимости от контекста
   - Сложность анализа токенизации при отладке моделей

4. **Проблемы с редкими словами**:
   - Очень редкие слова все равно разбиваются на множество мелких токенов
   - Возможна потеря семантики при слишком дробной токенизации

Понимание принципов работы BPE и его применения в токенизации текста является фундаментальным для работы с современными языковыми моделями, поскольку это напрямую влияет на эффективность обучения и качество работы моделей с различными типами данных.
