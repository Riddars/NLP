{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Установка и импорт библиотек"
      ],
      "metadata": {
        "id": "t3LGO8f-XxEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка необходимых библиотек для работы с текстом на русском языке\n",
        "!pip install nltk pymorphy2\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tLfpGuBREqU",
        "outputId": "32503e80-8374-4903-9757-f18e996b6d08"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.11/dist-packages (0.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- `nltk` - это Natural Language Toolkit, библиотека для обработки естественного языка. Она содержит инструменты для токенизации текста.\n",
        "- `pymorphy2` - морфологический анализатор для русского языка, который поможет нам с лемматизацией (приведением слов к начальной форме).\n",
        "- `re` - модуль для работы с регулярными выражениями, который пригодится для сложных операций с текстом.\n",
        "- `Counter` и `defaultdict` из модуля `collections` - структуры данных для подсчёта элементов и создания словарей с значениями по умолчанию.\n",
        "- `string` - содержит константы, такие как `string.punctuation` (набор знаков препинания).\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без этих библиотек мы не сможем выполнить задание, так как они предоставляют базовый функционал для обработки текста.\n",
        "- Можно заменить некоторые библиотеки аналогами (например, вместо `pymorphy2` использовать `natasha`), но потребуется изменить соответствующий код."
      ],
      "metadata": {
        "id": "pR5uvglpX54C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка ресурсов NLTK"
      ],
      "metadata": {
        "id": "98iLym5OYJAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка ресурсов NLTK для токенизации и работы со стоп-словами\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAyFN00YRH8h",
        "outputId": "d06369bc-056f-4d37-e2cc-299ec8b127f6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- `nltk.download('punkt')` - загружает модель для разделения текста на предложения (пунктуация).\n",
        "- `nltk.download('stopwords')` - загружает списки стоп-слов (часто встречающиеся слова, которые обычно не несут значимой информации).\n",
        "- `sent_tokenize` - функция для разделения текста на предложения.\n",
        "- `word_tokenize` - функция для разделения предложений на слова."
      ],
      "metadata": {
        "id": "Tluk8bfHYWuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример текста для обработки (можно заменить на любой другой)\n",
        "text = \"\"\"\n",
        "Искусственный интеллект (ИИ) - это область компьютерной науки, которая фокусируется на создании систем, способных выполнять задачи, требующие человеческого интеллекта.\n",
        "Машинное обучение является подобластью ИИ, которая позволяет компьютерам учиться на основе данных без явного программирования.\n",
        "Глубокое обучение использует нейронные сети с множеством слоев для анализа различных факторов данных.\n",
        "Современные языковые модели используют трансформеры для обработки естественного языка и генерации текста.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Исходный текст:\")\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX8gTnGSRIzD",
        "outputId": "6fe3b45a-1c92-42df-a38e-51bbb5e31a95"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходный текст:\n",
            "\n",
            "Искусственный интеллект (ИИ) - это область компьютерной науки, которая фокусируется на создании систем, способных выполнять задачи, требующие человеческого интеллекта.\n",
            "Машинное обучение является подобластью ИИ, которая позволяет компьютерам учиться на основе данных без явного программирования.\n",
            "Глубокое обучение использует нейронные сети с множеством слоев для анализа различных факторов данных.\n",
            "Современные языковые модели используют трансформеры для обработки естественного языка и генерации текста.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нужно скачать punkt_tab в строке ниже"
      ],
      "metadata": {
        "id": "QoSe3faFSPcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxx04jhyR1LC",
        "outputId": "f14ac05b-0db7-408e-f8be-864af860dacd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt_tab\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package punkt_tab to /root/nltk_data...\n",
            "      Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Токенизация текста на предложения"
      ],
      "metadata": {
        "id": "8jWam8LXYhcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение текста на предложения\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(\"\\nРазделение на предложения:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"Предложение {i+1}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLAkSnICSHkK",
        "outputId": "a78ab611-afa3-41c4-afed-0d264cfeaafe"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Разделение на предложения:\n",
            "Предложение 1: \n",
            "Искусственный интеллект (ИИ) - это область компьютерной науки, которая фокусируется на создании систем, способных выполнять задачи, требующие человеческого интеллекта.\n",
            "Предложение 2: Машинное обучение является подобластью ИИ, которая позволяет компьютерам учиться на основе данных без явного программирования.\n",
            "Предложение 3: Глубокое обучение использует нейронные сети с множеством слоев для анализа различных факторов данных.\n",
            "Предложение 4: Современные языковые модели используют трансформеры для обработки естественного языка и генерации текста.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- `sent_tokenize(text)` разбивает текст на отдельные предложения, анализируя пунктуацию и структуру текста.\n",
        "- Функция возвращает список предложений, который мы сохраняем в переменной `sentences`.\n",
        "- Затем мы выводим каждое предложение с его порядковым номером для наглядности.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без этого шага мы не сможем обрабатывать текст на уровне предложений, что важно для многих задач NLP.\n",
        "- Можно заменить `sent_tokenize` на свою функцию, например, разделяя текст по символам `.`, `!`, `?`, но это менее надежно."
      ],
      "metadata": {
        "id": "7VzRmibTYlZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Токенизация предложений на слова"
      ],
      "metadata": {
        "id": "9kQ3OP1_YuVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизация предложений на слова\n",
        "tokenized_sentences = []\n",
        "for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tokenized_sentences.append(tokens)\n",
        "\n",
        "print(\"\\nПример токенизации предложения на слова:\")\n",
        "print(f\"Предложение: {sentences[0]}\")\n",
        "print(f\"Токены: {tokenized_sentences[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfNBXMz5SLNJ",
        "outputId": "c224a087-fc44-4172-db82-d484fa1e5823"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Пример токенизации предложения на слова:\n",
            "Предложение: \n",
            "Искусственный интеллект (ИИ) - это область компьютерной науки, которая фокусируется на создании систем, способных выполнять задачи, требующие человеческого интеллекта.\n",
            "Токены: ['Искусственный', 'интеллект', '(', 'ИИ', ')', '-', 'это', 'область', 'компьютерной', 'науки', ',', 'которая', 'фокусируется', 'на', 'создании', 'систем', ',', 'способных', 'выполнять', 'задачи', ',', 'требующие', 'человеческого', 'интеллекта', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- Для каждого предложения из списка `sentences` мы применяем функцию `word_tokenize()`.\n",
        "- Эта функция разбивает предложение на отдельные слова и знаки препинания (токены).\n",
        "- Результаты сохраняются в список списков `tokenized_sentences`, где каждый вложенный список содержит токены одного предложения.\n",
        "- В конце выводим пример для первого предложения, чтобы наглядно показать результат.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без токенизации на слова мы не сможем анализировать и обрабатывать текст на уровне отдельных слов.\n",
        "- Можно заменить на разделение по пробелам (`sentence.split()`), но это не будет учитывать знаки препинания и другие нюансы."
      ],
      "metadata": {
        "id": "x7Sh7xayY20J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Нормализация токенов"
      ],
      "metadata": {
        "id": "kOvNBWJYYvNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импорт и настройка морфологического анализатора для русского языка\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Функция для нормализации русских токенов\n",
        "def normalize_tokens(tokens):\n",
        "    \"\"\"\n",
        "    Приводит токены к нормальной форме:\n",
        "    - переводит в нижний регистр\n",
        "    - удаляет знаки препинания\n",
        "    - выполняет лемматизацию (приведение к начальной форме)\n",
        "    \"\"\"\n",
        "    normalized_tokens = []\n",
        "    for token in tokens:\n",
        "        # Приведение к нижнему регистру\n",
        "        token = token.lower()\n",
        "\n",
        "        # Удаление знаков препинания\n",
        "        token = ''.join([char for char in token if char not in string.punctuation])\n",
        "\n",
        "        # Пропуск пустых токенов\n",
        "        if not token:\n",
        "            continue\n",
        "\n",
        "        # Лемматизация для русского языка с помощью pymorphy2\n",
        "        parsed_token = morph.parse(token)[0]\n",
        "        normalized = parsed_token.normal_form\n",
        "\n",
        "        normalized_tokens.append(normalized)\n",
        "\n",
        "    return normalized_tokens\n",
        "\n",
        "# Применение нормализации к нашим токенам\n",
        "normalized_sentences = []\n",
        "for tokens in tokenized_sentences:\n",
        "    normalized_tokens = normalize_tokens(tokens)\n",
        "    normalized_sentences.append(normalized_tokens)\n",
        "\n",
        "print(\"\\nПример нормализованного предложения:\")\n",
        "print(f\"Исходные токены: {tokenized_sentences[0]}\")\n",
        "print(f\"Нормализованные токены: {normalized_sentences[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IDhoKzcSYOB",
        "outputId": "ef4f999c-b574-40c8-8c94-99d5254985b4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Пример нормализованного предложения:\n",
            "Исходные токены: ['Искусственный', 'интеллект', '(', 'ИИ', ')', '-', 'это', 'область', 'компьютерной', 'науки', ',', 'которая', 'фокусируется', 'на', 'создании', 'систем', ',', 'способных', 'выполнять', 'задачи', ',', 'требующие', 'человеческого', 'интеллекта', '.']\n",
            "Нормализованные токены: ['искусственный', 'интеллект', 'ия', 'это', 'область', 'компьютерный', 'наука', 'который', 'фокусироваться', 'на', 'создание', 'система', 'способный', 'выполнять', 'задача', 'требовать', 'человеческий', 'интеллект']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- Создаем функцию `normalize_tokens`, которая выполняет три основных шага нормализации:\n",
        "  1. **Приведение к нижнему регистру**: \"Слово\" → \"слово\"\n",
        "  2. **Удаление знаков препинания**: \"слово,\" → \"слово\"\n",
        "  3. **Лемматизация**: \"словами\" → \"слово\", \"бежали\" → \"бежать\"\n",
        "- Лемматизация использует `pymorphy2` — морфологический анализатор для русского языка, который приводит слова к их начальной форме.\n",
        "- Для каждого предложения применяем функцию нормализации и сохраняем результаты в `normalized_sentences`.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без нормализации разные формы одного слова будут считаться разными токенами (например, \"слово\", \"слова\", \"словами\" — это три разных токена).\n",
        "- Если убрать лемматизацию, но оставить приведение к нижнему регистру, результат будет менее точным, но всё равно лучше, чем исходный текст.\n",
        "- Можно заменить `pymorphy2` на другую библиотеку"
      ],
      "metadata": {
        "id": "ZhnKxSg1ZOpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создание словаря с индексами и частотами"
      ],
      "metadata": {
        "id": "U6HyIK7TZhUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание словаря из нормализованных токенов\n",
        "# Словарь будет содержать уникальные токены и их частоту\n",
        "vocabulary = {}\n",
        "token_counter = Counter()\n",
        "\n",
        "# Подсчет всех токенов\n",
        "for sentence in normalized_sentences:\n",
        "    token_counter.update(sentence)\n",
        "\n",
        "# Создание словаря с индексами и частотами\n",
        "for idx, (token, count) in enumerate(token_counter.most_common()):\n",
        "    vocabulary[token] = {\n",
        "        \"id\": idx,\n",
        "        \"count\": count\n",
        "    }\n",
        "\n",
        "print(f\"\\nРазмер словаря: {len(vocabulary)} уникальных токенов\")\n",
        "print(\"\\nПример 10 наиболее часто встречающихся токенов:\")\n",
        "for token, info in list(vocabulary.items())[:10]:\n",
        "    print(f\"{token}: встречается {info['count']} раз, id={info['id']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhOicy7mT5Lq",
        "outputId": "4649c382-98db-49c4-a43c-64aaeb66ee63"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Размер словаря: 50 уникальных токенов\n",
            "\n",
            "Пример 10 наиболее часто встречающихся токенов:\n",
            "интеллект: встречается 2 раз, id=0\n",
            "ия: встречается 2 раз, id=1\n",
            "который: встречается 2 раз, id=2\n",
            "на: встречается 2 раз, id=3\n",
            "обучение: встречается 2 раз, id=4\n",
            "данные: встречается 2 раз, id=5\n",
            "использовать: встречается 2 раз, id=6\n",
            "для: встречается 2 раз, id=7\n",
            "искусственный: встречается 1 раз, id=8\n",
            "это: встречается 1 раз, id=9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- Создаем словарь `vocabulary`, который будет содержать информацию о всех уникальных токенах.\n",
        "- Используем `Counter` для подсчета частоты каждого токена во всем тексте.\n",
        "- Для каждого уникального токена создаем запись в словаре, содержащую:\n",
        "  - `id`: уникальный идентификатор токена (порядковый номер)\n",
        "  - `count`: количество появлений токена в тексте\n",
        "- Токены сортируются по частоте с помощью `most_common()`, так что самые частые получают меньшие id.\n",
        "- В конце выводим размер словаря и 10 наиболее часто встречающихся токенов.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без словаря мы не сможем присваивать числовые идентификаторы токенам, что необходимо для обучения моделей машинного обучения.\n",
        "- Можно изменить порядок сортировки (например, по алфавиту), но это менее эффективно, так как частотные токены должны иметь меньшие ID.\n"
      ],
      "metadata": {
        "id": "9MJ4UHzeZXBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Реализация алгоритма Byte-Pair Encoding (BPE)"
      ],
      "metadata": {
        "id": "SPZ01ErbZlRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(vocab):\n",
        "    \"\"\"\n",
        "    Подсчитывает частоту пар символов во всех словах словаря.\n",
        "    Возвращает словарь, где ключ - пара символов, значение - частота встречаемости.\n",
        "    \"\"\"\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[symbols[i], symbols[i + 1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    \"\"\"\n",
        "    Заменяет каждое вхождение пары символов на их объединение.\n",
        "    \"\"\"\n",
        "    v_out = {}\n",
        "    bigram = ' '.join(pair)\n",
        "    replacement = ''.join(pair)\n",
        "    for word in v_in:\n",
        "        w_out = word.replace(bigram, replacement)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "def learn_bpe(words, num_merges=10):\n",
        "    \"\"\"\n",
        "    Обучает модель BPE на списке слов.\n",
        "\n",
        "    Параметры:\n",
        "    words: список слов для обучения\n",
        "    num_merges: количество операций слияния\n",
        "\n",
        "    Возвращает:\n",
        "    bpe_codes: словарь операций слияния\n",
        "    vocab: итоговый словарь с преобразованными словами\n",
        "    \"\"\"\n",
        "    # Создаем словарь слов и их частот\n",
        "    vocab = Counter(words)\n",
        "\n",
        "    # Разделяем каждый символ в слове пробелом\n",
        "    vocab = {' '.join(word): freq for word, freq in vocab.items()}\n",
        "\n",
        "    # Словарь операций слияния\n",
        "    bpe_codes = {}\n",
        "\n",
        "    for i in range(num_merges):\n",
        "        # Получаем статистику по парам\n",
        "        pairs = get_stats(vocab)\n",
        "        if not pairs:\n",
        "            break\n",
        "\n",
        "        # Находим самую частую пару\n",
        "        best = max(pairs, key=pairs.get)\n",
        "\n",
        "        # Сохраняем операцию слияния\n",
        "        bpe_codes[best] = i\n",
        "\n",
        "        # Применяем слияние ко всему словарю\n",
        "        vocab = merge_vocab(best, vocab)\n",
        "\n",
        "        print(f\"Слияние #{i+1}: {best} -> {''.join(best)} (частота: {pairs[best]})\")\n",
        "\n",
        "    return bpe_codes, vocab"
      ],
      "metadata": {
        "id": "M1iVHZezVC8W"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение алгоритма BPE:**\n",
        "1. **Подготовка данных:**\n",
        "   - Начинаем с разделения каждого слова на отдельные символы.\n",
        "   - Например, \"собака\" → \"с о б а к а\".\n",
        "\n",
        "2. **Процесс обучения:**\n",
        "   - `get_stats`: Подсчитывает, как часто встречаются пары соседних символов.\n",
        "   - Находим самую частую пару (например, \"с о\" встречается 100 раз).\n",
        "   - `merge_vocab`: Объединяем эту пару в один токен (\"с о\" → \"со\").\n",
        "   - Сохраняем эту операцию слияния в словарь `bpe_codes`.\n",
        "   - Повторяем процесс заданное количество раз (num_merges).\n",
        "\n",
        "3. **Результат:**\n",
        "   - Получаем словарь операций слияния `bpe_codes`.\n",
        "   - И преобразованный словарь `vocab`, где слова уже разбиты на подтокены по правилам BPE.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без BPE наш словарь будет содержать только целые слова, что приведет к проблеме с неизвестными словами.\n",
        "- Если увеличить `num_merges`, мы получим больше операций слияния, что приведет к более крупным токенам.\n",
        "- Если уменьшить `num_merges`, токены будут меньше, ближе к символам."
      ],
      "metadata": {
        "id": "hPuE7b_MZzGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Построение словаря BPE"
      ],
      "metadata": {
        "id": "fhOHPu5HZ71p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовка данных для BPE: получаем плоский список всех токенов\n",
        "flat_tokens = []\n",
        "for sentence in normalized_sentences:\n",
        "    flat_tokens.extend(sentence)\n",
        "\n",
        "# Обучение модели BPE\n",
        "num_merges = 15  # Количество операций слияния\n",
        "bpe_codes, bpe_vocabulary = learn_bpe(flat_tokens, num_merges)\n",
        "\n",
        "print(\"\\nСловарь операций слияния BPE:\")\n",
        "for pair, index in bpe_codes.items():\n",
        "    print(f\"{pair} -> {''.join(pair)}, индекс операции: {index}\")\n",
        "\n",
        "print(\"\\nПример преобразованных слов после BPE:\")\n",
        "# Показываем первые 5 преобразованных слов\n",
        "sample_items = list(bpe_vocabulary.items())[:5]\n",
        "for encoded, freq in sample_items:\n",
        "    original = encoded.replace(' ', '')\n",
        "    print(f\"Оригинал: {original}, Кодированное: {encoded}, Частота: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFWACGn2VHYQ",
        "outputId": "fa444fb9-b0be-403f-f4f6-8e6a39296b39"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Слияние #1: ('н', 'ы') -> ны (частота: 11)\n",
            "Слияние #2: ('т', 'ь') -> ть (частота: 11)\n",
            "Слияние #3: ('ны', 'й') -> ный (частота: 9)\n",
            "Слияние #4: ('о', 'в') -> ов (частота: 9)\n",
            "Слияние #5: ('т', 'е') -> те (частота: 7)\n",
            "Слияние #6: ('е', 'н') -> ен (частота: 6)\n",
            "Слияние #7: ('о', 'б') -> об (частота: 6)\n",
            "Слияние #8: ('ов', 'а') -> ова (частота: 6)\n",
            "Слияние #9: ('п', 'о') -> по (частота: 6)\n",
            "Слияние #10: ('р', 'а') -> ра (частота: 6)\n",
            "Слияние #11: ('и', 'с') -> ис (частота: 4)\n",
            "Слияние #12: ('с', 'т') -> ст (частота: 4)\n",
            "Слияние #13: ('т', 'о') -> то (частота: 4)\n",
            "Слияние #14: ('к', 'о') -> ко (частота: 4)\n",
            "Слияние #15: ('н', 'а') -> на (частота: 4)\n",
            "\n",
            "Словарь операций слияния BPE:\n",
            "('н', 'ы') -> ны, индекс операции: 0\n",
            "('т', 'ь') -> ть, индекс операции: 1\n",
            "('ны', 'й') -> ный, индекс операции: 2\n",
            "('о', 'в') -> ов, индекс операции: 3\n",
            "('т', 'е') -> те, индекс операции: 4\n",
            "('е', 'н') -> ен, индекс операции: 5\n",
            "('о', 'б') -> об, индекс операции: 6\n",
            "('ов', 'а') -> ова, индекс операции: 7\n",
            "('п', 'о') -> по, индекс операции: 8\n",
            "('р', 'а') -> ра, индекс операции: 9\n",
            "('и', 'с') -> ис, индекс операции: 10\n",
            "('с', 'т') -> ст, индекс операции: 11\n",
            "('т', 'о') -> то, индекс операции: 12\n",
            "('к', 'о') -> ко, индекс операции: 13\n",
            "('н', 'а') -> на, индекс операции: 14\n",
            "\n",
            "Пример преобразованных слов после BPE:\n",
            "Оригинал: искусственный, Кодированное: ис к у с ст в ен ный, Частота: 1\n",
            "Оригинал: интеллект, Кодированное: и н те л л е к т, Частота: 2\n",
            "Оригинал: ия, Кодированное: и я, Частота: 2\n",
            "Оригинал: это, Кодированное: э то, Частота: 1\n",
            "Оригинал: область, Кодированное: об л а сть, Частота: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- Создаем `flat_tokens` - плоский список всех токенов из всех предложений.\n",
        "- Запускаем процесс обучения BPE с заданным числом слияний (15).\n",
        "- Получаем словарь операций слияния `bpe_codes` и преобразованный словарь `bpe_vocabulary`.\n",
        "- Выводим операции слияния, чтобы увидеть, какие пары символов объединялись.\n",
        "- Показываем примеры слов после применения BPE кодирования.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Изменение `num_merges` влияет на грануляцию токенизации:\n",
        "  - Больше слияний = более крупные токены, меньший словарь, но хуже обобщение.\n",
        "  - Меньше слияний = более мелкие токены, больший словарь, лучше обобщение на новые слова.\n",
        "\n"
      ],
      "metadata": {
        "id": "ekBcYnITaDdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Применение BPE к новому тексту и преобразование в идентификаторы"
      ],
      "metadata": {
        "id": "Bl7Td4DVaT63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_bpe_to_word(word, bpe_codes):\n",
        "    \"\"\"\n",
        "    Применяет обученную модель BPE к слову.\n",
        "    \"\"\"\n",
        "    # Разделяем слово на символы\n",
        "    word = ' '.join(list(word))\n",
        "\n",
        "    # Применяем операции слияния в порядке их изучения\n",
        "    for pair, _ in sorted(bpe_codes.items(), key=lambda x: x[1]):\n",
        "        bigram = ' '.join(pair)\n",
        "        replacement = ''.join(pair)\n",
        "        word = word.replace(bigram, replacement)\n",
        "\n",
        "    return word.split()\n",
        "\n",
        "def tokenize_with_bpe(text, bpe_codes):\n",
        "    \"\"\"\n",
        "    Токенизирует текст с помощью BPE.\n",
        "    \"\"\"\n",
        "    # Сначала разбиваем на предложения и слова\n",
        "    sentences = sent_tokenize(text)\n",
        "    result = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        normalized = normalize_tokens(tokens)\n",
        "\n",
        "        # Применяем BPE к каждому нормализованному токену\n",
        "        bpe_tokens = []\n",
        "        for token in normalized:\n",
        "            bpe_tokens.extend(apply_bpe_to_word(token, bpe_codes))\n",
        "\n",
        "        result.append(bpe_tokens)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Создаем словарь из BPE токенов\n",
        "bpe_token_to_id = {}\n",
        "id_counter = 0\n",
        "\n",
        "for word in bpe_vocabulary:\n",
        "    for token in word.split():\n",
        "        if token not in bpe_token_to_id:\n",
        "            bpe_token_to_id[token] = id_counter\n",
        "            id_counter += 1\n",
        "\n",
        "# Применяем BPE к новому тексту\n",
        "sample_text = \"Нейронные сети обрабатывают данные.\"\n",
        "bpe_tokenized = tokenize_with_bpe(sample_text, bpe_codes)\n",
        "\n",
        "# Преобразуем в идентификаторы\n",
        "token_ids = []\n",
        "for sentence in bpe_tokenized:\n",
        "    for token in sentence:\n",
        "        if token in bpe_token_to_id:\n",
        "            token_ids.append(bpe_token_to_id[token])\n",
        "        else:\n",
        "            # Можно добавить специальный токен для неизвестных слов\n",
        "            print(f\"Неизвестный токен: {token}\")\n",
        "\n",
        "print(\"\\nПример преобразования текста в идентификаторы токенов:\")\n",
        "print(f\"Исходный текст: {sample_text}\")\n",
        "print(f\"Токены после BPE: {bpe_tokenized}\")\n",
        "print(f\"Идентификаторы токенов: {token_ids}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ-mrKJ_VVAF",
        "outputId": "337b606b-0fc0-4115-e888-c68336a19a56"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Пример преобразования текста в идентификаторы токенов:\n",
            "Исходный текст: Нейронные сети обрабатывают данные.\n",
            "Токены после BPE: [['н', 'е', 'й', 'р', 'о', 'н', 'ный', 'с', 'е', 'ть', 'об', 'ра', 'б', 'а', 'т', 'ы', 'в', 'а', 'ть', 'д', 'а', 'ть']]\n",
            "Идентификаторы токенов: [9, 12, 28, 25, 30, 9, 7, 3, 12, 32, 17, 43, 38, 18, 13, 27, 5, 18, 32, 34, 18, 32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "1. **Функция `apply_bpe_to_word`:**\n",
        "   - Разбивает слово на символы.\n",
        "   - Применяет операции слияния в том порядке, в котором они были изучены.\n",
        "   - Возвращает список подтокенов после применения BPE.\n",
        "\n",
        "2. **Функция `tokenize_with_bpe`:**\n",
        "   - Разбивает текст на предложения и слова.\n",
        "   - Нормализует слова (нижний регистр, лемматизация).\n",
        "   - Применяет BPE к каждому нормализованному слову.\n",
        "   - Возвращает список списков токенов для каждого предложения.\n",
        "\n",
        "3. **Создание словаря идентификаторов:**\n",
        "   - Мы присваиваем уникальный числовой ID каждому подтокену из нашего BPE словаря.\n",
        "\n",
        "4. **Применение к новому тексту:**\n",
        "   - Берем новый пример текста.\n",
        "   - Применяем все шаги обработки: токенизация → нормализация → BPE.\n",
        "   - Преобразуем получившиеся токены в идентификаторы.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без функции применения BPE мы не сможем обрабатывать новые тексты с помощью нашего метода.\n",
        "- Изменение порядка применения операций слияния повлияет на результат токенизации, поэтому важно соблюдать тот же порядок, что и при обучении."
      ],
      "metadata": {
        "id": "Ln0aUZ_RaaMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Статистика по полученным результатам\n",
        "print(\"\\nИтоговая статистика:\")\n",
        "print(f\"Количество предложений в тексте: {len(sentences)}\")\n",
        "print(f\"Общее количество токенов до нормализации: {sum(len(s) for s in tokenized_sentences)}\")\n",
        "print(f\"Общее количество токенов после нормализации: {sum(len(s) for s in normalized_sentences)}\")\n",
        "print(f\"Размер исходного словаря (уникальных слов): {len(vocabulary)}\")\n",
        "print(f\"Количество операций слияния BPE: {len(bpe_codes)}\")\n",
        "print(f\"Размер BPE словаря: {len(bpe_token_to_id)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2lE2EhGVXw9",
        "outputId": "b3858aa3-3d4f-4fee-b705-4e50163af6d0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Итоговая статистика:\n",
            "Количество предложений в тексте: 4\n",
            "Общее количество токенов до нормализации: 69\n",
            "Общее количество токенов после нормализации: 58\n",
            "Размер исходного словаря (уникальных слов): 50\n",
            "Количество операций слияния BPE: 15\n",
            "Размер BPE словаря: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подробное объяснение технологии токенизации и BPE кодирования\n",
        "\n",
        "## 1. Зачем нужна токенизация текста\n",
        "\n",
        "**Что это такое:** Токенизация — это процесс разделения текста на более мелкие части (токены), которые могут быть словами, частями слов или даже отдельными символами.\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Компьютеры не могут напрямую работать с текстом — им нужны числа. Токенизация позволяет преобразовать текст в последовательность числовых идентификаторов.\n",
        "- Языковые модели обучаются предсказывать следующий токен на основе предыдущих. Без токенизации модель не сможет работать с текстом.\n",
        "- Токенизация определяет уровень грануляции, на котором модель \"понимает\" текст.\n",
        "\n",
        "**От чего зависит качество:**\n",
        "- От выбранного алгоритма токенизации\n",
        "- От особенностей языка (для русского важно учитывать богатую морфологию)\n",
        "- От специфики текстов в вашем датасете\n",
        "\n",
        "**Технические последствия:**\n",
        "- Чем больше размер словаря (количество уникальных токенов), тем больше памяти требуется\n",
        "- Слишком маленький словарь приведет к потере информации\n",
        "- Слишком большой словарь усложнит обучение модели и потребует больше данных\n",
        "\n",
        "## 2. Разделение на предложения\n",
        "\n",
        "**Что происходит:** Текст разбивается на отдельные предложения с помощью функции `sent_tokenize` из библиотеки NLTK.\n",
        "\n",
        "**Как это работает:**\n",
        "- Алгоритм анализирует знаки препинания (точки, восклицательные знаки, вопросительные знаки)\n",
        "- Учитывает исключения (сокращения, цифры с точками, инициалы)\n",
        "- Использует обученную модель, которая разбирает различные случаи на основе статистических паттернов\n",
        "\n",
        "**Зачем разделять на предложения:**\n",
        "- Предложение — логическая единица смысла в тексте\n",
        "- Обработка по предложениям эффективнее, чем обработка всего текста сразу\n",
        "- Многие задачи NLP (анализ тональности, классификация) работают на уровне предложений\n",
        "- Связи между словами в разных предложениях обычно слабее, чем внутри одного предложения\n",
        "\n",
        "**От чего зависит точность:**\n",
        "- Качество и правильность пунктуации в исходном тексте\n",
        "- Наличие специфических конструкций (прямая речь, списки, цитаты)\n",
        "- Язык текста и его соответствие обученной модели токенизатора\n",
        "\n",
        "## 3. Разделение предложений на токены (слова)\n",
        "\n",
        "**Что происходит:** Каждое предложение разбивается на слова и знаки препинания с помощью функции `word_tokenize`.\n",
        "\n",
        "**Как это работает:**\n",
        "- Алгоритм разделяет текст по пробелам и знакам препинания\n",
        "- Учитывает сложные случаи (апострофы, дефисы, сокращения)\n",
        "- Сохраняет знаки препинания как отдельные токены\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Слова — базовые единицы значения в языке\n",
        "- Разделение текста на слова позволяет анализировать структуру предложений\n",
        "- Создает основу для дальнейшей нормализации и обработки\n",
        "\n",
        "**Технические нюансы:**\n",
        "- В некоторых языках (например, китайском или японском) разделение на слова сложнее, так как нет явных разделителей\n",
        "- Специальные конструкции (email-адреса, URL, даты) требуют особой обработки\n",
        "- В русском языке важно правильно обрабатывать составные слова с дефисами\n",
        "\n",
        "## 4. Нормализация токенов\n",
        "\n",
        "**Что происходит:** Токены (слова) приводятся к стандартной форме через три основных процесса:\n",
        "1. Приведение к нижнему регистру\n",
        "2. Удаление знаков препинания\n",
        "3. Лемматизация (приведение к начальной форме)\n",
        "\n",
        "**Как работает лемматизация:**\n",
        "- Для русского языка используется библиотека `pymorphy2`\n",
        "- Анализируется морфологическая структура слова\n",
        "- Определяется часть речи и грамматические характеристики\n",
        "- Слово преобразуется к начальной форме:\n",
        "  - существительные → единственное число, именительный падеж\n",
        "  - глаголы → инфинитив\n",
        "  - прилагательные → мужской род, ед. число, именительный падеж\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Снижает количество уникальных токенов (размер словаря)\n",
        "- Объединяет разные формы одного слова: \"книга\", \"книги\", \"книгами\" → \"книга\"\n",
        "- Позволяет модели видеть связь между разными формами одного слова\n",
        "- Улучшает статистические показатели частотности слов\n",
        "\n",
        "**От чего зависит качество:**\n",
        "- От используемого морфологического анализатора\n",
        "- От особенностей языка (для русского лемматизация особенно важна из-за богатства словоформ)\n",
        "- От предметной области текста (специальные термины могут неверно лемматизироваться)\n",
        "\n",
        "## 5. Создание словаря (vocabulary)\n",
        "\n",
        "**Что происходит:** После нормализации создается словарь всех уникальных токенов, где каждому токену присваивается уникальный идентификатор и подсчитывается его частота в тексте.\n",
        "\n",
        "**Как это работает:**\n",
        "- Используется структура данных `Counter` для подсчета встречаемости каждого токена\n",
        "- Токены сортируются по частоте (от наиболее к наименее частым)\n",
        "- Каждому токену присваивается числовой идентификатор (ID)\n",
        "- Создается словарь, где ключ — токен, а значение — объект с его ID и частотой\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Языковые модели работают с числами, а не текстом\n",
        "- Словарь позволяет преобразовывать текст в последовательность чисел и обратно\n",
        "- Частотность токенов используется для оптимизации представления (часто встречающиеся токены получают меньшие ID)\n",
        "- Словарь определяет, какие слова \"знает\" модель\n",
        "\n",
        "**Технические соображения:**\n",
        "- Размер словаря прямо влияет на размер модели и требования к памяти\n",
        "- Слишком большой словарь приводит к разреженным представлениям и проблемам с обучением\n",
        "- Слишком маленький словарь вызывает проблему неизвестных слов (OOV — out-of-vocabulary)\n",
        "\n",
        "## 6. Алгоритм Byte-Pair Encoding (BPE)\n",
        "\n",
        "**Что это такое:** BPE — алгоритм сжатия данных, который в NLP используется для создания подсловных токенов, позволяющих эффективно представлять как частые, так и редкие слова.\n",
        "\n",
        "**Как работает:**\n",
        "1. **Начальное состояние:** Каждое слово разбивается на отдельные символы, разделенные пробелами\n",
        "2. **Итеративный процесс:**\n",
        "   - Подсчитываем частоту всех пар соседних символов/токенов\n",
        "   - Находим самую частую пару\n",
        "   - Объединяем эту пару в один новый токен\n",
        "   - Заменяем все вхождения этой пары в словаре на новый токен\n",
        "   - Повторяем процесс заданное число раз (num_merges)\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Решает проблему неизвестных слов, разбивая редкие слова на подсловные части\n",
        "- Более эффективно использует словарь, чем пословная токенизация\n",
        "- Позволяет модели улавливать морфологические особенности языка\n",
        "- Обеспечивает баланс между размером словаря и способностью представлять любые слова\n",
        "\n",
        "**От чего зависит эффективность:**\n",
        "- От количества операций слияния (num_merges):\n",
        "  - Малое количество → мелкие токены, ближе к посимвольному представлению\n",
        "  - Большое количество → крупные токены, ближе к пословному представлению\n",
        "- От размера и разнообразия тренировочного корпуса\n",
        "- От языковых особенностей (например, для агглютинативных языков BPE особенно эффективен)\n",
        "\n",
        "**Конкретные технические эффекты:**\n",
        "- BPE с 10-15 тысячами операций слияния обычно создает словарь размером 30-50 тысяч токенов\n",
        "- Частые слова представляются одним токеном, редкие разбиваются на несколько подтокенов\n",
        "- Слово \"переобучение\" может быть разбито на \"пере\" + \"обучение\", если эти части чаще встречаются по отдельности\n",
        "\n",
        "## 7. Применение BPE к новому тексту\n",
        "\n",
        "**Что происходит:** Когда приходит новый текст, мы применяем весь процесс обработки и используем ранее полученные правила BPE для его токенизации.\n",
        "\n",
        "**Как это работает:**\n",
        "1. Текст разбивается на предложения\n",
        "2. Предложения токенизируются на слова\n",
        "3. Слова нормализуются (нижний регистр, лемматизация)\n",
        "4. Каждое слово разбивается на символы\n",
        "5. К слову последовательно применяются операции слияния, в том же порядке, как они были найдены при обучении\n",
        "6. Полученные подтокены преобразуются в числовые идентификаторы\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Обеспечивает единообразное представление как тренировочных, так и новых данных\n",
        "- Позволяет модели работать с ранее не встречавшимися словами\n",
        "- Создает числовое представление текста, пригодное для обработки нейронными сетями\n",
        "\n",
        "**Технические особенности:**\n",
        "- Порядок применения операций слияния критически важен\n",
        "- Если токен не был встречен при обучении, можно использовать специальный токен [UNK] (unknown)\n",
        "- Некоторые реализации используют дополнительные специальные токены:\n",
        "  - [BOS]/[SOS] — начало предложения (Beginning/Start of Sentence)\n",
        "  - [EOS] — конец предложения (End of Sentence)\n",
        "  - [PAD] — заполнитель для выравнивания длины последовательностей\n",
        "\n",
        "## 8. От чего зависит качество всего процесса\n",
        "\n",
        "**Размер и качество обучающего корпуса:**\n",
        "- Больший корпус → лучшее покрытие языка\n",
        "- Разнообразие текстов → лучшая обобщающая способность\n",
        "- Качество текстов → меньше шума и ошибок в данных\n",
        "\n",
        "**Параметры токенизации:**\n",
        "- Выбор метода токенизации (WordPiece, BPE, Unigram и др.)\n",
        "- Размер словаря (маленький — компактность, большой — точность)\n",
        "- Количество операций слияния в BPE (баланс между детализацией и обобщением)\n",
        "\n",
        "**Предобработка текста:**\n",
        "- Качество нормализации (лемматизация vs стемминг)\n",
        "- Обработка специальных случаев (числа, даты, URL)\n",
        "- Удаление или сохранение пунктуации\n",
        "\n",
        "**Применение в языковой модели:**\n",
        "- Способ векторизации токенов (one-hot, embeddings)\n",
        "- Архитектура модели (RNN, Transformer)\n",
        "- Контекстное окно (сколько предыдущих токенов учитывается)\n",
        "\n",
        "## 9. Практическое значение всего процесса\n",
        "\n",
        "**Для языковых моделей:**\n",
        "- BPE позволяет эффективно работать со словарем ограниченного размера\n",
        "- Подсловные токены помогают улавливать морфологические и семантические связи\n",
        "- Сокращается количество неизвестных слов, улучшается обобщающая способность\n",
        "\n",
        "**Для практических приложений:**\n",
        "- Уменьшает требования к памяти (по сравнению с посимвольной токенизацией)\n",
        "- Улучшает работу с редкими словами и новыми терминами\n",
        "- Повышает эффективность машинного перевода, генерации текста, классификации и других задач NLP\n",
        "\n",
        "**Для многоязычных моделей:**\n",
        "- BPE позволяет создать общий словарь для нескольких языков\n",
        "- Обнаруживает общие морфемы между родственными языками\n",
        "- Эффективно работает как с аналитическими, так и с синтетическими языками\n",
        "\n",
        "Понимание процесса токенизации и BPE кодирования критически важно для работы с современными языковыми моделями, так как это первый и один из самых важных шагов в преобразовании текста в форму, понятную компьютеру."
      ],
      "metadata": {
        "id": "_mXudGZ8ccB1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1gHYS6OLcfpn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}