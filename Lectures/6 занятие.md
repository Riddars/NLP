# ПЕРЕСКАЗ ЛЕКЦИИ: ПОСЛЕДОВАТЕЛЬНЫЕ МОДЕЛИ В NLP

## 1. ОСНОВЫ ПОСЛЕДОВАТЕЛЬНЫХ МОДЕЛЕЙ

**Ключевые концепции:**
- **Последовательные модели** — архитектуры нейронных сетей, способные обрабатывать данные, представленные в виде последовательностей (текст, речь, временные ряды)
- **Токены** — базовые единицы последовательности (обычно слова или части слов в NLP)
- **Скрытое состояние (hidden state)** — внутреннее представление, сохраняющее информацию о предыдущих элементах последовательности

## 2. РЕКУРРЕНТНЫЕ НЕЙРОННЫЕ СЕТИ (RNN)

**Архитектура RNN:**
- **Фундаментальный принцип**: каждый элемент последовательности обрабатывается с учетом информации о предыдущих элементах
- **Передача информации**: на каждом временном шаге t модель получает:
  - Текущий вход x(t)
  - Скрытое состояние h(t-1) от предыдущего шага
- **Математически**: h(t) = f(W·x(t) + U·h(t-1) + b), где W, U — обучаемые матрицы весов, b — смещение

**Типы задач для RNN:**
1. **Разметка последовательности** (sequence tagging):
   - Назначение метки каждому элементу последовательности (NER, POS-tagging)
   - Выход на каждом шаге обработки

2. **Классификация последовательности** (sequence classification):
   - Присвоение метки всей последовательности
   - Использование финального скрытого состояния или агрегации всех состояний

3. **Языковое моделирование** (language modeling):
   - Предсказание следующего элемента последовательности
   - Обучение через предсказание следующего слова

**Усовершенствованные архитектуры RNN:**

1. **Стекированные RNN**:
   - Несколько слоев рекуррентных блоков, расположенных друг над другом
   - Каждый уровень добавляет более высокий уровень абстракции
   - Повышает моделирующую способность сети
   - Формально: выход нижнего слоя RNN становится входом для верхнего слоя

2. **Двунаправленные RNN**:
   - Две отдельные RNN обрабатывают последовательность в разных направлениях:
     * Прямая (→): от начала к концу
     * Обратная (←): от конца к началу
   - Скрытые состояния обеих RNN конкатенируются для получения выхода
   - Преимущество: учет как предыдущего, так и последующего контекста
   - Увеличивает количество параметров модели вдвое

## 3. ПРОБЛЕМЫ СТАНДАРТНЫХ RNN

**Проблема исчезающего градиента**:
- При обратном распространении ошибки через время градиенты могут становиться очень малыми
- Затрудняет обучение на длинных последовательностях
- Параметры ранних слоев почти не обновляются

**Проблема долгосрочных зависимостей**:
- Иллюстрация: предложение "The flights the airlines was canceled"
- RNN может "запутаться", к чему относится слово "was" — к "flights" или "airlines"
- Сложно поддерживать контекст на длинных дистанциях
- Информация от начала последовательности может "затухать" к её концу

## 4. LSTM: АРХИТЕКТУРА И КОМПОНЕНТЫ

**Long Short-Term Memory (LSTM)** — специальная архитектура RNN, разработанная для решения проблем стандартных RNN.

**Ключевая идея**: селективное сохранение или забывание информации через механизм "ворот" (gates).

**Компоненты LSTM**:

1. **Forget Gate (ворота забывания)**:
   - Определяет, какую информацию из контекстного вектора следует забыть
   - Математически: f(t) = σ(W_f·x(t) + U_f·h(t-1) + b_f)
   - Значения между 0 и 1: 0 — "забыть", 1 — "сохранить"
   - Работает как маска для контекстного вектора

2. **Input Gate (ворота входа)**:
   - Определяет, какую новую информацию запомнить
   - Математически: i(t) = σ(W_i·x(t) + U_i·h(t-1) + b_i)
   - Контролирует поступление новой информации в контекстный вектор

3. **Update Candidate (кандидат на обновление)**:
   - Формирует новую информацию для добавления в контекстный вектор
   - Математически: c̃(t) = tanh(W_c·x(t) + U_c·h(t-1) + b_c)
   - Использует tanh вместо сигмоиды для получения значений от -1 до 1

4. **Обновление состояния ячейки**:
   - Комбинация забытой информации и новой информации
   - Математически: C(t) = f(t)⊙C(t-1) + i(t)⊙c̃(t)
   - ⊙ обозначает поэлементное умножение

5. **Output Gate (ворота выхода)**:
   - Определяет, какая информация из ячейки будет подаваться на выход
   - Математически: o(t) = σ(W_o·x(t) + U_o·h(t-1) + b_o)
   - Скрытое состояние: h(t) = o(t)⊙tanh(C(t))

**Преимущества LSTM**:
- Решает проблему исчезающего градиента через "магистраль" контекстного вектора
- Лучше сохраняет долгосрочные зависимости
- Двунаправленный LSTM (BiLSTM) был state-of-the-art для задач NLP до появления трансформеров

## 5. SEQUENCE-TO-SEQUENCE МОДЕЛИ

**Архитектура Encoder-Decoder**:
- **Назначение**: преобразование одной последовательности в другую (перевод, суммаризация)
- **Компоненты**:
  1. **Энкодер**: читает входную последовательность, создает представление (контекстный вектор)
  2. **Контекстный вектор**: связующее звено между энкодером и декодером
  3. **Декодер**: генерирует выходную последовательность на основе контекстного вектора

**Процесс работы**:
1. **Энкодирование**:
   - Входная последовательность проходит через энкодер (обычно RNN/LSTM)
   - Формируются скрытые состояния для каждого элемента
   - Создается контекстный вектор (последнее состояние или функция от всех состояний)

2. **Декодирование при обучении**:
   - Декодер получает:
     * Контекстный вектор
     * Токен начала последовательности
     * Последовательно все правильные токены (teacher forcing)
   - На каждом шаге предсказывает следующий токен
   - Teacher forcing: использование правильных токенов вместо предсказанных для избежания накопления ошибок

3. **Декодирование при выводе (inference)**:
   - Начинается с контекстного вектора и токена начала последовательности
   - Каждый предсказанный токен используется как вход для следующего шага
   - Процесс продолжается до генерации токена конца последовательности

**Использование контекстного вектора**:
- **Базовый подход**: использование как начального скрытого состояния декодера
- **Улучшенный подход**: конкатенация с входом на каждом шаге декодера

## 6. ПЕРСПЕКТИВЫ РАЗВИТИЯ АРХИТЕКТУР

**Ограничения Seq2Seq моделей**:
- Контекстный вектор фиксированного размера создает "бутылочное горлышко"
- Снижение эффективности на длинных последовательностях

**Следующие шаги в эволюции архитектур**:
1. **Механизм внимания (Attention Mechanism)**:
   - Позволяет декодеру фокусироваться на разных частях входной последовательности
   - Снимает ограничение "бутылочного горлышка"
   - Улучшает работу с длинными последовательностями

2. **Трансформеры**:
   - Полностью заменяют рекуррентные блоки механизмами внимания
   - Позволяют параллельную обработку последовательности
   - Текущий state-of-the-art для большинства задач NLP

---

### ДОПОЛНИТЕЛЬНЫЕ ЗАМЕЧАНИЯ:

- **Выбор архитектуры**:
  * RNN подходят для простых задач и ограниченных наборов данных (меньше параметров)
  * LSTM/GRU эффективнее для сложных задач с длинными зависимостями
  * BiLSTM превосходит однонаправленные модели для большинства задач NLP

- **Практические рекомендации**:
  * Для текстовых задач обычно предпочтительнее использовать BiLSTM или трансформеры
  * При ограниченных вычислительных ресурсах или небольших наборах данных RNN/GRU могут быть более эффективными
  * В современных задачах обработки текста трансформеры и модели на их основе (BERT, GPT и т.д.) заменили рекуррентные архитектуры

- **Реализация**:
  * Большинство современных фреймворков (PyTorch, TensorFlow) предоставляют готовые реализации RNN, LSTM, GRU и Seq2Seq моделей
  * При практическом использовании обычно не требуется реализация этих архитектур с нуля
