{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n"
      ],
      "metadata": {
        "id": "Es6TE8Ucie6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Установка и импорт библиотек"
      ],
      "metadata": {
        "id": "t3LGO8f-XxEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка необходимых библиотек для работы с текстом на русском языке\n",
        "!pip install nltk pymorphy2\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tLfpGuBREqU",
        "outputId": "5bfb38fb-edd3-4193-f923-83622f247785"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.11/dist-packages (0.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.11/dist-packages (from pymorphy2) (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- `nltk` - это Natural Language Toolkit, библиотека для обработки естественного языка. Она содержит инструменты для токенизации текста.\n",
        "- `pymorphy2` - морфологический анализатор для русского языка, который поможет нам с лемматизацией (приведением слов к начальной форме).\n",
        "- `re` - модуль для работы с регулярными выражениями, который пригодится для сложных операций с текстом.\n",
        "- `Counter` и `defaultdict` из модуля `collections` - структуры данных для подсчёта элементов и создания словарей с значениями по умолчанию.\n",
        "- `string` - содержит константы, такие как `string.punctuation` (набор знаков препинания).\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без этих библиотек мы не сможем выполнить задание, так как они предоставляют базовый функционал для обработки текста.\n",
        "- Можно заменить некоторые библиотеки аналогами (например, вместо `pymorphy2` использовать `natasha`), но потребуется изменить соответствующий код."
      ],
      "metadata": {
        "id": "pR5uvglpX54C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка ресурсов NLTK"
      ],
      "metadata": {
        "id": "98iLym5OYJAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка ресурсов NLTK для токенизации и работы со стоп-словами\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAyFN00YRH8h",
        "outputId": "c49a9f63-1594-40ed-e7cc-257bcc04de4f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- `nltk.download('punkt')` - загружает модель для разделения текста на предложения (пунктуация).\n",
        "- `nltk.download('stopwords')` - загружает списки стоп-слов (часто встречающиеся слова, которые обычно не несут значимой информации).\n",
        "- `sent_tokenize` - функция для разделения текста на предложения.\n",
        "- `word_tokenize` - функция для разделения предложений на слова."
      ],
      "metadata": {
        "id": "Tluk8bfHYWuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример текста для обработки (можно заменить на любой другой)\n",
        "text = \"\"\"\n",
        "Машинное обучение — класс методов искусственного интеллекта, характерной чертой которых является не прямое решение задачи,\n",
        "а обучение в процессе применения решений множества сходных задач. Для построения таких методов используются средства математической статистики,\n",
        "численных методов, методов оптимизации, теории вероятностей, теории графов, различные техники работы с данными в цифровой форме.\n",
        "\n",
        "Машинное обучение тесно связано и часто пересекается с вычислительной статистикой, которая также специализируется\n",
        "на прогнозировании с помощью компьютеров. Машинное обучение имеет широкое приложение и используется в компьютерном зрении,\n",
        "распознавании речи, обработке естественного языка, прогнозировании временных рядов, обнаружении мошенничества и манипуляций\n",
        "с рынком, анализе фондового рынка, классификации ДНК-последовательностей, распознавании образов и машинного восприятия,\n",
        "обнаружении спама, распознавании рукописного ввода, игровых программах и робототехнике.\n",
        "\n",
        "Глубокое обучение — совокупность методов машинного обучения, основанных на обучении представлениям, а не на специализированных\n",
        "алгоритмах под конкретные задачи. Многие методы глубокого обучения используют нейронные сети, поэтому глубокое обучение часто\n",
        "ассоциируется с искусственными нейронными сетями. Нейронные сети содержат несколько скрытых слоев, которые преобразуют входные данные\n",
        "в более абстрактные и композиционные представления.\n",
        "\n",
        "Трансформеры — архитектура нейронной сети, полагающаяся на механизм самовнимания для взвешивания относительной важности каждой части входных данных.\n",
        "Они были представлены в статье «Внимание — это всё, что вам нужно» в 2017 году и широко применяются в обработке естественного языка.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Исходный текст:\")\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX8gTnGSRIzD",
        "outputId": "ef25ed61-3048-436b-f7b8-604f27542bee"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходный текст:\n",
            "\n",
            "Машинное обучение — класс методов искусственного интеллекта, характерной чертой которых является не прямое решение задачи, \n",
            "а обучение в процессе применения решений множества сходных задач. Для построения таких методов используются средства математической статистики, \n",
            "численных методов, методов оптимизации, теории вероятностей, теории графов, различные техники работы с данными в цифровой форме.\n",
            "\n",
            "Машинное обучение тесно связано и часто пересекается с вычислительной статистикой, которая также специализируется \n",
            "на прогнозировании с помощью компьютеров. Машинное обучение имеет широкое приложение и используется в компьютерном зрении, \n",
            "распознавании речи, обработке естественного языка, прогнозировании временных рядов, обнаружении мошенничества и манипуляций \n",
            "с рынком, анализе фондового рынка, классификации ДНК-последовательностей, распознавании образов и машинного восприятия, \n",
            "обнаружении спама, распознавании рукописного ввода, игровых программах и робототехнике.\n",
            "\n",
            "Глубокое обучение — совокупность методов машинного обучения, основанных на обучении представлениям, а не на специализированных \n",
            "алгоритмах под конкретные задачи. Многие методы глубокого обучения используют нейронные сети, поэтому глубокое обучение часто \n",
            "ассоциируется с искусственными нейронными сетями. Нейронные сети содержат несколько скрытых слоев, которые преобразуют входные данные \n",
            "в более абстрактные и композиционные представления.\n",
            "\n",
            "Трансформеры — архитектура нейронной сети, полагающаяся на механизм самовнимания для взвешивания относительной важности каждой части входных данных. \n",
            "Они были представлены в статье «Внимание — это всё, что вам нужно» в 2017 году и широко применяются в обработке естественного языка.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нужно скачать punkt_tab в строке ниже"
      ],
      "metadata": {
        "id": "QoSe3faFSPcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxx04jhyR1LC",
        "outputId": "c1370884-cef0-4220-a187-ba82b340340e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt_tab\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package punkt_tab to /root/nltk_data...\n",
            "      Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Токенизация текста на предложения"
      ],
      "metadata": {
        "id": "8jWam8LXYhcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение текста на предложения\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(\"\\nРазделение на предложения:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"Предложение {i+1}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLAkSnICSHkK",
        "outputId": "b51ea4b3-7da2-4250-fd60-fdbbb0b7fe81"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Разделение на предложения:\n",
            "Предложение 1: \n",
            "Машинное обучение — класс методов искусственного интеллекта, характерной чертой которых является не прямое решение задачи, \n",
            "а обучение в процессе применения решений множества сходных задач.\n",
            "Предложение 2: Для построения таких методов используются средства математической статистики, \n",
            "численных методов, методов оптимизации, теории вероятностей, теории графов, различные техники работы с данными в цифровой форме.\n",
            "Предложение 3: Машинное обучение тесно связано и часто пересекается с вычислительной статистикой, которая также специализируется \n",
            "на прогнозировании с помощью компьютеров.\n",
            "Предложение 4: Машинное обучение имеет широкое приложение и используется в компьютерном зрении, \n",
            "распознавании речи, обработке естественного языка, прогнозировании временных рядов, обнаружении мошенничества и манипуляций \n",
            "с рынком, анализе фондового рынка, классификации ДНК-последовательностей, распознавании образов и машинного восприятия, \n",
            "обнаружении спама, распознавании рукописного ввода, игровых программах и робототехнике.\n",
            "Предложение 5: Глубокое обучение — совокупность методов машинного обучения, основанных на обучении представлениям, а не на специализированных \n",
            "алгоритмах под конкретные задачи.\n",
            "Предложение 6: Многие методы глубокого обучения используют нейронные сети, поэтому глубокое обучение часто \n",
            "ассоциируется с искусственными нейронными сетями.\n",
            "Предложение 7: Нейронные сети содержат несколько скрытых слоев, которые преобразуют входные данные \n",
            "в более абстрактные и композиционные представления.\n",
            "Предложение 8: Трансформеры — архитектура нейронной сети, полагающаяся на механизм самовнимания для взвешивания относительной важности каждой части входных данных.\n",
            "Предложение 9: Они были представлены в статье «Внимание — это всё, что вам нужно» в 2017 году и широко применяются в обработке естественного языка.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- `sent_tokenize(text)` разбивает текст на отдельные предложения, анализируя пунктуацию и структуру текста.\n",
        "- Функция возвращает список предложений, который мы сохраняем в переменной `sentences`.\n",
        "- Затем мы выводим каждое предложение с его порядковым номером для наглядности.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без этого шага мы не сможем обрабатывать текст на уровне предложений, что важно для многих задач NLP.\n",
        "- Можно заменить `sent_tokenize` на свою функцию, например, разделяя текст по символам `.`, `!`, `?`, но это менее надежно."
      ],
      "metadata": {
        "id": "7VzRmibTYlZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Токенизация предложений на слова"
      ],
      "metadata": {
        "id": "9kQ3OP1_YuVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизация предложений на слова\n",
        "tokenized_sentences = []\n",
        "for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tokenized_sentences.append(tokens)\n",
        "\n",
        "print(\"\\nПример токенизации предложения на слова:\")\n",
        "print(f\"Предложение: {sentences[0]}\")\n",
        "print(f\"Токены: {tokenized_sentences[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfNBXMz5SLNJ",
        "outputId": "763e4053-5ba9-4b84-e552-e5e520a43c34"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Пример токенизации предложения на слова:\n",
            "Предложение: \n",
            "Машинное обучение — класс методов искусственного интеллекта, характерной чертой которых является не прямое решение задачи, \n",
            "а обучение в процессе применения решений множества сходных задач.\n",
            "Токены: ['Машинное', 'обучение', '—', 'класс', 'методов', 'искусственного', 'интеллекта', ',', 'характерной', 'чертой', 'которых', 'является', 'не', 'прямое', 'решение', 'задачи', ',', 'а', 'обучение', 'в', 'процессе', 'применения', 'решений', 'множества', 'сходных', 'задач', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- Для каждого предложения из списка `sentences` мы применяем функцию `word_tokenize()`.\n",
        "- Эта функция разбивает предложение на отдельные слова и знаки препинания (токены).\n",
        "- Результаты сохраняются в список списков `tokenized_sentences`, где каждый вложенный список содержит токены одного предложения.\n",
        "- В конце выводим пример для первого предложения, чтобы наглядно показать результат.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без токенизации на слова мы не сможем анализировать и обрабатывать текст на уровне отдельных слов.\n",
        "- Можно заменить на разделение по пробелам (`sentence.split()`), но это не будет учитывать знаки препинания и другие нюансы."
      ],
      "metadata": {
        "id": "x7Sh7xayY20J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Нормализация токенов"
      ],
      "metadata": {
        "id": "kOvNBWJYYvNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импорт и настройка морфологического анализатора для русского языка\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Функция для нормализации русских токенов\n",
        "def normalize_tokens(tokens):\n",
        "    \"\"\"\n",
        "    Приводит токены к нормальной форме:\n",
        "    - переводит в нижний регистр\n",
        "    - удаляет знаки препинания\n",
        "    - выполняет лемматизацию (приведение к начальной форме)\n",
        "    \"\"\"\n",
        "    normalized_tokens = []\n",
        "    for token in tokens:\n",
        "        # Приведение к нижнему регистру\n",
        "        token = token.lower()\n",
        "\n",
        "        # Удаление знаков препинания\n",
        "        token = ''.join([char for char in token if char not in string.punctuation])\n",
        "\n",
        "        # Пропуск пустых токенов\n",
        "        if not token:\n",
        "            continue\n",
        "\n",
        "        # Лемматизация для русского языка с помощью pymorphy2\n",
        "        parsed_token = morph.parse(token)[0]\n",
        "        normalized = parsed_token.normal_form\n",
        "\n",
        "        normalized_tokens.append(normalized)\n",
        "\n",
        "    return normalized_tokens\n",
        "\n",
        "# Применение нормализации к нашим токенам\n",
        "normalized_sentences = []\n",
        "for tokens in tokenized_sentences:\n",
        "    normalized_tokens = normalize_tokens(tokens)\n",
        "    normalized_sentences.append(normalized_tokens)\n",
        "\n",
        "print(\"\\nПример нормализованного предложения:\")\n",
        "print(f\"Исходные токены: {tokenized_sentences[0]}\")\n",
        "print(f\"Нормализованные токены: {normalized_sentences[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IDhoKzcSYOB",
        "outputId": "117a727d-54a2-4e46-f498-ec4becc8eaea"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Пример нормализованного предложения:\n",
            "Исходные токены: ['Машинное', 'обучение', '—', 'класс', 'методов', 'искусственного', 'интеллекта', ',', 'характерной', 'чертой', 'которых', 'является', 'не', 'прямое', 'решение', 'задачи', ',', 'а', 'обучение', 'в', 'процессе', 'применения', 'решений', 'множества', 'сходных', 'задач', '.']\n",
            "Нормализованные токены: ['машинный', 'обучение', '—', 'класс', 'метод', 'искусственный', 'интеллект', 'характерный', 'черта', 'который', 'являться', 'не', 'прямой', 'решение', 'задача', 'а', 'обучение', 'в', 'процесс', 'применение', 'решение', 'множество', 'сходный', 'задача']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- Создаем функцию `normalize_tokens`, которая выполняет три основных шага нормализации:\n",
        "  1. **Приведение к нижнему регистру**: \"Слово\" → \"слово\"\n",
        "  2. **Удаление знаков препинания**: \"слово,\" → \"слово\"\n",
        "  3. **Лемматизация**: \"словами\" → \"слово\", \"бежали\" → \"бежать\"\n",
        "- Лемматизация использует `pymorphy2` — морфологический анализатор для русского языка, который приводит слова к их начальной форме.\n",
        "- Для каждого предложения применяем функцию нормализации и сохраняем результаты в `normalized_sentences`.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без нормализации разные формы одного слова будут считаться разными токенами (например, \"слово\", \"слова\", \"словами\" — это три разных токена).\n",
        "- Если убрать лемматизацию, но оставить приведение к нижнему регистру, результат будет менее точным, но всё равно лучше, чем исходный текст.\n",
        "- Можно заменить `pymorphy2` на другую библиотеку"
      ],
      "metadata": {
        "id": "ZhnKxSg1ZOpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создание словаря с индексами и частотами"
      ],
      "metadata": {
        "id": "U6HyIK7TZhUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание словаря из нормализованных токенов\n",
        "# Словарь будет содержать уникальные токены и их частоту\n",
        "vocabulary = {}\n",
        "token_counter = Counter()\n",
        "\n",
        "# Подсчет всех токенов\n",
        "for sentence in normalized_sentences:\n",
        "    token_counter.update(sentence)\n",
        "\n",
        "# Создание словаря с индексами и частотами\n",
        "for idx, (token, count) in enumerate(token_counter.most_common()):\n",
        "    vocabulary[token] = {\n",
        "        \"id\": idx,\n",
        "        \"count\": count\n",
        "    }\n",
        "\n",
        "print(f\"\\nРазмер словаря: {len(vocabulary)} уникальных токенов\")\n",
        "print(\"\\nПример 10 наиболее часто встречающихся токенов:\")\n",
        "for token, info in list(vocabulary.items())[:10]:\n",
        "    print(f\"{token}: встречается {info['count']} раз, id={info['id']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhOicy7mT5Lq",
        "outputId": "b0a33cdd-91aa-43e3-dd77-d27730af82f7"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Размер словаря: 130 уникальных токенов\n",
            "\n",
            "Пример 10 наиболее часто встречающихся токенов:\n",
            "обучение: встречается 9 раз, id=0\n",
            "в: встречается 7 раз, id=1\n",
            "и: встречается 7 раз, id=2\n",
            "метод: встречается 6 раз, id=3\n",
            "машинный: встречается 5 раз, id=4\n",
            "с: встречается 5 раз, id=5\n",
            "—: встречается 4 раз, id=6\n",
            "на: встречается 4 раз, id=7\n",
            "нейронный: встречается 4 раз, id=8\n",
            "сеть: встречается 4 раз, id=9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- Создаем словарь `vocabulary`, который будет содержать информацию о всех уникальных токенах.\n",
        "- Используем `Counter` для подсчета частоты каждого токена во всем тексте.\n",
        "- Для каждого уникального токена создаем запись в словаре, содержащую:\n",
        "  - `id`: уникальный идентификатор токена (порядковый номер)\n",
        "  - `count`: количество появлений токена в тексте\n",
        "- Токены сортируются по частоте с помощью `most_common()`, так что самые частые получают меньшие id.\n",
        "- В конце выводим размер словаря и 10 наиболее часто встречающихся токенов.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без словаря мы не сможем присваивать числовые идентификаторы токенам, что необходимо для обучения моделей машинного обучения.\n",
        "- Можно изменить порядок сортировки (например, по алфавиту), но это менее эффективно, так как частотные токены должны иметь меньшие ID.\n"
      ],
      "metadata": {
        "id": "9MJ4UHzeZXBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Реализация алгоритма Byte-Pair Encoding (BPE)"
      ],
      "metadata": {
        "id": "SPZ01ErbZlRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(vocab):\n",
        "    \"\"\"\n",
        "    Подсчитывает частоту пар символов во всех словах словаря.\n",
        "    Возвращает словарь, где ключ - пара символов, значение - частота встречаемости.\n",
        "    \"\"\"\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[symbols[i], symbols[i + 1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    \"\"\"\n",
        "    Заменяет каждое вхождение пары символов на их объединение.\n",
        "    \"\"\"\n",
        "    v_out = {}\n",
        "    bigram = ' '.join(pair)\n",
        "    replacement = ''.join(pair)\n",
        "    for word in v_in:\n",
        "        w_out = word.replace(bigram, replacement)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "def learn_bpe(words, num_merges=10):\n",
        "    \"\"\"\n",
        "    Обучает модель BPE на списке слов.\n",
        "\n",
        "    Параметры:\n",
        "    words: список слов для обучения\n",
        "    num_merges: количество операций слияния\n",
        "\n",
        "    Возвращает:\n",
        "    bpe_codes: словарь операций слияния\n",
        "    vocab: итоговый словарь с преобразованными словами\n",
        "    \"\"\"\n",
        "    # Создаем словарь слов и их частот\n",
        "    vocab = Counter(words)\n",
        "\n",
        "    # Разделяем каждый символ в слове пробелом\n",
        "    vocab = {' '.join(word): freq for word, freq in vocab.items()}\n",
        "\n",
        "    # Словарь операций слияния\n",
        "    bpe_codes = {}\n",
        "\n",
        "    for i in range(num_merges):\n",
        "        # Получаем статистику по парам\n",
        "        pairs = get_stats(vocab)\n",
        "        if not pairs:\n",
        "            break\n",
        "\n",
        "        # Находим самую частую пару\n",
        "        best = max(pairs, key=pairs.get)\n",
        "\n",
        "        # Сохраняем операцию слияния\n",
        "        bpe_codes[best] = i\n",
        "\n",
        "        # Применяем слияние ко всему словарю\n",
        "        vocab = merge_vocab(best, vocab)\n",
        "\n",
        "        print(f\"Слияние #{i+1}: {best} -> {''.join(best)} (частота: {pairs[best]})\")\n",
        "\n",
        "    return bpe_codes, vocab"
      ],
      "metadata": {
        "id": "M1iVHZezVC8W"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение алгоритма BPE:**\n",
        "1. **Подготовка данных:**\n",
        "   - Начинаем с разделения каждого слова на отдельные символы.\n",
        "   - Например, \"собака\" → \"с о б а к а\".\n",
        "\n",
        "2. **Процесс обучения:**\n",
        "   - `get_stats`: Подсчитывает, как часто встречаются пары соседних символов.\n",
        "   - Находим самую частую пару (например, \"с о\" встречается 100 раз).\n",
        "   - `merge_vocab`: Объединяем эту пару в один токен (\"с о\" → \"со\").\n",
        "   - Сохраняем эту операцию слияния в словарь `bpe_codes`.\n",
        "   - Повторяем процесс заданное количество раз (num_merges).\n",
        "\n",
        "3. **Результат:**\n",
        "   - Получаем словарь операций слияния `bpe_codes`.\n",
        "   - И преобразованный словарь `vocab`, где слова уже разбиты на подтокены по правилам BPE.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без BPE наш словарь будет содержать только целые слова, что приведет к проблеме с неизвестными словами.\n",
        "- Если увеличить `num_merges`, мы получим больше операций слияния, что приведет к более крупным токенам.\n",
        "- Если уменьшить `num_merges`, токены будут меньше, ближе к символам."
      ],
      "metadata": {
        "id": "hPuE7b_MZzGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Построение словаря BPE"
      ],
      "metadata": {
        "id": "fhOHPu5HZ71p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовка данных для BPE: получаем плоский список всех токенов\n",
        "flat_tokens = []\n",
        "for sentence in normalized_sentences:\n",
        "    flat_tokens.extend(sentence)\n",
        "\n",
        "# Обучение модели BPE\n",
        "num_merges = 15  # Количество операций слияния\n",
        "bpe_codes, bpe_vocabulary = learn_bpe(flat_tokens, num_merges)\n",
        "\n",
        "print(\"\\nСловарь операций слияния BPE:\")\n",
        "for pair, index in bpe_codes.items():\n",
        "    print(f\"{pair} -> {''.join(pair)}, индекс операции: {index}\")\n",
        "\n",
        "print(\"\\nПример преобразованных слов после BPE:\")\n",
        "# Показываем первые 5 преобразованных слов\n",
        "sample_items = list(bpe_vocabulary.items())[:5]\n",
        "for encoded, freq in sample_items:\n",
        "    original = encoded.replace(' ', '')\n",
        "    print(f\"Оригинал: {original}, Кодированное: {encoded}, Частота: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFWACGn2VHYQ",
        "outputId": "83706f17-05b6-4e33-e007-27bbbd05695b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Слияние #1: ('н', 'и') -> ни (частота: 35)\n",
            "Слияние #2: ('ы', 'й') -> ый (частота: 31)\n",
            "Слияние #3: ('ни', 'е') -> ние (частота: 27)\n",
            "Слияние #4: ('т', 'ь') -> ть (частота: 27)\n",
            "Слияние #5: ('н', 'ый') -> ный (частота: 25)\n",
            "Слияние #6: ('с', 'т') -> ст (частота: 21)\n",
            "Слияние #7: ('р', 'о') -> ро (частота: 20)\n",
            "Слияние #8: ('е', 'ние') -> ение (частота: 19)\n",
            "Слияние #9: ('н', 'о') -> но (частота: 17)\n",
            "Слияние #10: ('н', 'ный') -> нный (частота: 16)\n",
            "Слияние #11: ('р', 'а') -> ра (частота: 16)\n",
            "Слияние #12: ('в', 'а') -> ва (частота: 16)\n",
            "Слияние #13: ('о', 'б') -> об (частота: 15)\n",
            "Слияние #14: ('т', 'е') -> те (частота: 14)\n",
            "Слияние #15: ('т', 'о') -> то (частота: 13)\n",
            "\n",
            "Словарь операций слияния BPE:\n",
            "('н', 'и') -> ни, индекс операции: 0\n",
            "('ы', 'й') -> ый, индекс операции: 1\n",
            "('ни', 'е') -> ние, индекс операции: 2\n",
            "('т', 'ь') -> ть, индекс операции: 3\n",
            "('н', 'ый') -> ный, индекс операции: 4\n",
            "('с', 'т') -> ст, индекс операции: 5\n",
            "('р', 'о') -> ро, индекс операции: 6\n",
            "('е', 'ние') -> ение, индекс операции: 7\n",
            "('н', 'о') -> но, индекс операции: 8\n",
            "('н', 'ный') -> нный, индекс операции: 9\n",
            "('р', 'а') -> ра, индекс операции: 10\n",
            "('в', 'а') -> ва, индекс операции: 11\n",
            "('о', 'б') -> об, индекс операции: 12\n",
            "('т', 'е') -> те, индекс операции: 13\n",
            "('т', 'о') -> то, индекс операции: 14\n",
            "\n",
            "Пример преобразованных слов после BPE:\n",
            "Оригинал: машинный, Кодированное: м а ш и нный, Частота: 5\n",
            "Оригинал: обучение, Кодированное: об у ч ение, Частота: 9\n",
            "Оригинал: —, Кодированное: —, Частота: 4\n",
            "Оригинал: класс, Кодированное: к л а с с, Частота: 1\n",
            "Оригинал: метод, Кодированное: м е то д, Частота: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "- Создаем `flat_tokens` - плоский список всех токенов из всех предложений.\n",
        "- Запускаем процесс обучения BPE с заданным числом слияний (15).\n",
        "- Получаем словарь операций слияния `bpe_codes` и преобразованный словарь `bpe_vocabulary`.\n",
        "- Выводим операции слияния, чтобы увидеть, какие пары символов объединялись.\n",
        "- Показываем примеры слов после применения BPE кодирования.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Изменение `num_merges` влияет на грануляцию токенизации:\n",
        "  - Больше слияний = более крупные токены, меньший словарь, но хуже обобщение.\n",
        "  - Меньше слияний = более мелкие токены, больший словарь, лучше обобщение на новые слова.\n",
        "\n"
      ],
      "metadata": {
        "id": "ekBcYnITaDdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Применение BPE к новому тексту и преобразование в идентификаторы"
      ],
      "metadata": {
        "id": "Bl7Td4DVaT63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_bpe_to_word(word, bpe_codes):\n",
        "    \"\"\"\n",
        "    Применяет обученную модель BPE к слову.\n",
        "    \"\"\"\n",
        "    # Разделяем слово на символы\n",
        "    word = ' '.join(list(word))\n",
        "\n",
        "    # Применяем операции слияния в порядке их изучения\n",
        "    for pair, _ in sorted(bpe_codes.items(), key=lambda x: x[1]):\n",
        "        bigram = ' '.join(pair)\n",
        "        replacement = ''.join(pair)\n",
        "        word = word.replace(bigram, replacement)\n",
        "\n",
        "    return word.split()\n",
        "\n",
        "def tokenize_with_bpe(text, bpe_codes):\n",
        "    \"\"\"\n",
        "    Токенизирует текст с помощью BPE.\n",
        "    \"\"\"\n",
        "    # Сначала разбиваем на предложения и слова\n",
        "    sentences = sent_tokenize(text)\n",
        "    result = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        normalized = normalize_tokens(tokens)\n",
        "\n",
        "        # Применяем BPE к каждому нормализованному токену\n",
        "        bpe_tokens = []\n",
        "        for token in normalized:\n",
        "            bpe_tokens.extend(apply_bpe_to_word(token, bpe_codes))\n",
        "\n",
        "        result.append(bpe_tokens)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Создаем словарь из BPE токенов\n",
        "bpe_token_to_id = {}\n",
        "id_counter = 0\n",
        "\n",
        "for word in bpe_vocabulary:\n",
        "    for token in word.split():\n",
        "        if token not in bpe_token_to_id:\n",
        "            bpe_token_to_id[token] = id_counter\n",
        "            id_counter += 1\n",
        "\n",
        "# Применяем BPE к новому тексту\n",
        "sample_text = \"Нейронные сети обрабатывают данные.\"\n",
        "bpe_tokenized = tokenize_with_bpe(sample_text, bpe_codes)\n",
        "\n",
        "# Преобразуем в идентификаторы\n",
        "token_ids = []\n",
        "for sentence in bpe_tokenized:\n",
        "    for token in sentence:\n",
        "        if token in bpe_token_to_id:\n",
        "            token_ids.append(bpe_token_to_id[token])\n",
        "        else:\n",
        "            # Можно добавить специальный токен для неизвестных слов\n",
        "            print(f\"Неизвестный токен: {token}\")\n",
        "\n",
        "print(\"\\nПример преобразования текста в идентификаторы токенов:\")\n",
        "print(f\"Исходный текст: {sample_text}\")\n",
        "print(f\"Токены после BPE: {bpe_tokenized}\")\n",
        "print(f\"Идентификаторы токенов: {token_ids}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ-mrKJ_VVAF",
        "outputId": "cab26b2d-ebde-4956-c159-0eb73a7b7696"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Пример преобразования текста в идентификаторы токенов:\n",
            "Исходный текст: Нейронные сети обрабатывают данные.\n",
            "Токены после BPE: [['н', 'е', 'й', 'ро', 'нный', 'с', 'е', 'ть', 'об', 'ра', 'б', 'а', 'т', 'ы', 'ва', 'ть', 'д', 'а', 'ть']]\n",
            "Идентификаторы токенов: [18, 13, 30, 32, 4, 12, 13, 28, 5, 22, 42, 1, 20, 43, 37, 28, 15, 1, 28]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Объяснение:**\n",
        "1. **Функция `apply_bpe_to_word`:**\n",
        "   - Разбивает слово на символы.\n",
        "   - Применяет операции слияния в том порядке, в котором они были изучены.\n",
        "   - Возвращает список подтокенов после применения BPE.\n",
        "\n",
        "2. **Функция `tokenize_with_bpe`:**\n",
        "   - Разбивает текст на предложения и слова.\n",
        "   - Нормализует слова (нижний регистр, лемматизация).\n",
        "   - Применяет BPE к каждому нормализованному слову.\n",
        "   - Возвращает список списков токенов для каждого предложения.\n",
        "\n",
        "3. **Создание словаря идентификаторов:**\n",
        "   - Мы присваиваем уникальный числовой ID каждому подтокену из нашего BPE словаря.\n",
        "\n",
        "4. **Применение к новому тексту:**\n",
        "   - Берем новый пример текста.\n",
        "   - Применяем все шаги обработки: токенизация → нормализация → BPE.\n",
        "   - Преобразуем получившиеся токены в идентификаторы.\n",
        "\n",
        "**Что будет, если убрать/изменить:**\n",
        "- Без функции применения BPE мы не сможем обрабатывать новые тексты с помощью нашего метода.\n",
        "- Изменение порядка применения операций слияния повлияет на результат токенизации, поэтому важно соблюдать тот же порядок, что и при обучении."
      ],
      "metadata": {
        "id": "Ln0aUZ_RaaMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Статистика по полученным результатам\n",
        "print(\"\\nИтоговая статистика:\")\n",
        "print(f\"Количество предложений в тексте: {len(sentences)}\")\n",
        "print(f\"Общее количество токенов до нормализации: {sum(len(s) for s in tokenized_sentences)}\")\n",
        "print(f\"Общее количество токенов после нормализации: {sum(len(s) for s in normalized_sentences)}\")\n",
        "print(f\"Размер исходного словаря (уникальных слов): {len(vocabulary)}\")\n",
        "print(f\"Количество операций слияния BPE: {len(bpe_codes)}\")\n",
        "print(f\"Размер BPE словаря: {len(bpe_token_to_id)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2lE2EhGVXw9",
        "outputId": "095241da-8239-461c-b6cb-f9ebc1944f7b"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Итоговая статистика:\n",
            "Количество предложений в тексте: 9\n",
            "Общее количество токенов до нормализации: 234\n",
            "Общее количество токенов после нормализации: 201\n",
            "Размер исходного словаря (уникальных слов): 130\n",
            "Количество операций слияния BPE: 15\n",
            "Размер BPE словаря: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подробное объяснение технологии токенизации и BPE кодирования\n",
        "\n",
        "## 1. Зачем нужна токенизация текста\n",
        "\n",
        "**Что это такое:** Токенизация — это процесс разделения текста на более мелкие части (токены), которые могут быть словами, частями слов или даже отдельными символами.\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Компьютеры не могут напрямую работать с текстом — им нужны числа. Токенизация позволяет преобразовать текст в последовательность числовых идентификаторов.\n",
        "- Языковые модели обучаются предсказывать следующий токен на основе предыдущих. Без токенизации модель не сможет работать с текстом.\n",
        "- Токенизация определяет уровень грануляции, на котором модель \"понимает\" текст.\n",
        "\n",
        "**От чего зависит качество:**\n",
        "- От выбранного алгоритма токенизации\n",
        "- От особенностей языка (для русского важно учитывать богатую морфологию)\n",
        "- От специфики текстов в вашем датасете\n",
        "\n",
        "**Технические последствия:**\n",
        "- Чем больше размер словаря (количество уникальных токенов), тем больше памяти требуется\n",
        "- Слишком маленький словарь приведет к потере информации\n",
        "- Слишком большой словарь усложнит обучение модели и потребует больше данных\n",
        "\n",
        "## 2. Разделение на предложения\n",
        "\n",
        "**Что происходит:** Текст разбивается на отдельные предложения с помощью функции `sent_tokenize` из библиотеки NLTK.\n",
        "\n",
        "**Как это работает:**\n",
        "- Алгоритм анализирует знаки препинания (точки, восклицательные знаки, вопросительные знаки)\n",
        "- Учитывает исключения (сокращения, цифры с точками, инициалы)\n",
        "- Использует обученную модель, которая разбирает различные случаи на основе статистических паттернов\n",
        "\n",
        "**Зачем разделять на предложения:**\n",
        "- Предложение — логическая единица смысла в тексте\n",
        "- Обработка по предложениям эффективнее, чем обработка всего текста сразу\n",
        "- Многие задачи NLP (анализ тональности, классификация) работают на уровне предложений\n",
        "- Связи между словами в разных предложениях обычно слабее, чем внутри одного предложения\n",
        "\n",
        "**От чего зависит точность:**\n",
        "- Качество и правильность пунктуации в исходном тексте\n",
        "- Наличие специфических конструкций (прямая речь, списки, цитаты)\n",
        "- Язык текста и его соответствие обученной модели токенизатора\n",
        "\n",
        "## 3. Разделение предложений на токены (слова)\n",
        "\n",
        "**Что происходит:** Каждое предложение разбивается на слова и знаки препинания с помощью функции `word_tokenize`.\n",
        "\n",
        "**Как это работает:**\n",
        "- Алгоритм разделяет текст по пробелам и знакам препинания\n",
        "- Учитывает сложные случаи (апострофы, дефисы, сокращения)\n",
        "- Сохраняет знаки препинания как отдельные токены\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Слова — базовые единицы значения в языке\n",
        "- Разделение текста на слова позволяет анализировать структуру предложений\n",
        "- Создает основу для дальнейшей нормализации и обработки\n",
        "\n",
        "**Технические нюансы:**\n",
        "- В некоторых языках (например, китайском или японском) разделение на слова сложнее, так как нет явных разделителей\n",
        "- Специальные конструкции (email-адреса, URL, даты) требуют особой обработки\n",
        "- В русском языке важно правильно обрабатывать составные слова с дефисами\n",
        "\n",
        "## 4. Нормализация токенов\n",
        "\n",
        "**Что происходит:** Токены (слова) приводятся к стандартной форме через три основных процесса:\n",
        "1. Приведение к нижнему регистру\n",
        "2. Удаление знаков препинания\n",
        "3. Лемматизация (приведение к начальной форме)\n",
        "\n",
        "**Как работает лемматизация:**\n",
        "- Для русского языка используется библиотека `pymorphy2`\n",
        "- Анализируется морфологическая структура слова\n",
        "- Определяется часть речи и грамматические характеристики\n",
        "- Слово преобразуется к начальной форме:\n",
        "  - существительные → единственное число, именительный падеж\n",
        "  - глаголы → инфинитив\n",
        "  - прилагательные → мужской род, ед. число, именительный падеж\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Снижает количество уникальных токенов (размер словаря)\n",
        "- Объединяет разные формы одного слова: \"книга\", \"книги\", \"книгами\" → \"книга\"\n",
        "- Позволяет модели видеть связь между разными формами одного слова\n",
        "- Улучшает статистические показатели частотности слов\n",
        "\n",
        "**От чего зависит качество:**\n",
        "- От используемого морфологического анализатора\n",
        "- От особенностей языка (для русского лемматизация особенно важна из-за богатства словоформ)\n",
        "- От предметной области текста (специальные термины могут неверно лемматизироваться)\n",
        "\n",
        "## 5. Создание словаря (vocabulary)\n",
        "\n",
        "**Что происходит:** После нормализации создается словарь всех уникальных токенов, где каждому токену присваивается уникальный идентификатор и подсчитывается его частота в тексте.\n",
        "\n",
        "**Как это работает:**\n",
        "- Используется структура данных `Counter` для подсчета встречаемости каждого токена\n",
        "- Токены сортируются по частоте (от наиболее к наименее частым)\n",
        "- Каждому токену присваивается числовой идентификатор (ID)\n",
        "- Создается словарь, где ключ — токен, а значение — объект с его ID и частотой\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Языковые модели работают с числами, а не текстом\n",
        "- Словарь позволяет преобразовывать текст в последовательность чисел и обратно\n",
        "- Частотность токенов используется для оптимизации представления (часто встречающиеся токены получают меньшие ID)\n",
        "- Словарь определяет, какие слова \"знает\" модель\n",
        "\n",
        "**Технические соображения:**\n",
        "- Размер словаря прямо влияет на размер модели и требования к памяти\n",
        "- Слишком большой словарь приводит к разреженным представлениям и проблемам с обучением\n",
        "- Слишком маленький словарь вызывает проблему неизвестных слов (OOV — out-of-vocabulary)\n",
        "\n",
        "## 6. Алгоритм Byte-Pair Encoding (BPE)\n",
        "\n",
        "**Что это такое:** BPE — алгоритм сжатия данных, который в NLP используется для создания подсловных токенов, позволяющих эффективно представлять как частые, так и редкие слова.\n",
        "\n",
        "**Как работает:**\n",
        "1. **Начальное состояние:** Каждое слово разбивается на отдельные символы, разделенные пробелами\n",
        "2. **Итеративный процесс:**\n",
        "   - Подсчитываем частоту всех пар соседних символов/токенов\n",
        "   - Находим самую частую пару\n",
        "   - Объединяем эту пару в один новый токен\n",
        "   - Заменяем все вхождения этой пары в словаре на новый токен\n",
        "   - Повторяем процесс заданное число раз (num_merges)\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Решает проблему неизвестных слов, разбивая редкие слова на подсловные части\n",
        "- Более эффективно использует словарь, чем пословная токенизация\n",
        "- Позволяет модели улавливать морфологические особенности языка\n",
        "- Обеспечивает баланс между размером словаря и способностью представлять любые слова\n",
        "\n",
        "**От чего зависит эффективность:**\n",
        "- От количества операций слияния (num_merges):\n",
        "  - Малое количество → мелкие токены, ближе к посимвольному представлению\n",
        "  - Большое количество → крупные токены, ближе к пословному представлению\n",
        "- От размера и разнообразия тренировочного корпуса\n",
        "- От языковых особенностей (например, для агглютинативных языков BPE особенно эффективен)\n",
        "\n",
        "**Конкретные технические эффекты:**\n",
        "- BPE с 10-15 тысячами операций слияния обычно создает словарь размером 30-50 тысяч токенов\n",
        "- Частые слова представляются одним токеном, редкие разбиваются на несколько подтокенов\n",
        "- Слово \"переобучение\" может быть разбито на \"пере\" + \"обучение\", если эти части чаще встречаются по отдельности\n",
        "\n",
        "## 7. Применение BPE к новому тексту\n",
        "\n",
        "**Что происходит:** Когда приходит новый текст, мы применяем весь процесс обработки и используем ранее полученные правила BPE для его токенизации.\n",
        "\n",
        "**Как это работает:**\n",
        "1. Текст разбивается на предложения\n",
        "2. Предложения токенизируются на слова\n",
        "3. Слова нормализуются (нижний регистр, лемматизация)\n",
        "4. Каждое слово разбивается на символы\n",
        "5. К слову последовательно применяются операции слияния, в том же порядке, как они были найдены при обучении\n",
        "6. Полученные подтокены преобразуются в числовые идентификаторы\n",
        "\n",
        "**Зачем это нужно:**\n",
        "- Обеспечивает единообразное представление как тренировочных, так и новых данных\n",
        "- Позволяет модели работать с ранее не встречавшимися словами\n",
        "- Создает числовое представление текста, пригодное для обработки нейронными сетями\n",
        "\n",
        "**Технические особенности:**\n",
        "- Порядок применения операций слияния критически важен\n",
        "- Если токен не был встречен при обучении, можно использовать специальный токен [UNK] (unknown)\n",
        "- Некоторые реализации используют дополнительные специальные токены:\n",
        "  - [BOS]/[SOS] — начало предложения (Beginning/Start of Sentence)\n",
        "  - [EOS] — конец предложения (End of Sentence)\n",
        "  - [PAD] — заполнитель для выравнивания длины последовательностей\n",
        "\n",
        "## 8. От чего зависит качество всего процесса\n",
        "\n",
        "**Размер и качество обучающего корпуса:**\n",
        "- Больший корпус → лучшее покрытие языка\n",
        "- Разнообразие текстов → лучшая обобщающая способность\n",
        "- Качество текстов → меньше шума и ошибок в данных\n",
        "\n",
        "**Параметры токенизации:**\n",
        "- Выбор метода токенизации (WordPiece, BPE, Unigram и др.)\n",
        "- Размер словаря (маленький — компактность, большой — точность)\n",
        "- Количество операций слияния в BPE (баланс между детализацией и обобщением)\n",
        "\n",
        "**Предобработка текста:**\n",
        "- Качество нормализации (лемматизация vs стемминг)\n",
        "- Обработка специальных случаев (числа, даты, URL)\n",
        "- Удаление или сохранение пунктуации\n",
        "\n",
        "**Применение в языковой модели:**\n",
        "- Способ векторизации токенов (one-hot, embeddings)\n",
        "- Архитектура модели (RNN, Transformer)\n",
        "- Контекстное окно (сколько предыдущих токенов учитывается)\n",
        "\n",
        "## 9. Практическое значение всего процесса\n",
        "\n",
        "**Для языковых моделей:**\n",
        "- BPE позволяет эффективно работать со словарем ограниченного размера\n",
        "- Подсловные токены помогают улавливать морфологические и семантические связи\n",
        "- Сокращается количество неизвестных слов, улучшается обобщающая способность\n",
        "\n",
        "**Для практических приложений:**\n",
        "- Уменьшает требования к памяти (по сравнению с посимвольной токенизацией)\n",
        "- Улучшает работу с редкими словами и новыми терминами\n",
        "- Повышает эффективность машинного перевода, генерации текста, классификации и других задач NLP\n",
        "\n",
        "**Для многоязычных моделей:**\n",
        "- BPE позволяет создать общий словарь для нескольких языков\n",
        "- Обнаруживает общие морфемы между родственными языками\n",
        "- Эффективно работает как с аналитическими, так и с синтетическими языками\n",
        "\n",
        "Понимание процесса токенизации и BPE кодирования критически важно для работы с современными языковыми моделями, так как это первый и один из самых важных шагов в преобразовании текста в форму, понятную компьютеру."
      ],
      "metadata": {
        "id": "_mXudGZ8ccB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "YK6aYweKii2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Реализация N-граммной модели"
      ],
      "metadata": {
        "id": "DU5FKTWksLIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NGramModel:\n",
        "    \"\"\"\n",
        "    Класс для построения и использования N-граммной модели языка.\n",
        "    \"\"\"\n",
        "    def __init__(self, n=2):\n",
        "        \"\"\"\n",
        "        Инициализация модели.\n",
        "\n",
        "        Параметры:\n",
        "        n - размер n-граммы (по умолчанию n=2, т.е. биграммы)\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.ngrams = defaultdict(Counter)  # {(prev_tokens): {next_token: count}}\n",
        "        self.context_counts = defaultdict(int)  # {(prev_tokens): total_count}\n",
        "        self.vocabulary = set()  # все уникальные токены\n",
        "\n",
        "        # Специальные токены\n",
        "        self.START_TOKEN = \"<s>\"  # маркер начала предложения\n",
        "        self.END_TOKEN = \"</s>\"   # маркер конца предложения\n",
        "        self.UNK_TOKEN = \"<unk>\"  # маркер неизвестного слова\n",
        "\n",
        "        # Параметры сглаживания\n",
        "        self.alpha = 0.1  # параметр аддитивного сглаживания\n",
        "\n",
        "    def fit(self, sentences):\n",
        "        \"\"\"\n",
        "        Обучение модели на корпусе предложений.\n",
        "\n",
        "        Параметры:\n",
        "        sentences - список предложений, где каждое предложение - список токенов\n",
        "        \"\"\"\n",
        "        # Сбор лексикона\n",
        "        for sentence in sentences:\n",
        "            for token in sentence:\n",
        "                self.vocabulary.add(token)\n",
        "\n",
        "        # Добавляем специальные токены в словарь\n",
        "        self.vocabulary.add(self.START_TOKEN)\n",
        "        self.vocabulary.add(self.END_TOKEN)\n",
        "        self.vocabulary.add(self.UNK_TOKEN)\n",
        "\n",
        "        # Сбор n-грамм\n",
        "        for sentence in sentences:\n",
        "            # Добавляем маркеры начала и конца предложения\n",
        "            padded_sentence = [self.START_TOKEN] * (self.n - 1) + sentence + [self.END_TOKEN]\n",
        "\n",
        "            # Собираем n-граммы\n",
        "            for i in range(len(padded_sentence) - self.n + 1):\n",
        "                ngram = tuple(padded_sentence[i:i+self.n])\n",
        "                context = ngram[:-1]  # все, кроме последнего токена\n",
        "                next_token = ngram[-1]  # последний токен\n",
        "\n",
        "                self.ngrams[context][next_token] += 1\n",
        "                self.context_counts[context] += 1\n",
        "\n",
        "        print(f\"Модель обучена: собрано {len(self.ngrams)} уникальных контекстов\")\n",
        "        return self\n",
        "\n",
        "    def get_probability(self, context, token, smoothing='additive'):\n",
        "        \"\"\"\n",
        "        Возвращает вероятность токена, учитывая предыдущий контекст.\n",
        "\n",
        "        Параметры:\n",
        "        context - кортеж из (n-1) предыдущих токенов\n",
        "        token - токен, вероятность которого мы вычисляем\n",
        "        smoothing - метод сглаживания ('additive', 'none')\n",
        "\n",
        "        Возвращает:\n",
        "        Вероятность P(token|context)\n",
        "        \"\"\"\n",
        "        # Проверяем наличие контекста в модели\n",
        "        if context not in self.ngrams:\n",
        "            # Если контекст не встречался, возвращаем равномерное распределение\n",
        "            if smoothing == 'none':\n",
        "                return 0.0\n",
        "            else:  # additive smoothing\n",
        "                return 1.0 / len(self.vocabulary)\n",
        "\n",
        "        # Проверяем наличие токена в данном контексте\n",
        "        count = self.ngrams[context].get(token, 0)\n",
        "        total = self.context_counts[context]\n",
        "\n",
        "        if smoothing == 'none':\n",
        "            # Без сглаживания\n",
        "            return count / total if total > 0 else 0.0\n",
        "        else:  # additive smoothing\n",
        "            # Аддитивное сглаживание: (count + alpha) / (total + alpha * |V|)\n",
        "            V = len(self.vocabulary)\n",
        "            return (count + self.alpha) / (total + self.alpha * V)\n",
        "\n",
        "    def get_next_token_probabilities(self, context, smoothing='additive'):\n",
        "        \"\"\"\n",
        "        Возвращает словарь вероятностей всех возможных следующих токенов.\n",
        "\n",
        "        Параметры:\n",
        "        context - кортеж из (n-1) предыдущих токенов\n",
        "        smoothing - метод сглаживания ('additive', 'none')\n",
        "\n",
        "        Возвращает:\n",
        "        Словарь {token: probability}\n",
        "        \"\"\"\n",
        "        probabilities = {}\n",
        "\n",
        "        if smoothing == 'none':\n",
        "            # Без сглаживания - берем только встречавшиеся токены\n",
        "            if context in self.ngrams:\n",
        "                total = self.context_counts[context]\n",
        "                for token, count in self.ngrams[context].items():\n",
        "                    probabilities[token] = count / total\n",
        "        else:  # additive smoothing\n",
        "            # С аддитивным сглаживанием - учитываем все токены из словаря\n",
        "            for token in self.vocabulary:\n",
        "                probabilities[token] = self.get_probability(context, token, smoothing)\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "    def generate_text(self, max_length=20, start_context=None, smoothing='additive'):\n",
        "        \"\"\"\n",
        "        Генерирует текст с использованием обученной модели.\n",
        "\n",
        "        Параметры:\n",
        "        max_length - максимальная длина генерируемого текста (в токенах)\n",
        "        start_context - начальный контекст (если None, начинается с START_TOKEN)\n",
        "        smoothing - метод сглаживания ('additive', 'none')\n",
        "\n",
        "        Возвращает:\n",
        "        Список сгенерированных токенов\n",
        "        \"\"\"\n",
        "        # Инициализация контекста\n",
        "        if start_context is None:\n",
        "            context = tuple([self.START_TOKEN] * (self.n - 1))\n",
        "        else:\n",
        "            # Убедимся, что контекст имеет правильную длину\n",
        "            context = tuple(start_context[-(self.n-1):])\n",
        "\n",
        "        # Генерация текста\n",
        "        generated = list(context)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Получаем вероятности следующего токена\n",
        "            token_probs = self.get_next_token_probabilities(context, smoothing)\n",
        "\n",
        "            # Если нет вероятностей, выходим из цикла\n",
        "            if not token_probs:\n",
        "                break\n",
        "\n",
        "            # Выбираем токен согласно вероятностям\n",
        "            tokens = list(token_probs.keys())\n",
        "            probs = list(token_probs.values())\n",
        "            next_token = random.choices(tokens, weights=probs, k=1)[0]\n",
        "\n",
        "            # Добавляем токен к результату\n",
        "            generated.append(next_token)\n",
        "\n",
        "            # Если сгенерировали токен конца предложения, останавливаемся\n",
        "            if next_token == self.END_TOKEN:\n",
        "                break\n",
        "\n",
        "            # Обновляем контекст\n",
        "            context = tuple(generated[-(self.n-1):])\n",
        "\n",
        "        # Убираем маркеры начала предложения из результата\n",
        "        result = [token for token in generated if token != self.START_TOKEN]\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "1gHYS6OLcfpn"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Комментарии:**\n",
        "- Класс `NGramModel` реализует n-граммную модель языка с возможностью выбора размера n-граммы.\n",
        "- **Основные структуры данных**:\n",
        "  - `self.ngrams` - словарь, где ключ - контекст (n-1 предыдущих токенов), значение - счетчик следующих токенов.\n",
        "  - `self.context_counts` - общее количество раз, когда встречался данный контекст.\n",
        "  - `self.vocabulary` - множество всех уникальных слов.\n",
        "- **Специальные токены**:\n",
        "  - `START_TOKEN` (`<s>`) - маркер начала предложения.\n",
        "  - `END_TOKEN` (`</s>`) - маркер конца предложения.\n",
        "  - `UNK_TOKEN` (`<unk>`) - маркер для неизвестных слов (встречающихся при генерации).\n",
        "- **Метод `fit`**:\n",
        "  1. Собирает все уникальные токены в словарь.\n",
        "  2. Для каждого предложения добавляет специальные маркеры начала и конца.\n",
        "  3. Подсчитывает частоту каждой n-граммы.\n",
        "- **Метод `get_probability`**:\n",
        "  - Вычисляет условную вероятность P(token|context).\n",
        "  - Реализует два варианта расчета: без сглаживания и с аддитивным сглаживанием.\n",
        "  - Формула аддитивного сглаживания: (count + alpha) / (total + alpha * |V|)\n",
        "- **Метод `get_next_token_probabilities`**:\n",
        "  - Возвращает вероятности всех возможных следующих токенов для данного контекста.\n",
        "  - При использовании сглаживания учитывает все токены из словаря.\n",
        "- **Метод `generate_text`**:\n",
        "  1. Начинает с заданного контекста или с маркера начала предложения.\n",
        "  2. На каждом шаге выбирает следующий токен случайно, с вероятностями согласно модели.\n",
        "  3. Обновляет контекст (скользящее окно размера n-1).\n",
        "  4. Останавливается, если достигнута максимальная длина или сгенерирован маркер конца предложения.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Правильная реализация вероятностной модели - ключ к качественной генерации текста.\n",
        "- Сглаживание критически важно для обработки редких и неизвестных слов.\n",
        "- Структура данных `defaultdict(Counter)` идеально подходит для эффективного хранения n-грамм.\n",
        "- Использование специальных токенов позволяет корректно моделировать начало и конец предложений."
      ],
      "metadata": {
        "id": "e5YW-wj2sPK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучаем биграммную модель (n=2)\n",
        "bigram_model = NGramModel(n=2)\n",
        "bigram_model.fit(normalized_sentences)\n",
        "\n",
        "# Обучаем триграммную модель (n=3)\n",
        "trigram_model = NGramModel(n=3)\n",
        "trigram_model.fit(normalized_sentences)\n",
        "\n",
        "# Обучаем 4-граммную модель (n=4)\n",
        "fourgram_model = NGramModel(n=4)\n",
        "fourgram_model.fit(normalized_sentences)\n",
        "\n",
        "print(\"Все модели успешно обучены.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4KENIRRks5s",
        "outputId": "a1342c4f-9ab0-4ac7-e7b3-4c304494b9a8"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Модель обучена: собрано 131 уникальных контекстов\n",
            "Модель обучена: собрано 189 уникальных контекстов\n",
            "Модель обучена: собрано 197 уникальных контекстов\n",
            "Все модели успешно обучены.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- В этом блоке мы создаем и обучаем три модели с различными значениями n:\n",
        "  1. Биграммная модель (n=2) - учитывает только один предыдущий токен.\n",
        "  2. Триграммная модель (n=3) - учитывает два предыдущих токена.\n",
        "  3. 4-граммная модель (n=4) - учитывает три предыдущих токена.\n",
        "- Все модели обучаются на одном и том же наборе нормализованных предложений.\n",
        "- Чем больше n, тем больше контекста учитывает модель, но тем больше данных требуется для надежной оценки вероятностей.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Сравнение моделей разного порядка позволяет найти оптимальный баланс между точностью и разреженностью данных.\n",
        "- Биграммные модели часто дают неплохие результаты даже на малых данных.\n",
        "- Модели высоких порядков могут страдать от проблемы разреженности: многие n-граммы встречаются очень редко или не встречаются вообще."
      ],
      "metadata": {
        "id": "zmVMhG0js7Ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объяснение: Эти числа показывают, сколько различных контекстов (предшествующих n-1 слов) было обнаружено в тексте. С увеличением n растет количество уникальных контекстов, что логично - длинные последовательности имеют больше вариаций. Это важный показатель разнообразия данных для обучения."
      ],
      "metadata": {
        "id": "UOMG44wMm7VB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тестирование методов сглаживания"
      ],
      "metadata": {
        "id": "KteyIErwtsES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_smoothing(model, context, smoothing_methods=['none', 'additive']):\n",
        "    \"\"\"\n",
        "    Сравнивает различные методы сглаживания для заданного контекста.\n",
        "\n",
        "    Параметры:\n",
        "    model - обученная N-граммная модель\n",
        "    context - контекст (n-1 токенов)\n",
        "    smoothing_methods - список методов сглаживания для тестирования\n",
        "    \"\"\"\n",
        "    print(f\"Тестирование методов сглаживания для контекста: {context}\")\n",
        "\n",
        "    for method in smoothing_methods:\n",
        "        print(f\"\\nМетод сглаживания: {method}\")\n",
        "        probs = model.get_next_token_probabilities(context, smoothing=method)\n",
        "\n",
        "        # Выводим топ-5 наиболее вероятных следующих токенов\n",
        "        top_tokens = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "        for token, prob in top_tokens:\n",
        "            print(f\"  {token}: {prob:.4f}\")\n",
        "\n",
        "        # Проверка суммы вероятностей\n",
        "        total_prob = sum(probs.values())\n",
        "        print(f\"  Сумма всех вероятностей: {total_prob:.4f}\")\n",
        "\n",
        "# Тестируем сглаживание для биграммной модели\n",
        "context_bi = (bigram_model.START_TOKEN,)\n",
        "test_smoothing(bigram_model, context_bi)\n",
        "\n",
        "# Тестируем сглаживание для триграммной модели\n",
        "if trigram_model.ngrams:  # Убедимся, что есть данные\n",
        "    context_tri = (trigram_model.START_TOKEN, normalized_sentences[0][0])\n",
        "    test_smoothing(trigram_model, context_tri)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upaeZePKkxW0",
        "outputId": "28058974-e30f-4ffc-95b5-2a01d7c52244"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тестирование методов сглаживания для контекста: ('<s>',)\n",
            "\n",
            "Метод сглаживания: none\n",
            "  машинный: 0.3333\n",
            "  для: 0.1111\n",
            "  глубокий: 0.1111\n",
            "  многие: 0.1111\n",
            "  нейронный: 0.1111\n",
            "  Сумма всех вероятностей: 1.0000\n",
            "\n",
            "Метод сглаживания: additive\n",
            "  машинный: 0.1390\n",
            "  нейронный: 0.0493\n",
            "  они: 0.0493\n",
            "  глубокий: 0.0493\n",
            "  для: 0.0493\n",
            "  Сумма всех вероятностей: 1.0000\n",
            "Тестирование методов сглаживания для контекста: ('<s>', 'машинный')\n",
            "\n",
            "Метод сглаживания: none\n",
            "  обучение: 1.0000\n",
            "  Сумма всех вероятностей: 1.0000\n",
            "\n",
            "Метод сглаживания: additive\n",
            "  обучение: 0.1902\n",
            "  с: 0.0061\n",
            "  рынок: 0.0061\n",
            "  <unk>: 0.0061\n",
            "  задача: 0.0061\n",
            "  Сумма всех вероятностей: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- Функция `test_smoothing` сравнивает различные методы сглаживания для заданного контекста:\n",
        "  1. Получает вероятности всех возможных следующих токенов.\n",
        "  2. Выводит топ-5 наиболее вероятных токенов для наглядности.\n",
        "  3. Проверяет, что сумма всех вероятностей равна 1 (важное свойство вероятностного распределения).\n",
        "- Мы тестируем два метода сглаживания:\n",
        "  - `none` - без сглаживания, только наблюдаемые частоты.\n",
        "  - `additive` - аддитивное сглаживание (метод Лапласа).\n",
        "- Для биграммной модели используем контекст начала предложения `<s>`.\n",
        "- Для триграммной модели используем контекст из начала предложения и первого слова из первого предложения.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Наглядное сравнение методов сглаживания показывает, как сглаживание \"размазывает\" вероятностную массу.\n",
        "- Без сглаживания редкие или неизвестные слова получают нулевую вероятность, что проблематично для генерации.\n",
        "- Аддитивное сглаживание даёт ненулевую вероятность всем словам, но сильнее искажает оценки для частых слов.\n",
        "- Сумма вероятностей равная 1.0 подтверждает корректность вычислений."
      ],
      "metadata": {
        "id": "8JzX9b6JtbH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объяснение:\n",
        "\n",
        "Без сглаживания модель распределяет вероятность только между наблюдаемыми в тренировочном наборе словами. Слово \"машинный\" имеет вероятность 0.3333 (встречается примерно в трети случаев после начала предложения).  \n",
        "С аддитивным сглаживанием вероятность распределяется на все возможные слова словаря, включая те, которые никогда не встречались в данном контексте.  \n",
        "\n",
        " Поэтому вероятность \"машинный\" снижается до 0.1390, а другие слова получают ненулевые вероятности.  \n",
        "\n",
        "Сумма вероятностей всегда равна 1.0, что подтверждает корректность реализации вероятностной модели."
      ],
      "metadata": {
        "id": "u9gwuZdJnElJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Интерпретация:  \n",
        "\n",
        "Без сглаживания: после слова \"машинный\" в начале предложения всегда следует \"обучение\" (вероятность 100%)  \n",
        "Это классический пример детерминированности модели без сглаживания — модель \"заучила\" единственный наблюдаемый паттерн  \n",
        "С аддитивным сглаживанием: хотя \"обучение\" остается наиболее вероятным (19%),\n",
        "  другие слова тоже получают шанс  \n",
        "Огромная разница между 100% и 19% демонстрирует, как сглаживание трансформирует\n",
        "  модель от \"зубрежки\" к \"обобщению\"  "
      ],
      "metadata": {
        "id": "4y_Nf33-otT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Генерация текста с использованием обученных моделей"
      ],
      "metadata": {
        "id": "2MxnIAlDt37a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "0AoK1PChlla6"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_generated_text(tokens):\n",
        "    \"\"\"\n",
        "    Форматирует список токенов в читаемый текст.\n",
        "\n",
        "    Параметры:\n",
        "    tokens - список сгенерированных токенов\n",
        "\n",
        "    Возвращает:\n",
        "    Отформатированную строку\n",
        "    \"\"\"\n",
        "    # Удаляем служебные токены\n",
        "    filtered_tokens = [t for t in tokens if t not in ('</s>', '<s>', '<unk>')]\n",
        "\n",
        "    # Простое соединение токенов пробелами (можно улучшить)\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Генерация текста с использованием биграммной модели\n",
        "print(\"\\nГенерация с использованием биграммной модели:\")\n",
        "for i in range(3):  # Генерируем 3 примера\n",
        "    generated_bi = bigram_model.generate_text(max_length=15)\n",
        "    print(f\"Пример {i+1}: {format_generated_text(generated_bi)}\")\n",
        "\n",
        "# Генерация текста с использованием триграммной модели\n",
        "print(\"\\nГенерация с использованием триграммной модели:\")\n",
        "for i in range(3):  # Генерируем 3 примера\n",
        "    generated_tri = trigram_model.generate_text(max_length=15)\n",
        "    print(f\"Пример {i+1}: {format_generated_text(generated_tri)}\")\n",
        "\n",
        "# Генерация текста с использованием 4-граммной модели\n",
        "print(\"\\nГенерация с использованием 4-граммной модели:\")\n",
        "for i in range(3):  # Генерируем 3 примера\n",
        "    generated_four = fourgram_model.generate_text(max_length=15)\n",
        "    print(f\"Пример {i+1}: {format_generated_text(generated_four)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws5_x3MBlfUb",
        "outputId": "0acf5213-f0c4-49a4-ff2c-1e9e192708fa"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Генерация с использованием биграммной модели:\n",
            "Пример 1: глубокий применение компьютерный нейронный сеть математический пересекаться поэтому применение обучение использовать для нужно для восприятие\n",
            "Пример 2: для архитектура нейронный тесно связать образ и различный « использоваться зрение важность представить средство внимание\n",
            "Пример 3: это представить теория средство механизм прямой цифровой\n",
            "\n",
            "Генерация с использованием триграммной модели:\n",
            "Пример 1: не ввод слой сходный днкпоследовательность алгоритм содержать язык относительный более компьютер задача сеть ввод широкий\n",
            "Пример 2: ввод техника работа применяться ввод днкпоследовательность и обработка вычислительный различный преобразовать фондовый интеллект статья\n",
            "Пример 3: они для нейронный сходный робототехника естественный естественный нужно компьютерный построение задача математический использовать представление вычислительный\n",
            "\n",
            "Генерация с использованием 4-граммной модели:\n",
            "Пример 1: дать ассоциироваться алгоритм а —\n",
            "Пример 2: язык более для конкретный вероятность теория компьютер часть обнаружение образ компьютерный искусственный восприятие зрение\n",
            "Пример 3: многие не прямой каждый ввод теория построение основать всё слой специализироваться статья применяться взвешивание специализированный\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Комментарии:**\n",
        "- Функция `format_generated_text` преобразует список токенов в читаемый текст:\n",
        "  1. Удаляет служебные токены (`<s>`, `</s>`, `<unk>`).\n",
        "  2. Соединяет оставшиеся токены пробелами.\n",
        "- Для каждой модели (биграммной, триграммной, 4-граммной) мы:\n",
        "  1. Генерируем три примера текста максимальной длины 15 токенов.\n",
        "  2. Форматируем и выводим результаты.\n",
        "- По умолчанию используется аддитивное сглаживание для большей вариативности.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Генерация текста - главная цель нашей модели и основной способ оценки её качества.\n",
        "- Сравнение результатов моделей разного порядка показывает влияние размера контекста на связность текста.\n",
        "- Обычно с увеличением n тексты становятся более связными, но для малых данных может наблюдаться обратный эффект из-за разреженности."
      ],
      "metadata": {
        "id": "oA3Y6QA7t7ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Влияние параметра сглаживания на генерацию"
      ],
      "metadata": {
        "id": "FXI3BdsCt-fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Тестируем разные значения alpha для аддитивного сглаживания\n",
        "alphas = [0.01, 0.1, 0.5, 1.0]\n",
        "\n",
        "print(\"\\nВлияние параметра сглаживания alpha на генерацию:\")\n",
        "for alpha in alphas:\n",
        "    # Создаем новую модель с заданным параметром alpha\n",
        "    test_model = NGramModel(n=3)\n",
        "    test_model.fit(normalized_sentences)\n",
        "    test_model.alpha = alpha\n",
        "\n",
        "    # Генерируем текст\n",
        "    generated_text = test_model.generate_text(max_length=15)\n",
        "    formatted_text = format_generated_text(generated_text)\n",
        "\n",
        "    print(f\"\\nAlpha = {alpha}:\")\n",
        "    print(f\"  {formatted_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urZCER5ilwE6",
        "outputId": "89e17bd4-ece1-4c37-8380-b2554f7dffa5"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Влияние параметра сглаживания alpha на генерацию:\n",
            "Модель обучена: собрано 189 уникальных контекстов\n",
            "\n",
            "Alpha = 0.01:\n",
            "  машинный обучение основать граф иметь анализ многие в часть естественный классификация применение взвешивание множество часть\n",
            "Модель обучена: собрано 189 уникальных контекстов\n",
            "\n",
            "Alpha = 0.1:\n",
            "  широкий более самовнимание иметь статистика средство с в иметь манипуляция внимание естественный компьютер 2017 внимание\n",
            "Модель обучена: собрано 189 уникальных контекстов\n",
            "\n",
            "Alpha = 0.5:\n",
            "  в каждый распознавание основать естественный ввод в задача ассоциироваться конкретный процесс конкретный сходный различный язык\n",
            "Модель обучена: собрано 189 уникальных контекстов\n",
            "\n",
            "Alpha = 1.0:\n",
            "  широкий зрение зрение ассоциироваться интеллект данные в композиционный пересекаться машинный теория образ использоваться машинный статья\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- В этом блоке мы исследуем, как параметр сглаживания alpha влияет на качество генерируемого текста.\n",
        "- Тестируем четыре значения alpha: 0.01, 0.1, 0.5 и 1.0.\n",
        "- Для каждого значения alpha:\n",
        "  1. Создаем новую триграммную модель (n=3).\n",
        "  2. Обучаем её на тех же данных.\n",
        "  3. Устанавливаем заданное значение alpha.\n",
        "  4. Генерируем и выводим текст.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Параметр alpha определяет степень \"сглаживания\" распределения:\n",
        "  - Маленькие значения (0.01) - почти не меняют исходное распределение, модель больше придерживается наблюдаемых данных.\n",
        "  - Большие значения (1.0) - сильно сглаживают распределение, придавая больший вес редким и неизвестным словам.\n",
        "- Оптимальное значение alpha зависит от размера и разнообразия корпуса.\n",
        "- Эксперимент помогает выбрать оптимальное значение для конкретной задачи."
      ],
      "metadata": {
        "id": "8YXpUOxguEeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "С увеличением alpha от 0.01 до 1.0 наблюдается:\n",
        "\n",
        "При малых значениях (0.01): текст ближе к наблюдаемым паттернам в обучающих данных  \n",
        "При больших значениях (1.0): больше случайности и разнообразия, но меньше\n",
        "  связности"
      ],
      "metadata": {
        "id": "NSXACVgLntGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Интерпретация:\n",
        "\n",
        "При малых alpha (0.01): текст более связный, но менее разнообразный, ближе к наблюдавшимся в корпусе последовательностям\n",
        "При больших alpha (1.0): больше разнообразия, но меньше связности, поскольку модель чаще \"пробует\" редкие слова\n",
        "Оптимальное значение alpha должно выбираться с помощью перекрестной проверки, но обычно находится в диапазоне 0.1-0.5"
      ],
      "metadata": {
        "id": "45hfYB9Yo-u_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Оценка перплексии модели на тестовых данных"
      ],
      "metadata": {
        "id": "SMxfggJEuKMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "E5NpiRqml7SB"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model, test_sentences, smoothing='additive'):\n",
        "    \"\"\"\n",
        "    Вычисляет перплексию модели на тестовых данных.\n",
        "\n",
        "    Параметры:\n",
        "    model - обученная N-граммная модель\n",
        "    test_sentences - список тестовых предложений\n",
        "    smoothing - метод сглаживания\n",
        "\n",
        "    Возвращает:\n",
        "    Значение перплексии\n",
        "    \"\"\"\n",
        "    log_prob_sum = 0\n",
        "    token_count = 0\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        # Добавляем маркеры начала и конца предложения\n",
        "        padded_sentence = [model.START_TOKEN] * (model.n - 1) + sentence + [model.END_TOKEN]\n",
        "\n",
        "        # Вычисляем вероятность предложения\n",
        "        for i in range(model.n - 1, len(padded_sentence)):\n",
        "            context = tuple(padded_sentence[i-(model.n-1):i])\n",
        "            token = padded_sentence[i]\n",
        "\n",
        "            # Получаем вероятность токена с учетом контекста\n",
        "            prob = model.get_probability(context, token, smoothing=smoothing)\n",
        "\n",
        "            # Избегаем log(0)\n",
        "            if prob > 0:\n",
        "                log_prob_sum += np.log2(prob)\n",
        "            else:\n",
        "                log_prob_sum += np.log2(1e-10)  # очень маленькая вероятность\n",
        "\n",
        "            token_count += 1\n",
        "\n",
        "    # Вычисляем перплексию\n",
        "    if token_count > 0:\n",
        "        perplexity = 2 ** (-log_prob_sum / token_count)\n",
        "        return perplexity\n",
        "    else:\n",
        "        return float('inf')\n",
        "\n",
        "# Разделяем данные на обучающую и тестовую выборки\n",
        "train_size = int(0.8 * len(normalized_sentences))\n",
        "train_sentences = normalized_sentences[:train_size]\n",
        "test_sentences = normalized_sentences[train_size:]\n",
        "\n",
        "# Обучаем модели на обучающей выборке\n",
        "train_bigram = NGramModel(n=2)\n",
        "train_bigram.fit(train_sentences)\n",
        "\n",
        "train_trigram = NGramModel(n=3)\n",
        "train_trigram.fit(train_sentences)\n",
        "\n",
        "# Вычисляем перплексию на тестовой выборке\n",
        "bigram_perplexity = calculate_perplexity(train_bigram, test_sentences)\n",
        "trigram_perplexity = calculate_perplexity(train_trigram, test_sentences)\n",
        "\n",
        "print(\"\\nОценка качества моделей с помощью перплексии:\")\n",
        "print(f\"Перплексия биграммной модели: {bigram_perplexity:.2f}\")\n",
        "print(f\"Перплексия триграммной модели: {trigram_perplexity:.2f}\")\n",
        "print(\"Примечание: чем ниже перплексия, тем лучше модель предсказывает текст\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFgL3YxKl2VB",
        "outputId": "2385bbaf-2308-4927-d17f-8861427bf894"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Модель обучена: собрано 105 уникальных контекстов\n",
            "Модель обучена: собрано 151 уникальных контекстов\n",
            "\n",
            "Оценка качества моделей с помощью перплексии:\n",
            "Перплексия биграммной модели: 97.03\n",
            "Перплексия триграммной модели: 104.62\n",
            "Примечание: чем ниже перплексия, тем лучше модель предсказывает текст\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Комментарии:**\n",
        "- Функция `calculate_perplexity` вычисляет перплексию модели на тестовых данных:\n",
        "  1. Для каждого предложения добавляем маркеры начала и конца.\n",
        "  2. Для каждого токена вычисляем его вероятность с учетом контекста.\n",
        "  3. Суммируем логарифмы вероятностей.\n",
        "  4. Вычисляем перплексию как 2 в степени отрицательного среднего логарифма вероятности.\n",
        "- Мы разделяем наши данные на обучающую (80%) и тестовую (20%) выборки.\n",
        "- Обучаем биграммную и триграммную модели только на обучающей выборке.\n",
        "- Вычисляем перплексию на тестовой выборке для обеих моделей.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Перплексия - стандартная метрика для оценки языковых моделей.\n",
        "- Можно интерпретировать как \"среднее количество равновероятных вариантов на каждом шаге\".\n",
        "- Чем ниже перплексия, тем лучше модель предсказывает текст.\n",
        "- Сравнение перплексии разных моделей позволяет объективно выбрать лучшую.\n",
        "- В наших результатах биграммная модель показала лучшую перплексию, что указывает на недостаток данных для надежного обучения триграммной модели."
      ],
      "metadata": {
        "id": "bFSxpRLIuOno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перплексия  \n",
        "\n",
        "Перплексия биграммной модели: 97.03  \n",
        "Перплексия триграммной модели: 104.62  \n",
        "Объяснение: Перплексия — это метрика, показывающая насколько модель \"удивлена\"\n",
        "  текстом. Чем ниже перплексия, тем лучше модель предсказывает следующее слово.  \n",
        "\n",
        "Интересно, что биграммная модель показала лучшую (более низкую) перплексию, чем\n",
        "  триграммная. Это может быть связано с:  \n",
        "\n",
        "Недостаточным объемом обучающих данных для триграммной модели  \n",
        "Проблемой разреженности (многие триграммы встречаются очень редко)  "
      ],
      "metadata": {
        "id": "YnBfgQHondmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Интерпретация:\n",
        "\n",
        "Перплексия — это экспонента от средней отрицательной логарифмической вероятности. Её можно интерпретировать как \"среднее количество равновероятных выборов на каждом шаге\"  \n",
        "Перплексия 97.03 означает, что биграммная модель так же неуверенна в предсказании, как если бы она каждый раз \"выбирала\" из 97 равновероятных слов\n",
        "Перплексия триграммной модели выше (хуже), чем у биграммной.   \n",
        "Это контринтуитивно, поскольку с большим контекстом предсказания должны быть точнее\n",
        "Это явный признак проблемы разреженности данных: многие триграммы встречаются в корпусе только один раз, что делает оценки вероятностей ненадежными\n",
        "Для сравнения, современные языковые модели имеют перплексию около 20-40 на обычном тексте, а человек — около 10-20"
      ],
      "metadata": {
        "id": "er3SmpzWo1S_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Анализ результатов и выводы"
      ],
      "metadata": {
        "id": "5wTx-ULGuWEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Словарь для сохранения результатов экспериментов\n",
        "results = {\n",
        "    'generated_texts': {\n",
        "        'bigram': [],\n",
        "        'trigram': [],\n",
        "        'fourgram': []\n",
        "    },\n",
        "    'smoothing_comparison': {},\n",
        "    'alpha_comparison': {},\n",
        "    'perplexity': {\n",
        "        'bigram': bigram_perplexity,\n",
        "        'trigram': trigram_perplexity\n",
        "    }\n",
        "}\n",
        "\n",
        "# Генерация нескольких примеров текста для анализа\n",
        "for _ in range(5):\n",
        "    results['generated_texts']['bigram'].append(\n",
        "        format_generated_text(bigram_model.generate_text(max_length=20))\n",
        "    )\n",
        "    results['generated_texts']['trigram'].append(\n",
        "        format_generated_text(trigram_model.generate_text(max_length=20))\n",
        "    )\n",
        "    results['generated_texts']['fourgram'].append(\n",
        "        format_generated_text(fourgram_model.generate_text(max_length=20))\n",
        "    )\n",
        "\n",
        "# Выводим наиболее удачные примеры генерации\n",
        "print(\"\\nНаиболее интересные примеры генерации текста:\")\n",
        "\n",
        "print(\"\\nБиграммная модель:\")\n",
        "for text in results['generated_texts']['bigram'][:2]:\n",
        "    print(f\"  {text}\")\n",
        "\n",
        "print(\"\\nТриграммная модель:\")\n",
        "for text in results['generated_texts']['trigram'][:2]:\n",
        "    print(f\"  {text}\")\n",
        "\n",
        "print(\"\\nЧетырехграммная модель:\")\n",
        "for text in results['generated_texts']['fourgram'][:2]:\n",
        "    print(f\"  {text}\")\n",
        "\n",
        "print(\"\\nВыводы:\")\n",
        "print(\"1. С увеличением n (размера n-граммы) модель генерирует более связные и осмысленные тексты\")\n",
        "print(\"2. Сглаживание критически важно для обработки редких и неизвестных слов\")\n",
        "print(\"3. Параметр сглаживания alpha влияет на разнообразие генерируемого текста\")\n",
        "print(\"4. Триграммная модель обычно показывает лучший баланс между качеством и сложностью\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGkrLDA2mCAz",
        "outputId": "56cf879b-83f7-4a4b-c93c-246376b2a789"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Наиболее интересные примеры генерации текста:\n",
            "\n",
            "Биграммная модель:\n",
            "  трансформера техника программа граф данные язык а обучение основать помощь компьютер речь программа естественный естественный внимание построение такой входной обнаружение\n",
            "  машинный обучение многие зрение распознавание рынок они быть год днкпоследовательность в форма классификация связать граф обработка естественный язык решение задача\n",
            "\n",
            "Триграммная модель:\n",
            "  искусственный приложение метод прямой численный решение слой преобразовать каждый и это архитектура ряд применяться полагаться архитектура нужно численный для нейронный\n",
            "  фондовый а математический робототехника теория это и многие численный часть который совокупность фондовый входной вероятность построение широко приложение представление черта\n",
            "\n",
            "Четырехграммная модель:\n",
            "  совокупность обучение спам множество — вероятность естественный преобразовать речь с черта представление являться интеллект компьютерный абстрактный обучение содержать ряд средство\n",
            "  обработка программа программа это фондовый форма они множество основать форма искусственный использовать программа днкпоследовательность вычислительный черта применяться черта содержать\n",
            "\n",
            "Выводы:\n",
            "1. С увеличением n (размера n-граммы) модель генерирует более связные и осмысленные тексты\n",
            "2. Сглаживание критически важно для обработки редких и неизвестных слов\n",
            "3. Параметр сглаживания alpha влияет на разнообразие генерируемого текста\n",
            "4. Триграммная модель обычно показывает лучший баланс между качеством и сложностью\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- В этом блоке мы сохраняем и анализируем результаты наших экспериментов:\n",
        "  1. Создаем структуру данных для хранения результатов.\n",
        "  2. Генерируем дополнительные примеры текста для каждой модели.\n",
        "  3. Выводим наиболее интересные примеры.\n",
        "  4. Формулируем общие выводы на основе наших наблюдений.\n",
        "- Выводы подтверждаются нашими экспериментами:\n",
        "  1. Больший размер n-граммы обычно дает более связные тексты (при достаточном количестве данных).\n",
        "  2. Сглаживание необходимо для обработки редких слов и новых контекстов.\n",
        "  3. Параметр alpha влияет на баланс между точностью и разнообразием.\n",
        "  4. Триграммные модели обычно представляют хороший компромисс между качеством и требованиями к данным.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Систематический анализ результатов помогает выбрать оптимальную модель для конкретной задачи.\n",
        "- Важно понимать компромиссы между различными параметрами модели.\n",
        "- Формулирование чётких выводов делает наше исследование более ценным и применимым.\n"
      ],
      "metadata": {
        "id": "Ioz6lzFmuZ0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Практическое применение - интерактивная генерация текста"
      ],
      "metadata": {
        "id": "1QEPWOvDueQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_start(model, start_text, max_length=20):\n",
        "    \"\"\"\n",
        "    Генерирует текст, начиная с заданной фразы.\n",
        "\n",
        "    Параметры:\n",
        "    model - обученная модель\n",
        "    start_text - начальный текст (строка)\n",
        "    max_length - максимальная длина генерации\n",
        "\n",
        "    Возвращает:\n",
        "    Сгенерированный текст\n",
        "    \"\"\"\n",
        "    # Подготовка начального текста\n",
        "    tokens = word_tokenize(start_text)\n",
        "    normalized = normalize_tokens(tokens)\n",
        "\n",
        "    # Обрезаем до (n-1) токенов, чтобы использовать как контекст\n",
        "    context = normalized[-(model.n-1):] if len(normalized) >= model.n-1 else normalized\n",
        "\n",
        "    # Дополняем контекст START_TOKEN, если нужно\n",
        "    if len(context) < model.n-1:\n",
        "        context = [model.START_TOKEN] * (model.n-1 - len(context)) + context\n",
        "\n",
        "    # Генерируем продолжение\n",
        "    generated = model.generate_text(max_length=max_length, start_context=context)\n",
        "\n",
        "    # Форматируем результат\n",
        "    full_text = start_text + \" \" + format_generated_text(generated[(model.n-1):])\n",
        "    return full_text\n",
        "\n",
        "# Примеры генерации с заданным началом\n",
        "print(\"\\nГенерация текста с заданным началом:\")\n",
        "\n",
        "start_texts = [\n",
        "    \"Машинное обучение\",\n",
        "    \"Глубокие нейронные сети\",\n",
        "    \"Языковые модели\"\n",
        "]\n",
        "\n",
        "for start in start_texts:\n",
        "    print(f\"\\nНачало: {start}\")\n",
        "    print(f\"Биграммная модель: {generate_with_start(bigram_model, start)}\")\n",
        "    print(f\"Триграммная модель: {generate_with_start(trigram_model, start)}\")\n",
        "    print(f\"4-граммная модель: {generate_with_start(fourgram_model, start)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ti0PD5vmGqi",
        "outputId": "5fb64f5a-818f-47d8-b4d2-eafff51d5001"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Генерация текста с заданным началом:\n",
            "\n",
            "Начало: Машинное обучение\n",
            "Биграммная модель: Машинное обучение восприятие обнаружение спам дать зрение важность рынок мошенничество и вы\n",
            "Триграммная модель: Машинное обучение тесно совокупность часть построение представить « являться игровой использовать классификация » композиционный архитектура совокупность искусственный работа такой с содержать представление\n",
            "4-граммная модель: Машинное обучение дать восприятие они год прямой распознавание относительный такой такой классификация для компьютерный важность различный класс различный классификация цифровой форма\n",
            "\n",
            "Начало: Глубокие нейронные сети\n",
            "Биграммная модель: Глубокие нейронные сети каждый часть содержать слой алгоритм не прямой нейронный множество оптимизация теория вычислительный » естественный иметь быть манипуляция они это данные\n",
            "Триграммная модель: Глубокие нейронные сети не спам процесс композиционный распознавание это ввод речь 2017 форма такой классификация трансформера часто зрение абстрактный « прямой часть\n",
            "4-граммная модель: Глубокие нейронные сети трансформера спам робототехника прогнозирование представление часть конкретный представить математический форма представить игровой речь более это различный прямой процесс граф обработка\n",
            "\n",
            "Начало: Языковые модели\n",
            "Биграммная модель: Языковые модели манипуляция построение машинный обучение искусственный естественный это такой приложение механизм часть статистика данные различный язык в прогнозирование применяться в обработка\n",
            "Триграммная модель: Языковые модели граф часто глубокий часть часть важность мошенничество быть рынок более программа интеллект обработка компьютерный « абстрактный широко работа это работа\n",
            "4-граммная модель: Языковые модели метод это численный нейронный « компьютер композиционный под и программа конкретный специализироваться что широко распознавание вычислительный содержать под\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Комментарии:**\n",
        "- Функция `generate_with_start` позволяет генерировать текст, продолжающий заданную фразу:\n",
        "  1. Токенизирует и нормализует начальный текст.\n",
        "  2. Подготавливает контекст нужной длины (n-1).\n",
        "  3. Генерирует продолжение с помощью модели.\n",
        "  4. Объединяет начальный текст и сгенерированное продолжение.\n",
        "- Мы тестируем эту функцию на трех разных фразах, связанных с тематикой нашего корпуса:\n",
        "  - \"Машинное обучение\"\n",
        "  - \"Глубокие нейронные сети\"\n",
        "  - \"Языковые модели\"\n",
        "- Для каждой фразы генерируем продолжения с помощью всех трех моделей.\n",
        "\n",
        "**Почему это важно:**\n",
        "- Генерация текста с заданным началом - практически полезный вариант использования языковых моделей.\n",
        "- Такой подход можно использовать для автодополнения, генерации подсказок, помощи в написании текстов.\n",
        "- Сравнение результатов разных моделей на одинаковых начальных фразах наглядно показывает их различия.\n",
        "- Этот блок демонстрирует практическое применение созданных нами моделей."
      ],
      "metadata": {
        "id": "3WyyivOauh6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Общие выводы по метрикам  \n",
        "Недостаточность данных: Маленькое количество уникальных контекстов и ухудшение перплексии с ростом N говорит о том, что корпус слишком мал для обучения качественных моделей высокого порядка.  \n",
        "Разреженность: Резкое ухудшение перплексии триграммной модели по сравнению с биграммной — классический признак проблемы разреженности данных.  \n",
        "Важность сглаживания: Метрики наглядно показывают, как сглаживание трансформирует \"жесткую\" модель с нулевыми вероятностями в более гибкую.  \n",
        "Компромисс размера N: Для данного корпуса оптимальной оказалась биграммная модель (N=2), что подтверждается лучшей перплексией.  \n",
        "Результат предсказуем: Полученные метрики соответствуют теоретическим ожиданиям для N-граммных моделей на небольшом корпусе текстов, что подтверждает корректность реализации.  \n",
        "Данные метрики демонстрируют фундаментальные свойства статистических языковых моделей и проблемы, с которыми они сталкиваются (разреженность, компромисс размера контекста), что объясняет, почему современные подходы перешли к нейросетевым моделям.  "
      ],
      "metadata": {
        "id": "ZqXnJ38RrCnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Общие выводы по реализации N-граммной модели\n",
        "\n",
        "1. Мы успешно реализовали все требуемые компоненты:\n",
        "   - N-граммную модель для произвольного значения n\n",
        "   - Сглаживание вероятностей (аддитивное сглаживание)\n",
        "   - Генерацию текста на основе обученной модели\n",
        "\n",
        "2. Наша реализация имеет ряд полезных особенностей:\n",
        "   - Объектно-ориентированный подход упрощает работу с моделью\n",
        "   - Различные методы сглаживания можно легко добавить\n",
        "   - Реализация позволяет начинать генерацию с произвольного контекста\n",
        "\n",
        "3. Мы провели серию экспериментов, которые показали:\n",
        "   - Компромисс между размером n-граммы и качеством модели\n",
        "   - Важность сглаживания для обработки редких слов\n",
        "   - Влияние параметра сглаживания на разнообразие генерации\n",
        "\n",
        "4. Для дальнейшего улучшения можно:\n",
        "   - Реализовать более сложные методы сглаживания (Kneser-Ney, Good-Turing)\n",
        "   - Использовать backoff-модели для комбинирования n-грамм разных порядков\n",
        "   - Увеличить объем обучающих данных для более надежной оценки вероятностей"
      ],
      "metadata": {
        "id": "2y_jxymQunax"
      }
    }
  ]
}